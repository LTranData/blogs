<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-blog">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<link rel="search" type="application/opensearchdescription+xml" title="Tran Lam" href="/blogs/opensearch.xml">
<link rel="alternate" type="application/rss+xml" href="/blogs/blog/rss.xml" title="Tran Lam RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blogs/blog/atom.xml" title="Tran Lam Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Một số câu hỏi phỏng vấn AI/ML</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://lam1051999.github.io//blogs/blog/ai-interview-questions"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Một số câu hỏi phỏng vấn AI/ML | Tran Lam"><meta data-rh="true" name="description" content="Intro"><meta data-rh="true" property="og:description" content="Intro"><meta data-rh="true" property="og:image" content="https://lam1051999.github.io//blogs/assets/images/intro-0449b090aa577145a0488596b4103224.JPEG"><meta data-rh="true" name="twitter:image" content="https://lam1051999.github.io//blogs/assets/images/intro-0449b090aa577145a0488596b4103224.JPEG"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2021-07-24T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/lam1051999"><meta data-rh="true" property="article:tag" content="AI,ML,Machine Learning,Artificial Intelligence"><link data-rh="true" rel="icon" href="/blogs/img/de.svg"><link data-rh="true" rel="canonical" href="https://lam1051999.github.io//blogs/blog/ai-interview-questions"><link data-rh="true" rel="alternate" href="https://lam1051999.github.io//blogs/blog/ai-interview-questions" hreflang="en"><link data-rh="true" rel="alternate" href="https://lam1051999.github.io//blogs/blog/ai-interview-questions" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://KPAEHOL9XK-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/blogs/assets/css/styles.7b917ad4.css">
<link rel="preload" href="/blogs/assets/js/runtime~main.8cce032e.js" as="script">
<link rel="preload" href="/blogs/assets/js/main.40443e59.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/blogs/"><div class="navbar__logo"><img src="/blogs/img/de.svg" alt="TL Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/blogs/img/de.svg" alt="TL Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Tran Lam</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blogs/">About</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blogs/blog">Blog</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/mysql-series-mysql-indexing">MySQL series - Indexing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/mysql-series-mysql-mvcc">MySQL series - Multiversion concurrency control</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/mysql-series-mysql-transaction">MySQL series - Transaction trong MySQL</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/mysql-series-mysql-architecture">MySQL series - Tổng quan kiến trúc MySQL</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/spark-kafka-docker">Tạo luồng streaming dữ liệu với Spark Streaming, Kafka, Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/spark-cluster-docker">Tạo 1 Standalone Spark Cluster với Docker</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blogs/blog/ai-interview-questions">Một số câu hỏi phỏng vấn AI/ML</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/receptive-field">Receptive field trong thị giác máy tính</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/linear-algebra-part-1">Đại số tuyến tính cơ bản - Phần 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/probability">Xác suất thống kê cơ bản</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/avl-tree">Cây AVL, thuật toán sắp xếp AVL</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/binarysearch-tree">Cây tìm kiếm nhị phân</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/sorting-algorithms">Các thuật toán sắp xếp cơ bản</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blogs/blog/peak-finding">Thuật toán tìm đỉnh Peak Finding</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_Ikge" itemprop="headline">Một số câu hỏi phỏng vấn AI/ML</h1><div class="blogPostData_SAv4 margin-vert--md"><time datetime="2021-07-24T00:00:00.000Z" itemprop="datePublished">July 24, 2021</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_sTYa"><div class="avatar margin-bottom--sm"><a href="https://github.com/lam1051999" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/lam1051999.png" alt="Trần Lâm"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/lam1051999" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trần Lâm</span></a></div><small class="avatar__subtitle" itemprop="description">Data Engineer @ Giaohangtietkiem</small></div></div></div></div></header><meta itemprop="image" content="https://lam1051999.github.io//blogs/assets/images/intro-0449b090aa577145a0488596b4103224.JPEG"><div id="post-content" class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Intro" src="/blogs/assets/images/intro-0449b090aa577145a0488596b4103224.JPEG" width="800" height="450" class="img_ev3q"></p><p>Gần đây, AI/ML là một cái trend, người người AI, nhà nhà AI. Sinh viên đổ xô đi học AI hết, các trường đại học cũng dần mở các môn về học máy, trí tuệ nhân tạo, rồi thị giác máy tính để &quot;bắt kịp&quot;. <!-- -->Dưới đây, mình sẽ trình bày về các câu hỏi phỏng vấn cơ bản, dành cho các bạn muốn tìm các vị trí thực tập trong lĩnh vực này.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1-bias-variance">1. Bias, variance<a class="hash-link" href="#1-bias-variance" title="Direct link to heading">​</a></h3><p><strong>Các câu hỏi mục này sẽ xung quanh các thông số trên là gì, nó cao nó thấp thì ảnh hưởng như nào, xử lý thế nào?</strong></p><ul><li>Bias là sai số giữa kết quả dự đoán của model và các nhãn thực sự của chúng ta. Bias thể hiện năng lực của model trong việc dự đoán. Bias cao nói lên rằng model của chúng ta không quan tâm đến dữ liệu, model quá đơn giản để mà có thể học được các đặc trưng từ dữ liệu. Bias cao thường cho kết quả lỗi cao trên cả tập huấn luyện và tập kiểm thử. Hiện tượng này gọi là underfitting.</li><li>Variance là độ phân tán của kết quả dự đoán của model chúng ta. Variance cao nói lên rằng model của chúng ta quá tập trung vào tập dữ liệu huấn luyện, có thể phân loại các điểm dữ liệu huấn luyện tốt mà không tổng quát tốt, thích ứng tốt với các điểm dữ liệu mới, model của chúng ta quá phức tạp nhưng lượng dữ liệu của chúng ta lại không đủ lớn. Hiện tượng này gọi là overfitting.</li></ul><p>Dưới đây là hình ảnh minh họa cho bias và variance
<img loading="lazy" alt="Bias Variance Tradeoff" src="/blogs/assets/images/bias_variance_tradeoff-dc2b1699263e0d06be2007e90793afc5.PNG" width="1103" height="650" class="img_ev3q"></p><p>Gọi ground truth (có nghĩa là giá trị nhãn thực sự của dữ liệu đầu vào) là điểm mà nằm ở tâm các hình tròn ở trên, còn các prediction (là các giá trị nhãn dự đoán của ta), là các chấm tròn màu xanh nước biển. Với trường hợp low bias và low variance, các điểm nhãn dự đoán sẽ rất sát nhau và sát với tâm của bia tròn, thể hiện rằng độ phân tán các kết quả dự đoán thấp và khoảng cách giữa nhãn dự đoán và nhãn thực tế thấp, tượng tự các trường hợp khác.</p><p>Các phương pháp giảm thiểu overfitting</p><ul><li>Kiếm nhiều dữ liệu hơn cho mô hình.</li><li>Các kỹ thuật regularization, drop out, early stopping, batch normalization (cái này giảm thiểu một ít thôi).</li><li>Chỉnh sửa các siêu tham số của mô hình để kiểm soát quá trình huấn luyện.</li></ul><p>Các phương pháp giảm thiểu underfitting</p><ul><li>Huấn luyện nhiều epoch hơn.</li><li>Xây mạng to hơn, tăng số đặc trưng đầu vào lên.</li><li>Loại bỏ các điểm dữ liệu gây nhiễu.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2-batch-normalization">2. Batch normalization<a class="hash-link" href="#2-batch-normalization" title="Direct link to heading">​</a></h3><p>Là một kỹ thuật giúp mô hình ổn định hơn trong quá trình huấn luyện (ở đây mình sẽ không trình bày về toán của batch normalization). Một số tính chất của batch normalization như sau</p><ul><li>Batchnorm khiến quá trình huấn luyện ổn định hơn, mạng hội tụ nhanh hơn.</li><li>Batchnorm cho phép ta sử dụng learning rate lớn hơn.</li><li>Batchnorm khiến việc khởi tạo các ma trận trọng số dễ dàng hơn vì nó giúp giảm thiểu độ nhạy của mô hình với các vấn đề gradient vanishing/exploding. Từ đó, việc tạo dựng mạng của chúng ta dễ thở hơn.</li><li>Batchnorm cho ta nhiều lựa chọn các hàm kích hoạt, bởi vì một số hàm kích hoạt có đạo hàm thường co về <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span> nhanh trong quá trình backpropagation như sigmoid,… Batchnorm điều tiết dữ liệu của chúng ta về 1 phân bố nhất định để trong khoảng giá trị đó, đạo hàm các hàm kích hoạt có độ lớn tương đối.</li><li>Batchnorm cho một số hiệu ứng phụ của regularization, bởi vì Batchnorm khiến các điểm dữ liệu có kỳ vọng là <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span>, độ lệch chuẩn nhỏ, ở đó, các hàm kích hoạt thường có dạng gần với tuyến tính, nên mô hình của ta nó đỡ độ phi tuyến hơn.</li></ul><p><strong>Tại sao Batchnorm giúp thuật toán hội tụ nhanh hơn?</strong></p><ul><li>Batchnorm khiến cho các đầu vào trước các hàm kích hoạt của chúng ta về khoảng giá trị nhất định và có kỳ vọng là <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span>. Đầu vào lúc đó không quá to hay cũng không quá nhỏ, nên đạo hàm của hàm kích hoạt sẽ không gần đến <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span>, do đó hội tụ nhanh hơn.</li><li>Batchnorm khiến cho các đầu vào nằm trong 1 khoảng giá trị nhỏ, khi đó thì hàm mất mát của ta có đồ thị tròn hơn, do vậy, khoảng cách từ 1 điểm bất kỳ trên đồ thị tới một điểm cực tiểu bất kỳ sẽ ngắn hơn, model của chúng ta sẽ tìm được điểm cực tiểu nhanh hơn.</li></ul><p><strong>Sự khác nhau của train và test khi thực hiện Batchnorm?</strong></p><p>Khi train, Batchnorm có thể được áp dụng trên các minibatch, 1 mini-batch có N điểm dữ liệu. Trong quá trình train, ta Batchnorm trên <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> điểm dữ liệu đó. Nhưng ở test time, ta chỉ có 1 dữ liệu 1 lần và phải đưa dự đoán với 1 điểm dữ liệu đó. Có 1 cách là ta ước lượng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span> và <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> bằng Exponentially Weighted Average để sử dụng vào test time.</p><p>Ví dụ: Trong quá trình train, tại layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span>, nó cho <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span> và <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> nhất định, ta lưu nó,… Khi hoàn tất quá trình train, mỗi layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> đều số cặp <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span> và <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> bằng số mini-batch. Và ta sử dụng kỹ thuật Exponentially Weighted Average để tính <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span> và <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> cho layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> cho điểm dữ liệu ở test time đó.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="3-mini-batch-nhỏ-hay-lớn-ảnh-hưởng-thế-nào">3. Mini-batch nhỏ hay lớn ảnh hưởng thế nào?<a class="hash-link" href="#3-mini-batch-nhỏ-hay-lớn-ảnh-hưởng-thế-nào" title="Direct link to heading">​</a></h3><ul><li>Mini-batch nhỏ thì trên đồ thị hàm loss, đường đi từ 1 điểm bất kỳ đến 1 điểm cực trị local minimum sẽ rất gồ ghề và có thể cần nhiều epoch hơn để có được 1 local minimum tốt. Nhưng việc tính toán sẽ nhanh hơn, bộ nhớ nạp dữ liệu tốn ít hơn.</li><li>Mini-batch lớn thì đường đi thẳng về đích hơn nhưng việc tính toán đắt đỏ hơn sẽ làm quá trình huấn luyện chạy lâu hơn.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="4-hiện-tượng-imbalanced-data">4. Hiện tượng Imbalanced Data<a class="hash-link" href="#4-hiện-tượng-imbalanced-data" title="Direct link to heading">​</a></h3><p>Là hiện tượng mà tập dữ liệu của ta có các lớp phân loại có sự sai khác lớn về số lượng dữ liệu.
Ví dụ: trong chuẩn đoán bệnh ung thư, tập dữ liệu cho thấy, có <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>95</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">95\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">95%</span></span></span></span></span> được đánh nhãn là không có dấu hiệu bị ung thư, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">5%</span></span></span></span></span> còn lại là có dấu hiệu bị ung thư. Khi đó mô hình của ta sẽ rất khó khăn để đưa ra dự đoán, thậm chí trong trường hợp đưa ra dự đoán âm tính hết thì cũng đã có accuracy là <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mn>95</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\sim95\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">95%</span></span></span></span></span>. Khi này accuracy không còn là 1 metric tốt để mà đánh giá mô hình nữa, ta sẽ dùng precision, recall, hay F2-score.</p><p>Để giải quyết, dùng các kỹ thuật sau</p><ul><li>Ta có thể data augmentation lớp phân loại mà có số lượng ít dữ liệu, như crop, xoay hình, làm méo,…</li><li>Sử dụng metric phù hợp để đánh giá.</li><li>Oversampling và Undersampling.</li><li>Thay đổi hàm mất mát của ta, bằng việc tăng trọng số cho phần hàm mất mát tại các điểm ở lớp phân loại có lượng dữ liệu thấp, trừng phạt nặng nếu mô hình dự đoán sai các điểm dữ liệu đó.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="5-gradient-vanishingexploding">5. Gradient vanishing/exploding<a class="hash-link" href="#5-gradient-vanishingexploding" title="Direct link to heading">​</a></h3><p>Theo lý thuyết, mạng của ta càng sâu, thì độ chính xác trong quá trình dự đoán càng cao hơn, bởi vì khi đó mạng của ta sẽ học được nhiều đặc trưng phức tạp. Nhưng thực tế, độ chính xác của mạng sẽ bão hòa đến một mức sâu nào đó của mạng, thậm chí còn giảm khi mạng ta quá sâu. Nguyên nhân là bởi các hiện tượng gradient vanishing/exploding.</p><p><strong>Hiện tượng Gradient vanishing/exploding là gì?</strong></p><p>Quá trình khởi tạo ma trận trọng số, các giá trị thường có kỳ vọng là <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span>, độ lệch chuẩn là <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>, do vậy các giá trị đó thường nằm trong khoảng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>. Các hàm kích hoạt thường có giá trị đầu ra nằm trong khoảng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> để biểu diễn xác suất của quá trình phân loại.</p><ul><li>Gradient vanishing: hiện tượng mà khi trong quá trình thực hiện thuật toán lan truyền ngược backpropagation, gradient càng về các lớp nông hơn sẽ càng co dần về <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span>, làm cho các lớp ở đầu đó không thể cập nhật, không thể “học” được nữa. Các giá trị gradient ở tầng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> sẽ phụ thuộc vào giá trị gradient ở tầng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span>, giá trị đầu ra hàm kích hoạt tại tầng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> và giá trị ma trận trọng số tại tầng <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>. Do vậy, giá trị sẽ co về <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord"><span class="mord">0</span></span></span></span></span></span> rất nhanh vì nó bao gồm nhiều số nhỏ hơn <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> nhân với nhau.</li><li>Gradient exploding: tương tự như trên nhưng ngược lại.</li></ul><p><strong>Các phương pháp chống Gradient vanishing/exploding?</strong></p><ul><li>Khởi tạo ma trận trọng số bằng cách khởi tạo khác: Xavier initialization,…</li><li>Chọn hàm kích hoạt phù hợp: những biến thể của Relu (ELU, SELU)…</li><li>Sử dụng Batchnorm. Tuy nhiên sử dụng Batchnorm sẽ khiến mạng phức tạp hơn, tính toán lâu hơn, ta nên cân nhắc giải pháp sử dụng các hàm kích hoạt phù hợp trước khi áp dụng Batchnorm.</li><li>Gradient clipping (xử lý gradient exploding).
<img loading="lazy" alt="Gradient Clipping" src="/blogs/assets/images/gradient_clipping-0fc07e4c81169991ab51b3f2e71b6f16.PNG" width="1507" height="510" class="img_ev3q"></li></ul></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_u0Nl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blogs/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blogs/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blogs/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blogs/blog/tags/artificial-intelligence">Artificial Intelligence</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/lam1051999/blogs/edit/main/blog/2021-07-24-ai-interview-questions/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_eYIM" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><div class="margin-vert--xl"></div><div class="margin-vert--lg" style="display:flex;align-items:center;justify-content:center"><a class="button button--link" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flam1051999.github.io%2F%2Fblogs%2Fblog%2Fai-interview-questions&amp;text=I%20just%20read%20%22M%E1%BB%99t%20s%E1%BB%91%20c%C3%A2u%20h%E1%BB%8Fi%20ph%E1%BB%8Fng%20v%E1%BA%A5n%20AI%2FML%22%20by%20%40kgajera24" target="_blank" rel="noreferrer noopener" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 248 204" height="20" width="20" style="margin-right:7px"><g><path fill="#1D9BF0" d="M221.95,51.29c0.15,2.17,0.15,4.34,0.15,6.53c0,66.73-50.8,143.69-143.69,143.69v-0.04 C50.97,201.51,24.1,193.65,1,178.83c3.99,0.48,8,0.72,12.02,0.73c22.74,0.02,44.83-7.61,62.72-21.66 c-21.61-0.41-40.56-14.5-47.18-35.07c7.57,1.46,15.37,1.16,22.8-0.87C27.8,117.2,10.85,96.5,10.85,72.46c0-0.22,0-0.43,0-0.64 c7.02,3.91,14.88,6.08,22.92,6.32C11.58,63.31,4.74,33.79,18.14,10.71c25.64,31.55,63.47,50.73,104.08,52.76 c-4.07-17.54,1.49-35.92,14.61-48.25c20.34-19.12,52.33-18.14,71.45,2.19c11.31-2.23,22.15-6.38,32.07-12.26 c-3.77,11.69-11.66,21.62-22.2,27.93c10.01-1.18,19.79-3.86,29-7.95C240.37,35.29,231.83,44.14,221.95,51.29z"></path></g></svg>Share on Twitter</a></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blogs/blog/spark-cluster-docker"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Tạo 1 Standalone Spark Cluster với Docker</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blogs/blog/receptive-field"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Receptive field trong thị giác máy tính</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-bias-variance" class="table-of-contents__link toc-highlight">1. Bias, variance</a></li><li><a href="#2-batch-normalization" class="table-of-contents__link toc-highlight">2. Batch normalization</a></li><li><a href="#3-mini-batch-nhỏ-hay-lớn-ảnh-hưởng-thế-nào" class="table-of-contents__link toc-highlight">3. Mini-batch nhỏ hay lớn ảnh hưởng thế nào?</a></li><li><a href="#4-hiện-tượng-imbalanced-data" class="table-of-contents__link toc-highlight">4. Hiện tượng Imbalanced Data</a></li><li><a href="#5-gradient-vanishingexploding" class="table-of-contents__link toc-highlight">5. Gradient vanishing/exploding</a></li></ul></div></div></div></div></div><div class="footer-wrapper footer--dark"><div class="container margin-vert--lg"><div style="display:flex;justify-content:center"><div style="max-width:650px">I believe each of us was designed to do certain things, that we have certain duties. Most significantly, since nature intended us to be social creatures, we have duties to our fellow men.</div><div style="margin-left:30px;min-width:169px"><img src="/blogs/img/avatar.jpg" style="height:154px;width:139px"><p style="font-size:0.8em;margin-top:1em">Data Engineer <br>@ Giaohangtietkiem</p></div></div></div><div style="display:flex;justify-content:center"><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blogs/">About</a></li><li class="footer__item"><a class="footer__link-item" href="/blogs/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:lam1051999@gmail.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mail<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/lamtt1005" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="tel:+84962007024" target="_blank" rel="noopener noreferrer" class="footer__link-item">Phone<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/lam1051999" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Tran Lam. All content is the property of Tran Lam.</div></div></div></footer></div></div></div>
<script src="/blogs/assets/js/runtime~main.8cce032e.js"></script>
<script src="/blogs/assets/js/main.40443e59.js"></script>
</body>
</html>