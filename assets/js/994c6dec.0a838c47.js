"use strict";(self.webpackChunkkgajera_blog=self.webpackChunkkgajera_blog||[]).push([[705],{3905:(n,t,e)=>{e.d(t,{Zo:()=>p,kt:()=>g});var a=e(7294);function r(n,t,e){return t in n?Object.defineProperty(n,t,{value:e,enumerable:!0,configurable:!0,writable:!0}):n[t]=e,n}function i(n,t){var e=Object.keys(n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(n,t).enumerable}))),e.push.apply(e,a)}return e}function c(n){for(var t=1;t<arguments.length;t++){var e=null!=arguments[t]?arguments[t]:{};t%2?i(Object(e),!0).forEach((function(t){r(n,t,e[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(e)):i(Object(e)).forEach((function(t){Object.defineProperty(n,t,Object.getOwnPropertyDescriptor(e,t))}))}return n}function l(n,t){if(null==n)return{};var e,a,r=function(n,t){if(null==n)return{};var e,a,r={},i=Object.keys(n);for(a=0;a<i.length;a++)e=i[a],t.indexOf(e)>=0||(r[e]=n[e]);return r}(n,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(n);for(a=0;a<i.length;a++)e=i[a],t.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(n,e)&&(r[e]=n[e])}return r}var o=a.createContext({}),s=function(n){var t=a.useContext(o),e=t;return n&&(e="function"==typeof n?n(t):c(c({},t),n)),e},p=function(n){var t=s(n.components);return a.createElement(o.Provider,{value:t},n.children)},h="mdxType",d={inlineCode:"code",wrapper:function(n){var t=n.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(n,t){var e=n.components,r=n.mdxType,i=n.originalType,o=n.parentName,p=l(n,["components","mdxType","originalType","parentName"]),h=s(e),u=r,g=h["".concat(o,".").concat(u)]||h[u]||d[u]||i;return e?a.createElement(g,c(c({ref:t},p),{},{components:e})):a.createElement(g,c({ref:t},p))}));function g(n,t){var e=arguments,r=t&&t.mdxType;if("string"==typeof n||r){var i=e.length,c=new Array(i);c[0]=u;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=n,l[h]="string"==typeof n?n:r,c[1]=l;for(var s=2;s<i;s++)c[s]=e[s];return a.createElement.apply(null,c)}return a.createElement.apply(null,e)}u.displayName="MDXCreateElement"},9779:(n,t,e)=>{e.r(t),e.d(t,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var a=e(7462),r=(e(7294),e(3905));const i={slug:"spark-catalyst-optimizer-and-spark-session-extension",title:"Spark catalyst optimizer v\xe0 Spark session extension",authors:"tranlam",tags:["Bigdata","Spark","Apache"],image:"./images/spark-catalyst-optimizer.JPG"},c=void 0,l={permalink:"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension",editUrl:"https://github.com/lam1051999/blogs/edit/main/blog/2023-01-01-spark-catalyst-optimizer-and-spark-extension/index.md",source:"@site/blog/2023-01-01-spark-catalyst-optimizer-and-spark-extension/index.md",title:"Spark catalyst optimizer v\xe0 Spark session extension",description:"Spark catalyst optimizer n\u1eb1m trong ph\u1ea7n core c\u1ee7a Spark SQL v\u1edbi m\u1ee5c \u0111\xedch t\u1ed1i \u01b0u c\xe1c truy v\u1ea5n c\xf3 c\u1ea5u tr\xfac \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi d\u1ea1ng SQL ho\u1eb7c qua c\xe1c API DataFrame/Dataset, gi\u1ea3m thi\u1ec3u th\u1eddi gian v\xe0 chi ph\xed ch\u1ea1y c\u1ee7a \u1ee9ng d\u1ee5ng. Khi s\u1eed d\u1ee5ng Spark, th\u01b0\u1eddng m\u1ecdi ng\u01b0\u1eddi xem catalyst optimizer nh\u01b0 l\xe0 m\u1ed9t black box, khi ch\xfang ta m\u1eb7c nhi\xean cho r\u1eb1ng n\xf3 ho\u1ea1t \u0111\u1ed9ng m\u1ed9t c\xe1ch th\u1ea7n b\xed m\xe0 kh\xf4ng th\u1ef1c s\u1ef1 quan t\xe2m b\xean trong n\xf3 x\u1ea3y ra nh\u1eefng g\xec. \u1ede b\xe0i vi\u1ebft n\xe0y, m\xecnh s\u1ebd \u0111i v\xe0o t\xecm hi\u1ec3u b\xean trong logic c\u1ee7a n\xf3 th\u1ef1c s\u1ef1 th\u1ebf n\xe0o, c\xe1c th\xe0nh ph\u1ea7n, v\xe0 c\xe1ch m\xe0 Spark session extension tham gia \u0111\u1ec3 thay \u0111\u1ed5i c\xe1c plan c\u1ee7a catalyst.",date:"2023-01-01T00:00:00.000Z",formattedDate:"January 1, 2023",tags:[{label:"Bigdata",permalink:"/blogs/blog/tags/bigdata"},{label:"Spark",permalink:"/blogs/blog/tags/spark"},{label:"Apache",permalink:"/blogs/blog/tags/apache"}],readingTime:9.945,truncated:!0,authors:[{name:"Tr\u1ea7n L\xe2m",title:"Data Engineer",url:"https://github.com/lam1051999",imageURL:"https://github.com/lam1051999.png",key:"tranlam"}],frontMatter:{slug:"spark-catalyst-optimizer-and-spark-session-extension",title:"Spark catalyst optimizer v\xe0 Spark session extension",authors:"tranlam",tags:["Bigdata","Spark","Apache"],image:"./images/spark-catalyst-optimizer.JPG"},nextItem:{title:"MySQL series - Indexing",permalink:"/blogs/blog/mysql-series-mysql-indexing"}},o={image:e(642).Z,authorsImageUrls:[void 0]},s=[{value:"1. Tree v\xe0 Node",id:"1-tree-v\xe0-node",level:3},{value:"2. Rules",id:"2-rules",level:3},{value:"3. C\xe1c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Catalyst trong Spark SQL",id:"3-c\xe1c-ho\u1ea1t-\u0111\u1ed9ng-c\u1ee7a-catalyst-trong-spark-sql",level:3},{value:"3.1. Parsing v\xe0 Analyzing",id:"31-parsing-v\xe0-analyzing",level:4},{value:"3.2. Logical plan optimizations",id:"32-logical-plan-optimizations",level:4},{value:"3.3. Physical planning",id:"33-physical-planning",level:4},{value:"3.4. Code generation",id:"34-code-generation",level:4},{value:"4. Spark session extension",id:"4-spark-session-extension",level:3},{value:"5. T\xe0i li\u1ec7u tham kh\u1ea3o",id:"5-t\xe0i-li\u1ec7u-tham-kh\u1ea3o",level:3}],p={toc:s};function h(n){let{components:t,...i}=n;return(0,r.kt)("wrapper",(0,a.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Spark catalyst optimizer n\u1eb1m trong ph\u1ea7n core c\u1ee7a Spark SQL v\u1edbi m\u1ee5c \u0111\xedch t\u1ed1i \u01b0u c\xe1c truy v\u1ea5n c\xf3 c\u1ea5u tr\xfac \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi d\u1ea1ng SQL ho\u1eb7c qua c\xe1c API DataFrame/Dataset, gi\u1ea3m thi\u1ec3u th\u1eddi gian v\xe0 chi ph\xed ch\u1ea1y c\u1ee7a \u1ee9ng d\u1ee5ng. Khi s\u1eed d\u1ee5ng Spark, th\u01b0\u1eddng m\u1ecdi ng\u01b0\u1eddi xem catalyst optimizer nh\u01b0 l\xe0 m\u1ed9t black box, khi ch\xfang ta m\u1eb7c nhi\xean cho r\u1eb1ng n\xf3 ho\u1ea1t \u0111\u1ed9ng m\u1ed9t c\xe1ch th\u1ea7n b\xed m\xe0 kh\xf4ng th\u1ef1c s\u1ef1 quan t\xe2m b\xean trong n\xf3 x\u1ea3y ra nh\u1eefng g\xec. \u1ede b\xe0i vi\u1ebft n\xe0y, m\xecnh s\u1ebd \u0111i v\xe0o t\xecm hi\u1ec3u b\xean trong logic c\u1ee7a n\xf3 th\u1ef1c s\u1ef1 th\u1ebf n\xe0o, c\xe1c th\xe0nh ph\u1ea7n, v\xe0 c\xe1ch m\xe0 Spark session extension tham gia \u0111\u1ec3 thay \u0111\u1ed5i c\xe1c plan c\u1ee7a catalyst."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"spark catalyst optimizer",src:e(642).Z,width:"1280",height:"720"})),(0,r.kt)("h3",{id:"1-tree-v\xe0-node"},"1. Tree v\xe0 Node"),(0,r.kt)("p",null,"C\xe1c th\xe0nh ph\u1ea7n ch\xednh trong Catalyst \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng c\xe2y v\xe0 c\xe1c node, \u0111\u01b0\u1ee3c k\u1ebf th\u1eeba t\u1eeb class ",(0,r.kt)("inlineCode",{parentName:"p"},"TreeNode"),", ho\u1eb7c c\xe1c class con c\u1ee7a n\xf3. Class ",(0,r.kt)("inlineCode",{parentName:"p"},"TreeNode")," n\xe0y c\xf3 t\u1eadp c\xe1c node con \u1ee9ng v\u1edbi thu\u1ed9c t\xednh ",(0,r.kt)("inlineCode",{parentName:"p"},"children"),", ki\u1ec3u d\u1eef li\u1ec7u ",(0,r.kt)("inlineCode",{parentName:"p"},"Seq[BaseType]"),", do v\u1eady, m\u1ed9t ",(0,r.kt)("inlineCode",{parentName:"p"},"TreeNode")," c\xf3 th\u1ec3 c\xf3 0 ho\u1eb7c nhi\u1ec1u c\xe1c node con. C\xe1c object n\xe0y l\xe0 immutable v\xe0 \u0111\u01b0\u1ee3c thao t\xe1c b\u1eb1ng nh\u1eefng functional transformation, khi\u1ebfn cho vi\u1ec7c debug optimizer tr\u1edf n\xean d\u1ec5 d\xe0ng h\u01a1n v\xe0 c\xe1c ho\u1ea1t \u0111\u1ed9ng song song tr\u1edf n\xean d\u1ec5 \u0111o\xe1n h\u01a1n.",(0,r.kt)("br",{parentName:"p"}),"\n","Hai class quan tr\u1ecdng l\xe0 ",(0,r.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," v\xe0 ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkPlan")," \u0111\u1ec1u l\xe0 subclass c\u1ee7a ",(0,r.kt)("inlineCode",{parentName:"p"},"QueryPlan"),", class k\u1ebf th\u1eeba tr\u1ef1c ti\u1ebfp t\u1eeb ",(0,r.kt)("inlineCode",{parentName:"p"},"TreeNode"),". Trong s\u01a1 \u0111\u1ed3 Catalyst b\xean tr\xean, 3 th\xe0nh ph\u1ea7n \u0111\u1ea7u l\xe0 c\xe1c logical plans, c\xe1c node trong logical plan th\u01b0\u1eddng l\xe0 c\xe1c to\xe1n t\u1eed \u0111\u1ea1i s\u1ed1 nh\u01b0 join, and, or,... 2 th\xe0nh ph\u1ea7n \u0111\u1eb1ng sau l\xe0 c\xe1c spark plan (physical plan), c\xe1c node th\u01b0\u1eddng l\xe0 c\xe1c to\xe1n t\u1eed low-level nh\u01b0 ",(0,r.kt)("inlineCode",{parentName:"p"},"ShuffledHashJoinExec"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"SortMergeJoinExec"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"BroadcastHashJoinExec"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"FileSourceScanExec"),",... C\xe1c leaf node s\u1ebd \u0111\u1ecdc d\u1eef li\u1ec7u t\u1eeb c\xe1c source, storage, memory,... c\xf2n root node c\u1ee7a c\xe2y l\xe0 to\xe1n t\u1eed ngo\xe0i c\xf9ng v\xe0 tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c t\xednh to\xe1n."),(0,r.kt)("h3",{id:"2-rules"},"2. Rules"),(0,r.kt)("p",null,"\u0110\u1ec3 thao t\xe1c tr\xean TreeNode ta s\u1eed d\u1ee5ng c\xe1c Rule, c\xe1c Rule th\u1ef1c ch\u1ea5t ch\u1ee9a c\xe1c h\xe0m bi\u1ebfn \u0111\u1ed5i t\u1eeb c\xe2y n\xe0y sang c\xe2y kh\xe1c. Th\u01b0\u1eddng c\xe1c h\xe0m n\xe0y \u0111\u01b0\u1ee3c vi\u1ebft s\u1eed d\u1ee5ng pattern matching trong scala \u0111\u1ec3 t\xecm c\xe1c matching t\u01b0\u01a1ng \u1ee9ng trong subtree c\u1ee7a n\xf3 v\xe0 thay th\u1ebf b\u1eb1ng c\xe1c c\u1ea5u tr\xfac kh\xe1c.\nC\xe1c c\xe2y cung c\u1ea5p c\xe1c h\xe0m transform c\xf3 th\u1ec3 \xe1p d\u1ee5ng pattern matching n\xe0y \u0111\u1ec3 bi\u1ebfn \u0111\u1ed5i c\xe2y nh\u01b0 ",(0,r.kt)("inlineCode",{parentName:"p"},"transform"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"transformDown"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"transformUp"),",..."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"package org.apache.spark.sql.catalyst.trees\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\n   * When `rule` does not apply to a given node it is left unchanged.\n   * Users should not expect a specific directionality. If a specific directionality is needed,\n   * transformDown or transformUp should be used.\n   *\n   * @param rule the function used to transform this nodes children\n*/\ndef transform(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformDown(rule)\n}\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\n   * When `rule` does not apply to a given node it is left unchanged.\n   * Users should not expect a specific directionality. If a specific directionality is needed,\n   * transformDown or transformUp should be used.\n   *\n   * @param rule   the function used to transform this nodes children\n   * @param cond   a Lambda expression to prune tree traversals. If `cond.apply` returns false\n   *               on a TreeNode T, skips processing T and its subtree; otherwise, processes\n   *               T and its subtree recursively.\n   * @param ruleId is a unique Id for `rule` to prune unnecessary tree traversals. When it is\n   *               UnknownRuleId, no pruning happens. Otherwise, if `rule` (with id `ruleId`)\n   *               has been marked as in effective on a TreeNode T, skips processing T and its\n   *               subtree. Do not pass it if the rule is not purely functional and reads a\n   *               varying initial state for different invocations.\n*/\ndef transformWithPruning(cond: TreePatternBits => Boolean,\nruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n: BaseType = {\n    transformDownWithPruning(cond, ruleId)(rule)\n}\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to it and all of its\n   * children (pre-order). When `rule` does not apply to a given node it is left unchanged.\n   *\n   * @param rule the function used to transform this nodes children\n*/\ndef transformDown(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformDownWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\n}\n\ndef transformDownWithPruning(cond: TreePatternBits => Boolean,\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n  : BaseType = {\n    /* More code */    \n}\n\ndef transformUp(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformUpWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\n}\n\ndef transformUpWithPruning(cond: TreePatternBits => Boolean,\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n  : BaseType = {\n    /* More code */    \n}\n\n/* ... */\n")),(0,r.kt)("p",null,"D\u01b0\u1edbi \u0111\xe2y l\xe0 v\xed d\u1ee5 \u0111\u01a1n gi\u1ea3n v\u1ec1 s\u1eed d\u1ee5ng transform v\xe0 parttern matching \u0111\u1ec3 bi\u1ebfn \u0111\u1ed5i m\u1ed9t Treenode sang Treenode kh\xe1c"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'package com.tranlam\n\nimport org.apache.spark.sql.catalyst.expressions.{Add, BinaryOperator, Expression, IntegerLiteral, Literal, Multiply, Subtract, UnaryMinus}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nobject TestTransform {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName("test_transform").setMaster("local[*]")\n    val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n    val firstExpr: Expression = UnaryMinus(Multiply(Subtract(Literal(11), Literal(2)), Subtract(Literal(9), Literal(5))))\n    val transformed: Expression = firstExpr transformDown {\n      case BinaryOperator(l, r) => Add(l, r)\n      case IntegerLiteral(i) if i > 5 => Literal(1)\n      case IntegerLiteral(i) if i < 5 => Literal(0)\n    }\n    println(firstExpr) // -((11 - 2) * (9 - 5))\n    println(transformed) // -((1 + 0) + (1 + 5))\n    spark.sql(s"SELECT ${firstExpr.sql}").show()\n    spark.sql(s"SELECT ${transformed.sql}").show()\n  }\n}\n')),(0,r.kt)("p",null,"Trong v\xed d\u1ee5 tr\xean, h\xe0m transformDown \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng, n\xf3 \u0111i qua c\xe1c node c\u1ee7a 1 c\xe2y v\xe0 s\u1eed d\u1ee5ng parttern matching \u0111\u1ec3 tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 kh\xe1c. N\u1ebfu nh\u01b0 node \u0111\xf3 l\xe0 d\u1ea1ng binary operator nh\u01b0 Multiply, Subtract, n\xf3 s\u1ebd bi\u1ebfn \u0111\u1ed5i th\xe0nh ph\xe9p c\u1ed9ng Add. N\u1ebfu node l\xe0 h\u1eb1ng s\u1ed1 nguy\xean l\u1edbn h\u01a1n 5, n\xf3 s\u1ebd bi\u1ebfn \u0111\u1ed5i v\u1ec1 1, h\u1eb1ng s\u1ed1 b\xe9 h\u01a1n 5 s\u1ebd bi\u1ebfn \u0111\u1ed5i th\xe0nh 0, h\u1eb1ng s\u1ed1 b\u1eb1ng 5 th\xec gi\u1eef nguy\xean gi\xe1 tr\u1ecb."),(0,r.kt)("h3",{id:"3-c\xe1c-ho\u1ea1t-\u0111\u1ed9ng-c\u1ee7a-catalyst-trong-spark-sql"},"3. C\xe1c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Catalyst trong Spark SQL"),(0,r.kt)("p",null,"Spark Catalyst s\u1eed d\u1ee5ng c\xe1c ph\xe9p bi\u1ebfn \u0111\u1ed5i c\xe2y trong 4 phase ch\xednh: (1) ph\xe2n t\xedch logical plan \u0111\u1ec3 duy\u1ec7t c\xe1c relation trong plan \u0111\xf3, (2) logical plan optimization, (3) physical planning, (4) code generation \u0111\u1ec3 compile c\xe1c query th\xe0nh Java bytecode. "),(0,r.kt)("h4",{id:"31-parsing-v\xe0-analyzing"},"3.1. Parsing v\xe0 Analyzing"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"spark catalyst parseing analyzing",src:e(8238).Z,width:"642",height:"519"})),(0,r.kt)("p",null,"\u1ede phase n\xe0y, c\xe1c Catalyst rule v\xe0 Catalog object s\u1ebd \u0111\u01b0\u1ee3c Spark SQL s\u1eed d\u1ee5ng \u0111\u1ec3 ki\u1ec3m tra xem c\xe1c relation trong c\xe2u query c\u1ee7a ch\xfang ta c\xf3 t\u1ed3n t\u1ea1i hay kh\xf4ng, c\xe1c thu\u1ed9c t\xednh c\u1ee7a relation nh\u01b0 c\u1ed9t, t\xean c\u1ed9t c\u0169ng \u0111\u01b0\u1ee3c ki\u1ec3m tra n\xf3 c\xf3 chu\u1ea9n hay kh\xf4ng, syntax \u0111\xe3 \u0111\xfang ch\u01b0a v\xe0 resolve c\xe1c relation \u0111\xf3."),(0,r.kt)("p",null,'V\xed d\u1ee5, nh\xecn v\xe0o plan c\xe2u query d\u01b0\u1edbi \u0111\xe2y, \u0111\u1ea7u ti\xean Spark SQL s\u1ebd bi\u1ebfn \u0111\u1ed5i query v\u1ec1 m\u1ed9t parsed tree \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 "unresolved logical plan" v\u1edbi c\xe1c thu\u1ed9c t\xednh v\xe0 datatypes ch\u01b0a x\xe1c \u0111\u1ecbnh, ch\u01b0a \u0111\u01b0\u1ee3c g\xe1n v\u1edbi m\u1ed9t table (ho\u1eb7c alias) c\u1ee5 th\u1ec3 n\xe0o. Sau \u0111\xf3 n\xf3 s\u1ebd'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"T\xecm ki\u1ebfm relation theo t\xean t\u1eeb Catalog object."),(0,r.kt)("li",{parentName:"ul"},"Mapping c\xe1c thu\u1ed9c t\xednh nh\u01b0 c\u1ed9t c\u1ee7a input v\u1edbi c\xe1c relation \u0111\xe3 t\xecm \u0111\u01b0\u1ee3c."),(0,r.kt)("li",{parentName:"ul"},"Quy\u1ebft \u0111\u1ecbnh xem c\xe1c thu\u1ed9c t\xednh n\xe0o s\u1ebd tr\u1ecf t\u1edbi c\xf9ng gi\xe1 tr\u1ecb \u0111\u1ec3 g\xe1n cho n\xf3 m\u1ed9t unique ID (ph\u1ee5c v\u1ee5 m\u1ee5c \u0111\xedch v\u1ec1 sau \u0111\u1ec3 t\u1ed1i \u01b0u c\xe1c expressions nh\u01b0 ",(0,r.kt)("inlineCode",{parentName:"li"},"col = col"),")."),(0,r.kt)("li",{parentName:"ul"},"Cast c\xe1c expression v\u1ec1 datatype c\u1ee5 th\u1ec3 (v\xed d\u1ee5, ch\xfang ta s\u1ebd kh\xf4ng bi\u1ebft datatype tr\u1ea3 v\u1ec1 c\u1ee7a ",(0,r.kt)("inlineCode",{parentName:"li"},"col * 2")," cho t\u1edbi khi col \u0111\u01b0\u1ee3c resolved v\xe0 \u0111\u01b0\u1ee3c x\xe1c \u0111\u1ecbnh datatype).")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM test.describe_abc;\n\n== Parsed Logical Plan ==\n'Project [*]\n+- 'UnresolvedRelation [test, describe_abc], [], false\n\n== Analyzed Logical Plan ==\nid: int, name: string\nProject [id#5833, name#5834]\n+- SubqueryAlias spark_catalog.test.describe_abc\n   +- Relation test.describe_abc[id#5833,name#5834] parquet\n\n== Optimized Logical Plan ==\nRelation test.describe_abc[id#5833,name#5834] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet test.describe_abc[id#5833,name#5834] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://bigdataha/user/hive/warehouse/test.db/describe_abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string>\n")),(0,r.kt)("h4",{id:"32-logical-plan-optimizations"},"3.2. Logical plan optimizations"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"spark LP optimization",src:e(8822).Z,width:"642",height:"519"})),(0,r.kt)("p",null,"Catalyst \xe1p d\u1ee5ng c\xe1c standard optimization rule cho logical plan \u0111\u01b0\u1ee3c \u0111\u01b0\u1ee3c ph\xe2n t\xedch \u1edf b\u01b0\u1edbc tr\xean, v\u1edbi c\xe1c d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cache. \u1ede \u0111\xe2y, cost-based optimization \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 sinh ra nhi\u1ec1u plans, v\xe0 sau \u0111\xf3 t\xednh to\xe1n cost cho t\u1eebng plan. Ph\u1ea7n n\xe0y bao g\u1ed3m c\xe1c rule nh\u01b0 "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Constant folding: lo\u1ea1i b\u1ecf c\xe1c expression t\xednh to\xe1n m\u1ed9t gi\xe1 tr\u1ecb m\xe0 c\xf3 ta c\xf3 th\u1ec3 x\xe1c \u0111\u1ecbnh tr\u01b0\u1edbc khi code ch\u1ea1y, v\xed d\u1ee5 nh\u01b0 ",(0,r.kt)("inlineCode",{parentName:"li"},"y = x * 2 * 2"),", compiler s\u1ebd kh\xf4ng sinh ra 2 multiply instruction m\xe0 n\xf3 s\u1ebd thay th\u1ebf tr\u01b0\u1edbc c\xe1c gi\xe1 tr\u1ecb c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c t\xednh to\xe1n ",(0,r.kt)("inlineCode",{parentName:"li"},"y = x * 4"),"."),(0,r.kt)("li",{parentName:"ul"},"Predicate pushdown: push down c\xe1c ph\u1ea7n c\u1ee7a query t\u1edbi n\u01a1i d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef, filter l\u01b0\u1ee3ng l\u1edbn d\u1eef li\u1ec7u, c\u1ea3i thi\u1ec7n network traffic."),(0,r.kt)("li",{parentName:"ul"},"Projection: ch\u1ecdn \u0111\xfang c\xe1c c\u1ed9t \u0111\u01b0\u1ee3c select, s\u1ed1 c\u1ed9t \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb storage t\u1edbi Spark \xedt h\u01a1n, ph\u1ee5c v\u1ee5 \u0111\u1ecdc c\xe1c columnar storage nhanh h\u01a1n v\xe0 ch\u1ec9 \u0111\u1ecdc c\xe1c c\u1ed9t c\u1ea7n thi\u1ebft."),(0,r.kt)("li",{parentName:"ul"},"Boolean expression simplification: v\xed d\u1ee5, A and (A or B) = A(A+B) = A.A + A.B = A + A.B = A.(1+B) = A."),(0,r.kt)("li",{parentName:"ul"},"V\xe0 nhi\u1ec1u c\xe1c rule kh\xe1c... ")),(0,r.kt)("p",null,"Catalyst optimizer c\u1ee7a Spark s\u1ebd bao g\u1ed3m c\xe1c batch of rule, m\u1ed9t s\u1ed1 rule c\xf3 th\u1ec3 t\u1ed3n t\u1ea1i trong nhi\u1ec1u batch. Th\u01b0\u1eddng c\xe1c batch rule n\xe0y s\u1ebd \u0111\u01b0\u1ee3c ch\u1ea1y 1 l\u1ea7n tr\xean plan \u0111\xf3, tuy nhi\xean, c\xf3 m\u1ed9t s\u1ed1 batch s\u1ebd ch\u1ea1y l\u1eb7p \u0111i l\u1eb7p l\u1ea1i cho \u0111\u1ebfn m\u1ed9t s\u1ed1 l\u1ea7n duy\u1ec7t nh\u1ea5t \u0111\u1ecbnh."),(0,r.kt)("h4",{id:"33-physical-planning"},"3.3. Physical planning"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"spark PP planning",src:e(7621).Z,width:"642",height:"519"})),(0,r.kt)("p",null,"Spark SQL nh\u1eadn v\xe0o logical plan v\xe0 sinh ra m\u1ed9t ho\u1eb7c nhi\u1ec1u physical plan, sau \u0111\xf3 n\xf3 s\u1ebd ch\u1ecdn physical plan ph\xf9 h\u1ee3p d\u1ef1a v\xe0o c\xe1c cost model. C\xe1c cost model th\u01b0\u1eddng d\u1ef1a v\xe0o c\xe1c ch\u1ec9 s\u1ed1 th\u1ed1ng k\xea c\u1ee7a c\xe1c relation, \u0111\u1ecbnh l\u01b0\u1ee3ng c\xe1c th\u1ed1ng k\xea ch\u1ea3y v\xe0o m\u1ed9t node trong TreeNode nh\u01b0"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"K\xedch th\u01b0\u1edbc d\u1eef li\u1ec7u ch\u1ea3y v\xe0o node."),(0,r.kt)("li",{parentName:"ul"},"L\u01b0\u1ee3ng b\u1ea3n ghi t\u1eebng b\u1ea3ng."),(0,r.kt)("li",{parentName:"ul"},"C\xe1c ch\u1ec9 s\u1ed1 th\u1ed1ng k\xea li\xean quan t\u1edbi c\u1ed9t nh\u01b0: l\u01b0\u1ee3ng gi\xe1 tr\u1ecb ph\xe2n bi\u1ec7t, gi\xe1 tr\u1ecb l\u1edbn nh\u1ea5t gi\xe1 tr\u1ecb nh\u1ecf nh\u1ea5t, gi\xe1 tr\u1ecb \u0111\u1ed9 d\xe0i trung b\xecnh v\xe0 l\u1edbn nh\u1ea5t c\u1ee7a c\u1ed9t, histogram c\xe1c gi\xe1 tr\u1ecb c\u1ee7a c\u1ed9t,...")),(0,r.kt)("p",null,"M\u1ed9t s\u1ed1 h\u01b0\u1edbng ti\u1ebfp c\u1eadn c\u1ee7a Spark SQL cho ph\u1ea7n cost model n\xe0y"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Size-only approach: ch\u1ec9 s\u1eed d\u1ee5ng th\u1ed1ng k\xea v\u1ec1 k\xedch th\u01b0\u1edbc v\u1eadt l\xfd c\u1ee7a d\u1eef li\u1ec7u ch\u1ea3y v\xe0o node, c\xf3 th\u1ec3 th\xeam ch\u1ec9 s\u1ed1 s\u1ed1 b\u1ea3n ghi trong m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p."),(0,r.kt)("li",{parentName:"ul"},"Cost-based approach: th\u1ed1ng k\xea c\xe1c th\xf4ng tin li\xean quan \u0111\u1ebfn m\u1ee9c c\u1ed9t cho c\xe1c node Aggregate, Filter, Join, Project (l\u01b0u \xfd, cost-based approach ch\u1ec9 \u0111\u01b0\u1ee3c \xe1p d\u1ee5ng cho c\xe1c node lo\u1ea1i n\xe0y, v\u1edbi nh\u1eefng node lo\u1ea1i kh\xe1c, n\xf3 s\u1ebd tr\u1edf v\u1ec1 s\u1eed d\u1ee5ng size-only approach), c\u1ea3i thi\u1ec7n k\xedch th\u01b0\u1edbc v\xe0 l\u01b0\u1ee3ng b\u1ea3n ghi cho c\xe1c node \u0111\xf3.")),(0,r.kt)("p",null,"Cost-based approach \u0111\u01b0\u1ee3c ch\u1ecdn n\u1ebfu ta set ",(0,r.kt)("inlineCode",{parentName:"p"},"spark.sql.cbo.enabled=true"),". B\xean c\u1ea1nh \u0111\xf3, c\xe1c th\u1ed1ng k\xea v\u1ec1 b\u1ea3ng v\xe0 c\u1ed9t c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c thu th\u1eadp \u0111\u1ec3 Spark c\xf3 th\u1ec3 d\u1ef1a v\xe0o \u0111\xf3 t\xednh to\xe1n, b\u1eb1ng vi\u1ec7c ch\u1ea1y c\xe1c l\u1ec7nh ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html"},"ANALYZE"))),(0,r.kt)("h4",{id:"34-code-generation"},"3.4. Code generation"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"spark codegen",src:e(6392).Z,width:"642",height:"519"})),(0,r.kt)("p",null,"Sau khi \u0111\xe3 l\u1ef1a ch\u1ecdn \u0111\u01b0\u1ee3c physical plan ph\xf9 h\u1ee3p \u0111\u1ec3 ch\u1ea1y, Catalyst s\u1ebd compile m\u1ed9t c\xe2y c\xe1c plans h\u1ed7 tr\u1ee3 codegen th\xe0nh m\u1ed9t h\xe0m Java duy nh\u1ea5t, v\u1ec1 Java bytecode \u0111\u1ec3 ch\u1ea1y tr\xean driver v\xe0 c\xe1c executor. Ph\u1ea7n codegen n\xe0y c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 ch\u1ea1y r\u1ea5t nhi\u1ec1u khi m\xe0 Spark SQL th\u01b0\u1eddng ho\u1ea1t \u0111\u1ed9ng tr\xean c\xe1c in-memory dataset, vi\u1ec7c x\u1eed l\xfd d\u1eef li\u1ec7u th\u01b0\u1eddng g\u1eafn ch\u1eb7t v\u1edbi CPU. Catalyst d\u1ef1a v\xe0o m\u1ed9t t\xednh n\u0103ng c\u1ee7a Scala l\xe0 quasiquotes \u0111\u1ec3 th\u1ef1c hi\u1ec7n \u0111\u01a1n gi\u1ea3n ho\xe1 ph\u1ea7n codegen n\xe0y (quasiquotes cho ph\xe9p x\xe2y d\u1ef1ng c\xe1c abstract syntax tree (ASTs), sau \u0111\xf3 s\u1ebd input v\xe0o Scala compiler \u0111\u1ec3 t\u1ea1o ra bytecode)."),(0,r.kt)("h3",{id:"4-spark-session-extension"},"4. Spark session extension"),(0,r.kt)("p",null,"Spark extension l\xe0 ph\u1ea7n "),(0,r.kt)("h3",{id:"5-t\xe0i-li\u1ec7u-tham-kh\u1ea3o"},"5. T\xe0i li\u1ec7u tham kh\u1ea3o"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html"},"Deep Dive into Spark SQL's Catalyst Optimizer")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.unraveldata.com/resources/catalyst-analyst-a-deep-dive-into-sparks-optimizer/"},"Spark Catalyst Pipeline: A Deep Dive Into Spark\u2019s Optimizer")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html"},"Adaptive Query Execution: Speeding Up Spark SQL at Runtime")))}h.isMDXComponent=!0},8822:(n,t,e)=>{e.d(t,{Z:()=>a});const a=e.p+"assets/images/catalyst-pipeline-LP-optimization-f6ab83bcb7f4fe16849cb54ee255ac1b.PNG"},7621:(n,t,e)=>{e.d(t,{Z:()=>a});const a=e.p+"assets/images/catalyst-pipeline-PP-planning-965d236e9af5638f8e5b6a15278a4331.PNG"},6392:(n,t,e)=>{e.d(t,{Z:()=>a});const a=e.p+"assets/images/catalyst-pipeline-codegen-4303b28d4eb24d57d52bc29fca45a337.PNG"},8238:(n,t,e)=>{e.d(t,{Z:()=>a});const a=e.p+"assets/images/catalyst-pipeline-parsing-analyzing-434229ee4a24f7a81d885d239a4ea344.PNG"},642:(n,t,e)=>{e.d(t,{Z:()=>a});const a=e.p+"assets/images/spark-catalyst-optimizer-03f0f2a4cb6e0c86499aaa51ba69b065.JPG"}}]);