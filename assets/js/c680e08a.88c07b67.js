"use strict";(self.webpackChunkkgajera_blog=self.webpackChunkkgajera_blog||[]).push([[1689],{3905:(e,n,a)=>{a.d(n,{Zo:()=>m,kt:()=>k});var t=a(7294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=t.createContext({}),p=function(e){var n=t.useContext(c),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},m=function(e){var n=p(e.components);return t.createElement(c.Provider,{value:n},e.children)},l={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(a),k=r,g=u["".concat(c,".").concat(k)]||u[k]||l[k]||o;return a?t.createElement(g,i(i({ref:n},m),{},{components:a})):t.createElement(g,i({ref:n},m))}));function k(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=u;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=a[p];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}u.displayName="MDXCreateElement"},4408:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var t=a(7462),r=(a(7294),a(3905));const o={slug:"spark-kafka-docker",title:"T\u1ea1o lu\u1ed3ng streaming d\u1eef li\u1ec7u v\u1edbi Spark Streaming, Kafka, Docker",authors:"tranlam",tags:["Bigdata","Spark","Apache","Kafka","Docker"],image:"./images/architecture.PNG"},i=void 0,s={permalink:"/blogs/blog/spark-kafka-docker",editUrl:"https://github.com/lam1051999/blogs/edit/main/blog/2022-09-11-spark-kafka-docker/index.md",source:"@site/blog/2022-09-11-spark-kafka-docker/index.md",title:"T\u1ea1o lu\u1ed3ng streaming d\u1eef li\u1ec7u v\u1edbi Spark Streaming, Kafka, Docker",description:"Architecture",date:"2022-09-11T00:00:00.000Z",formattedDate:"September 11, 2022",tags:[{label:"Bigdata",permalink:"/blogs/blog/tags/bigdata"},{label:"Spark",permalink:"/blogs/blog/tags/spark"},{label:"Apache",permalink:"/blogs/blog/tags/apache"},{label:"Kafka",permalink:"/blogs/blog/tags/kafka"},{label:"Docker",permalink:"/blogs/blog/tags/docker"}],readingTime:9.13,truncated:!0,authors:[{name:"Tr\u1ea7n L\xe2m",title:"Data Engineer @ Giaohangtietkiem",url:"https://github.com/lam1051999",imageURL:"https://github.com/lam1051999.png",key:"tranlam"}],frontMatter:{slug:"spark-kafka-docker",title:"T\u1ea1o lu\u1ed3ng streaming d\u1eef li\u1ec7u v\u1edbi Spark Streaming, Kafka, Docker",authors:"tranlam",tags:["Bigdata","Spark","Apache","Kafka","Docker"],image:"./images/architecture.PNG"},nextItem:{title:"T\u1ea1o 1 Standalone Spark Cluster v\u1edbi Docker",permalink:"/blogs/blog/spark-cluster-docker"}},c={image:a(4194).Z,authorsImageUrls:[void 0]},p=[{value:"1. T\u1ed5ng quan m\xf4 h\xecnh",id:"1-t\u1ed5ng-quan-m\xf4-h\xecnh",level:3},{value:"2. D\u1ef1ng c\xe1c container c\u1ea7n thi\u1ebft tr\xean Docker",id:"2-d\u1ef1ng-c\xe1c-container-c\u1ea7n-thi\u1ebft-tr\xean-docker",level:3},{value:"2.1. T\u1ea1o Spark cluster",id:"21-t\u1ea1o-spark-cluster",level:4},{value:"2.2. Th\xeam c\xe1c container Zookeeper, Kafka, Postgres, Schema Registry",id:"22-th\xeam-c\xe1c-container-zookeeper-kafka-postgres-schema-registry",level:4},{value:"3. T\u1ea1o m\u1ed9t Kafka Producer b\u1eafn d\u1eef li\u1ec7u gi\u1ea3 b\u1eb1ng Java Faker",id:"3-t\u1ea1o-m\u1ed9t-kafka-producer-b\u1eafn-d\u1eef-li\u1ec7u-gi\u1ea3-b\u1eb1ng-java-faker",level:3},{value:"4. Submit job Spark",id:"4-submit-job-spark",level:3},{value:"4.1. C\u1ea5u h\xecnh Postgres",id:"41-c\u1ea5u-h\xecnh-postgres",level:4},{value:"4.2. C\u1ea5u h\xecnh Spark",id:"42-c\u1ea5u-h\xecnh-spark",level:4}],m={toc:p};function l(e){let{components:n,...o}=e;return(0,r.kt)("wrapper",(0,t.Z)({},m,o,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Architecture",src:a(4194).Z,width:"1920",height:"1080"})),(0,r.kt)("p",null,"Ch\xe0o c\xe1c b\u1ea1n, m\xecnh \u0111\xe3 tr\u1edf l\u1ea1i sau m\u1ed9t th\u1eddi gian kh\xe1 l\xe2u ch\u01b0a vi\u1ebft g\xec. H\xf4m nay, m\xecnh mu\u1ed1n chia s\u1ebb v\u1ec1 c\xe1ch t\u1ea1o 1 lu\u1ed3ng Spark Streaming ti\xeau th\u1ee5 d\u1eef li\u1ec7u t\u1eeb Kafka, t\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9 \u0111\u01b0\u1ee3c d\u1ef1ng tr\xean Docker."),(0,r.kt)("h3",{id:"1-t\u1ed5ng-quan-m\xf4-h\xecnh"},"1. T\u1ed5ng quan m\xf4 h\xecnh"),(0,r.kt)("p",null,"M\xf4 h\xecnh \u0111\u01b0\u1ee3c containerize b\u1edfi Docker. Bao g\u1ed3m c\xe1c th\xe0nh ph\u1ea7n sau"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Producer: l\xe0 m\u1ed9t Kafka Producer s\u1ea3n xu\u1ea5t d\u1eef li\u1ec7u fake v\u1ec1 th\xf4ng tin m\u1ed9t c\xe1 th\u1ec3 b\u1eb1ng Java Faker v\xe0 b\u1eafn c\xe1c message l\xean Kafka."),(0,r.kt)("li",{parentName:"ul"},"Kafka cluster: bao g\u1ed3m c\xe1c broker \u0111\u1ec3 l\u01b0u tr\u1eef d\u1eef li\u1ec7u v\xe0 Zookeeper \u0111\u1ec3 qu\u1ea3n l\xfd c\xe1c broker \u0111\xf3."),(0,r.kt)("li",{parentName:"ul"},"Spark cluster: l\xe0 m\u1ed9t c\u1ee5m Spark g\u1ed3m 3 nodes: 1 driver v\xe0 2 worker \u0111\u1ec3 ti\xeau th\u1ee5 d\u1eef li\u1ec7u t\u1eeb Kafka."),(0,r.kt)("li",{parentName:"ul"},"Schema Registry: cung c\u1ea5p restful interface \u0111\u1ec3 l\u01b0u tr\u1eef v\xe0 l\u1ea5y c\xe1c schema, gi\xfap cho Kafka producer v\xe0 consumer ho\u1ea1t \u0111\u1ed9ng v\u1edbi nhau theo quy chu\u1ea9n. Khi m\xe0 2 \u0111\u1ea7u s\u1ea3n xu\u1ea5t v\xe0 ti\xeau th\u1ee5 message t\u1eeb 2 \u0111\u1ea7u Kafka l\xe0 \u0111\u1ed9c l\u1eadp, b\xean \u0111\u1ea7u ti\xeau th\u1ee5 kh\xf4ng c\u1ea7n bi\u1ebft b\xean s\u1ea3n xu\u1ea5t b\u1eafn message v\u1edbi format th\u1ebf n\xe0o, th\xec Schema Registry nh\u01b0 l\xe0 trung gian \u0111\u1ec3 2 b\xean \u0111\u0103ng k\xed format message v\u1edbi nhau, tr\xe1nh l\u1ed7i h\u1ec7 th\u1ed1ng."),(0,r.kt)("li",{parentName:"ul"},"Postgres: l\xe0 database \u0111\u1ec3 cung c\u1ea5p c\xe1c c\u1ea5u h\xecnh cho app Spark Streaming v\xe0 \u1edf b\xe0i vi\u1ebft n\xe0y c\u0169ng l\xe0 n\u01a1i \u0111\u1ec3 \u0111\u1ea9y d\u1eef li\u1ec7u streaming xu\u1ed1ng. ")),(0,r.kt)("h3",{id:"2-d\u1ef1ng-c\xe1c-container-c\u1ea7n-thi\u1ebft-tr\xean-docker"},"2. D\u1ef1ng c\xe1c container c\u1ea7n thi\u1ebft tr\xean Docker"),(0,r.kt)("h4",{id:"21-t\u1ea1o-spark-cluster"},"2.1. T\u1ea1o Spark cluster"),(0,r.kt)("p",null,"Nh\u01b0 trong ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"/blogs/blog/spark-cluster-docker"},"b\xe0i vi\u1ebft tr\u01b0\u1edbc"))," m\xecnh c\xf3 vi\u1ebft v\u1ec1 c\xe1ch d\u1ef1ng c\u1ee5m Spark tr\xean Docker, \u1edf b\xe0i n\xe0y m\xecnh t\u1eadn d\u1ee5ng lu\xf4n c\u1ee5m \u0111\xf3. Tuy nhi\xean, c\xf3 m\u1ed9t ch\xfat thay \u0111\u1ed5i, l\u01b0\u1ee3c b\u1ecf \u0111i m\u1ed9t s\u1ed1 th\u1ee9 \u0111\u1ec3 ph\xf9 h\u1ee3p v\u1edbi b\xe0i vi\u1ebft n\xe0y. C\xe1c b\u1ea1n c\xf3 th\u1ec3 t\xecm \u0111\u01b0\u1ee3c script build image ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://github.com/lam1051999/spark_kafka_docker/tree/main/spark_cluster"},"t\u1ea1i \u0111\xe2y")),". Th\u1ebf l\xe0 ta \u0111\xe3 c\xf3 c\xe1c image c\u1ea7n thi\u1ebft cho Spark cluster. Sau \u0111\xe2y l\xe0 ph\u1ea7n c\u1ea5u h\xecnh c\xe1c container trong docker-compose.yml"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yml"},"  spark-master:\n    image: spark-master\n    container_name: spark-master\n    ports:\n      - 8080:8080\n      - 7077:7077\n      - 4040:4040\n    volumes:\n      - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\n  spark-worker-1:\n    image: spark-worker\n    container_name: spark-worker-1\n    environment:\n      - SPARK_WORKER_CORES=1\n      - SPARK_WORKER_MEMORY=1024m\n    ports:\n      - 18081:8081\n    volumes:\n      - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\n    depends_on:\n      - spark-master\n  spark-worker-2:\n    image: spark-worker\n    container_name: spark-worker-2\n    environment:\n      - SPARK_WORKER_CORES=1\n      - SPARK_WORKER_MEMORY=1024m\n    ports:\n      - 28081:8081\n    volumes:\n      - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\n    depends_on:\n      - spark-master\n")),(0,r.kt)("h4",{id:"22-th\xeam-c\xe1c-container-zookeeper-kafka-postgres-schema-registry"},"2.2. Th\xeam c\xe1c container Zookeeper, Kafka, Postgres, Schema Registry"),(0,r.kt)("p",null,"Ti\u1ebfp theo s\u1ebd l\xe0 l\xean c\xe1c container Zookeeper, Kafka, Postgres v\xe0 Schema Registry"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yml"},'  zookeeper:\n    image: confluentinc/cp-zookeeper:3.3.1\n    container_name: zookeeper\n    ports:\n      - "2181:2181"\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n  kafka:\n    image: confluentinc/cp-kafka:3.3.1\n    container_name: kafka\n    depends_on:\n      - zookeeper\n    ports:\n      - "29092:29092"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n  db:\n    image: postgres\n    container_name: db-postgres\n    volumes:\n      - ./data/db:/var/lib/postgresql/data\n    ports:\n      - "5432:5432"\n    environment:\n      - POSTGRES_NAME=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n\n  schema-registry:\n    image: confluentinc/cp-schema-registry:3.3.1\n    container_name: schema-registry\n    depends_on:\n      - zookeeper\n      - kafka\n    ports:\n      - "8081:8081"\n    environment:\n      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n')),(0,r.kt)("p",null,"T\u1ed5ng h\u1ee3p l\u1ea1i, ta c\xf3 m\u1ed9t file docker-compose.yml ho\xe0n ch\u1ec9nh nh\u01b0 ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://github.com/lam1051999/spark_kafka_docker/blob/main/spark_ex/docker-compose.yml"},"\u1edf \u0111\xe2y")),".\nSau \u0111\xf3, ta th\u1ef1c hi\u1ec7n kh\u1edfi \u0111\u1ed9ng c\xe1c container b\u1eb1ng"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-docker"},"docker-compose up -d\n")),(0,r.kt)("p",null,"L\u01b0u \xfd, vi\u1ec7c n\xe0y kh\u1edfi \u0111\u1ed9ng t\u1ea5t c\u1ea3 c\xe1c container c\xf9ng m\u1ed9t l\xfac, m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p Kafka v\xe0 Schema Registry s\u1ebd b\u1ecb l\u1ed7i v\xec n\xf3 ph\u1ee5 thu\u1ed9c v\xe0o Zookeeper. H\xe3y ch\u1edd container Zookeeper l\xean xong r\u1ed3i restart l\u1ea1i container Kafka v\xe0 Schema Registry."),(0,r.kt)("h3",{id:"3-t\u1ea1o-m\u1ed9t-kafka-producer-b\u1eafn-d\u1eef-li\u1ec7u-gi\u1ea3-b\u1eb1ng-java-faker"},"3. T\u1ea1o m\u1ed9t Kafka Producer b\u1eafn d\u1eef li\u1ec7u gi\u1ea3 b\u1eb1ng Java Faker"),(0,r.kt)("p",null,"Ti\u1ebfp \u0111\u1ebfn, ta t\u1ea1o m\u1ed9t Kafka Producer \u0111\u1ec3 b\u1eafn d\u1eef li\u1ec7u gi\u1ea3 b\u1eb1ng Java. Tr\u01b0\u1edbc ti\xean, ta c\u1ea7n t\u1ea1o 1 schema tr\xean Schema Registry. V\xec Schema Registry cung c\u1ea5p m\u1ed9t restful interface n\xean ta c\xf3 th\u1ec3 d\u1ec5 d\xe0ng t\u01b0\u01a1ng t\xe1c v\u1edbi n\xf3 b\u1eb1ng c\xe1c l\u1eddi g\u1ecdi GET, POST,... Schema ta s\u1eed d\u1ee5ng trong b\xe0i vi\u1ebft n\xe0y s\u1ebd c\xf3 d\u1ea1ng nh\u01b0 sau"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{"namespace": "com.cloudurable.phonebook",\n  "type": "record",\n  "name": "Employee",\n  "doc" : "Represents an Employee at a company",\n  "fields": [\n    {"name": "id", "type": "string", "doc": "The person id"},\n    {"name": "firstName", "type": "string", "doc": "The persons given name"},\n    {"name": "nickName", "type": ["null", "string"], "default" : null},\n    {"name": "lastName", "type": "string"},\n    {"name": "age",  "type": "int", "default": -1},\n    {"name": "emails", "type": "string", "doc": "The person email"},\n    {"name": "phoneNumber",  "type":\n      { "type": "record",   "name": "PhoneNumber",\n        "fields": [\n          {"name": "areaCode", "type": "string"},\n          {"name": "countryCode", "type": "string", "default" : ""},\n          {"name": "prefix", "type": "string"},\n          {"name": "number", "type": "string"}\n        ]\n      }\n    },\n    {"name": "status", "type": "string"}\n  ]\n}\n')),(0,r.kt)("p",null,"Tr\u01b0\u1edbc h\u1ebft, \u0111\u1ec3 POST schema n\xe0y l\xean Schema Registry, ta ph\u1ea3i chuy\u1ec3n schema n\xe0y v\u1ec1 d\u1ea1ng escaped json, truy c\u1eadp ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://www.freeformatter.com/json-escape.html"},"trang web n\xe0y"))," \u0111\u1ec3 chuy\u1ec3n.\nSau \u0111\xf3, d\xf9ng POST method \u0111\u1ec3 push schema l\xean nh\u01b0 sau"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-http"},'curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \\\n  --data \'{"schema": "{\\"namespace\\": \\"com.cloudurable.phonebook\\",\\"type\\": \\"record\\",\\"name\\": \\"Employee\\",\\"doc\\" : \\"Represents an Employee at a company\\",\\"fields\\": [{\\"name\\": \\"id\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person id\\"},{\\"name\\": \\"firstName\\", \\"type\\": \\"string\\", \\"doc\\": \\"The persons given name\\"},{\\"name\\": \\"nickName\\", \\"type\\": [\\"null\\", \\"string\\"], \\"default\\" : null},{\\"name\\": \\"lastName\\", \\"type\\": \\"string\\"},{\\"name\\": \\"age\\",  \\"type\\": \\"int\\", \\"default\\": -1},{\\"name\\": \\"emails\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person email\\"},{\\"name\\": \\"phoneNumber\\",  \\"type\\":{ \\"type\\": \\"record\\",   \\"name\\": \\"PhoneNumber\\",\\"fields\\": [{\\"name\\": \\"areaCode\\", \\"type\\": \\"string\\"},{\\"name\\": \\"countryCode\\", \\"type\\": \\"string\\", \\"default\\" : \\"\\"},{\\"name\\": \\"prefix\\", \\"type\\": \\"string\\"},{\\"name\\": \\"number\\", \\"type\\": \\"string\\"}]}},{\\"name\\": \\"status\\", \\"type\\": \\"string\\"}]}"}\' \\\n  http://localhost:8081/subjects/personinformation-value/versions\n')),(0,r.kt)("p",null,"Sau \u0111\xf3, GET v\u1ec1 \u0111\u1ec3 check xem schema \u0111\xe3 l\xean hay ch\u01b0a"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-http"},"curl -X GET http://localhost:8081/subjects/personinformation-value/versions/ // xem c\xe1c version\ncurl -X GET http://localhost:8081/subjects/personinformation-value/versions/1 // xem schema version 1\n")),(0,r.kt)("p",null,"Khi n\xe0y, Kafka \u0111\xe3 l\xean, schema \u0111\xe3 c\xf3 tr\xean Schema Registry, vi\u1ec7c c\xf2n l\u1ea1i l\xe0 \u0111\u1ea9y message l\xean topic \u0111\xf3. Vi\u1ebft m\u1ed9t class nh\u01b0 sau (xem \u0111\u1ea7y \u0111\u1ee7 code ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://github.com/lam1051999/spark_kafka_docker/tree/main/KafkaClient"},"t\u1ea1i \u0111\xe2y")),"), v\xe0 ch\u1ea1y th\xec d\u1eef li\u1ec7u \u0111\xe3 l\xean Kafka v\u1edbi chema \u1edf tr\xean r\u1ed3i."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'package kafkaclient;\n\nimport com.github.javafaker.Faker;\nimport io.confluent.kafka.serializers.KafkaAvroSerializerConfig;\nimport org.apache.avro.Schema;\nimport org.apache.avro.generic.GenericData;\nimport org.apache.avro.generic.GenericRecord;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport io.confluent.kafka.serializers.KafkaAvroSerializer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Properties;\n\npublic class KafkaProducerExample {\n    private final static String TOPIC = "personinformation";\n    private final static String BOOTSTRAP_SERVERS = "localhost:29092";\n    private final static String SCHEMA_REGISTRY_URL = "http://localhost:8081";\n    private final static String LOCAL_SCHEMA_PATH = "src/main/resources/person.avsc";\n    private final static Schema schema;\n\n    private final static int nPersons = 1000;\n\n    static {\n        try {\n            schema = new Schema.Parser().parse(new File(LOCAL_SCHEMA_PATH));\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    private static Producer<String, GenericRecord> createProducer(){\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());\n        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\n\n        return new KafkaProducer<>(props);\n    }\n\n    static void runProducer() {\n        final Producer<String, GenericRecord> producer = createProducer();\n        Faker faker = new Faker();\n\n        for (int i = 0; i < nPersons; i ++){\n            String id = faker.idNumber().valid();\n            String firstName = faker.name().firstName();\n            String nickName = faker.name().username();\n            String lastName = faker.name().lastName();\n            int age = faker.number().numberBetween(18, 90);\n//            ArrayList<String> emails = new ArrayList<String>();\n//            int nEmails = 3;\n//            for(int k = 0; k < nEmails; k++){\n//                emails.add(faker.internet().safeEmailAddress());\n//            }\n            String emails = faker.internet().safeEmailAddress();\n            String areaCode = String.valueOf(faker.number().numberBetween(200, 500));\n            String countryCode = String.valueOf(faker.number().numberBetween(80, 85));\n            String prefix = String.valueOf(faker.number().numberBetween(400, 600));\n            String number = String.valueOf(faker.number().numberBetween(1234, 6789));\n\n            GenericRecord phoneNumber = new GenericData.Record(schema.getField("phoneNumber").schema());\n            phoneNumber.put("areaCode", areaCode);\n            phoneNumber.put("countryCode", countryCode);\n            phoneNumber.put("prefix", prefix);\n            phoneNumber.put("number", number);\n\n            StatusEnum status = StatusEnum.getRandomStatus();\n\n            GenericRecord personInfo = new GenericData.Record(schema);\n            personInfo.put("id", id);\n            personInfo.put("firstName", firstName);\n            personInfo.put("nickName", nickName);\n            personInfo.put("lastName", lastName);\n            personInfo.put("age", age);\n            personInfo.put("emails", emails);\n            personInfo.put("phoneNumber", phoneNumber);\n            personInfo.put("status", status.toString());\n\n            ProducerRecord<String, GenericRecord> data = new ProducerRecord<String, GenericRecord>(TOPIC, String.format("%s %s %s", firstName, lastName, nickName), personInfo);\n            producer.send(data);\n            System.out.println("Send successfully!!!");\n            try {\n                Thread.sleep(2000);\n            }catch (Exception e){\n                e.printStackTrace();\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        try {\n            runProducer();\n        }catch (Exception e){\n            e.printStackTrace();\n        }\n    }\n}\n')),(0,r.kt)("p",null,"\u1ede tr\xean, m\u1ed7i 2 gi\xe2y ta s\u1ebd \u0111\u1ea9y 1 message l\xean Kafka, \u0111\u1ea9y t\u1ed5ng c\u1ed9ng 1000 message."),(0,r.kt)("h3",{id:"4-submit-job-spark"},"4. Submit job Spark"),(0,r.kt)("h4",{id:"41-c\u1ea5u-h\xecnh-postgres"},"4.1. C\u1ea5u h\xecnh Postgres"),(0,r.kt)("p",null,"Tr\u01b0\u1edbc khi c\xf3 th\u1ec3 ch\u1ea1y job, ta c\u1ea7n c\u1ea5u h\xecnh Postgres v\u1edbi c\xe1c b\u1ea3ng sau"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"B\u1ea3ng c\u1ea5u h\xecnh app Spark ch\u1ea1y")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-postgres"},'CREATE TABLE spark_launcher_config (\n    id serial primary  key,\n    "desc" varchar(1000) NULL,\n    app_name varchar(255) NULL,\n    properties text,\n    created timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\'),\n    modified timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\')\n)\n\nINSERT INTO public.spark_launcher_config\n    (id, "desc", app_name, properties, created, modified)\n    VALUES(2, \'kafka_ingest\', \'ingest_avro_from_kafka\', \'{\n    "appname": "ingest_avro_from_kafka",\n    "master": "spark://spark-master:7077",\n    "duration": "10",\n    "groupId": "ingest_avro_from_kafka",\n    "zookeeper.hosts": "zookeeper:2181",\n    "checkpoint": "./spark_checkpoint/ingest_avro_from_kafka",\n    "zookeeper.timeout": "40000",\n    "spark.sql.shuffle.partitions": "10",\n    "spark.sql.sources.partitionOverwriteMode": "dynamic",\n    "spark.sql.hive.verifyPartitionPath": "true",\n    "spark.streaming.kafka.maxRatePerPartition": 10000,\n    "_kafka_.bootstrap.servers": "kafka:9092",\n    "_kafka_.group.id": "ingest_avro_from_kafka",\n    "_kafka_.auto.offset.reset": "earliest",\n    "_kafka_.max.poll.interval.ms": 5000000,\n    "_kafka_.max.poll.records": 10000,\n    "_kafka_.schema.registry.url": "http://schema-registry:8081",\n    "_kafka_.auto.commit": "false",\n    "_kafka_.session.timeout.ms": "50000",\n    "_kafka_.heartbeat.interval.ms": "25000",\n    "_kafka_.request.timeout.ms": "50000"\n    }\', \'2022-04-12 09:35:27.511\', \'2022-04-12 09:35:27.511\');\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"B\u1ea3ng c\u1ea5u h\xecnh ti\xeau th\u1ee5 topic")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-postgres"},"CREATE TABLE spark_ingest_config (\n    id serial primary key,\n    app_name varchar(255) not null unique,\n    type varchar(255)  NULL,\n    \"order\" int NULL,\n    topic varchar(255) not null unique,\n    status int not null DEFAULT 0,\n    fields text,\n    temp_view_first varchar(255)  NULL,\n    sql_parser text,\n    prd_id varchar(255)  NULL,\n    keys varchar(255)  NULL,\n    path_hdfs varchar(255) NOT NULL,\n    table_dest varchar(255) NOT NULL,\n    impala_driver varchar(255) null DEFAULT '',\n    impala_url varchar(255) null DEFAULT '',\n    kafka_msg_type kkmt DEFAULT 'avro_flat',\n    json_schema text,\n    repartition_des int not null DEFAULT 1,\n    msg_type mst DEFAULT 'NOT_DEFINE',\n    created timestamp without time zone DEFAULT (now() at time zone 'Asia/Ho_Chi_Minh'),\n    modified timestamp without time zone DEFAULT (now() at time zone 'Asia/Ho_Chi_Minh')\n)\n\nINSERT INTO public.spark_ingest_config\n(id, app_name, \"type\", \"order\", topic, status, fields, temp_view_first, sql_parser, prd_id, keys, path_hdfs, table_dest, impala_driver, impala_url, kafka_msg_type, json_schema, repartition_des, msg_type, created, modified)\nVALUES(1, 'ingest_avro_from_kafka', 'insert', 0, 'personinformation', 1, 'firstName,\nnickName,\nlastName,\nage,\nemails,\nphoneNumber,\nstatus', 'ingest_avro_from_kafka_personinformation', 'select \n    cast(firstName as STRING) as firstName,\n    cast(nickName as STRING) as nickName,\n    cast(lastName as STRING) as lastName,\n    cast(age as INT) as age,\n    cast(emails as STRING) as emails,\n    cast(concat(phoneNumber.countryCode, \"-\", phoneNumber.areaCode, \"-\", phoneNumber.prefix, \"-\", phoneNumber.number) as STRING) as phoneNumber,\n    cast(status as STRING) as status\nfrom ingest_avro_from_kafka_personinformation', '', '', '', 'personinformation', '', '', 'avro_flat'::public.\"kkmt\", '', 1, 'NOT_DEFINE'::public.\"mst\", '2022-04-06 19:59:41.745', '2022-04-06 19:59:41.745');\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"B\u1ea3ng l\u01b0u d\u1eef li\u1ec7u streaming")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-postgres"},"create table personinformation (\n    firstName varchar(250) not null,\n    nickName varchar(250) not null,\n    lastName varchar(250) not null,\n    age integer not null,\n    emails varchar(250) not null,\n    phoneNumber varchar(250) not null,\n    status varchar(10) not null\n);\n")),(0,r.kt)("h4",{id:"42-c\u1ea5u-h\xecnh-spark"},"4.2. C\u1ea5u h\xecnh Spark"),(0,r.kt)("p",null,"Code Spark Streaming \u0111\u1ea7y \u0111\u1ee7 b\u1ea1n c\xf3 th\u1ec3 t\xecm th\u1ea5y ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://github.com/lam1051999/spark_kafka_docker/tree/main/spark_ex"},"t\u1ea1i \u0111\xe2y")),". C\xe1c b\u1ea1n compile project b\u1eb1ng vi\u1ec7c ch\u1ea1y"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"sh run.sh\n")),(0,r.kt)("p",null,"Khi m\u1ecdi container \u0111\xe3 ch\u1ea1y \u1ed5n \u0111\u1ecbnh, Kafka \u0111\xe3 c\xf3 d\u1eef li\u1ec7u, ta truy c\u1eadp v\xe0o shell c\u1ee7a container Spark master"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"docker exec -it spark-master bash\n")),(0,r.kt)("p",null,"Sau khi \u0111\xe3 v\xe0o shell, b\u1ea1n ti\u1ebfp t\u1ee5c ch\u1ea1y l\u1ec7nh d\u01b0\u1edbi \u0111\xe2y \u0111\u1ec3 submit job Spark"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$SPARK_HOME/bin/spark-submit --jars $(echo /execution_files/dependency/*.jar | tr ' ' ',') --class com.tranlam.App /execution_files/spark_ex-1.0-SNAPSHOT.jar --app-name ingest_avro_from_kafka --jdbc-url \"jdbc:postgresql://db:5432/postgres?user=postgres&password=postgres\"\n")),(0,r.kt)("p",null,"Nh\u01b0 v\u1eady l\xe0 \u0111\xe3 c\xf3 1 job Spark ti\xeau th\u1ee5 d\u1eef li\u1ec7u Kafka r\u1ed3i. Sau \u0111\xf3 b\u1ea1n truy c\u1eadp ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"http://localhost:4040/streaming"},"http://localhost:4040/streaming"))," \u0111\u1ec3 xem c\xe1c batch \u0111ang ch\u1ea1y"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Architecture",src:a(2178).Z,width:"2878",height:"1708"})),(0,r.kt)("p",null,"V\xe0o Postgres query b\u1ea3ng ",(0,r.kt)("inlineCode",{parentName:"p"},"personinformation")," ta th\u1ea5y c\xf3 d\u1eef li\u1ec7u nh\u01b0 mong mu\u1ed1n"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Postgres",src:a(2943).Z,width:"936",height:"378"})),(0,r.kt)("p",null,"Tr\xean \u0111\xe2y l\xe0 t\u1ea5t c\u1ea3 n\u1ed9i dung \u0111\u1ec3 d\u1ef1ng m\u1ed9t lu\u1ed3ng streaming c\u01a1 b\u1ea3n t\u1eeb Kafka. C\xf2n m\u1ed9t \u0111i\u1ec1u l\u01b0u \xfd n\u1eefa l\xe0 thay v\xec b\u1ea1n commit offset c\u1ee7a c\xe1c l\u1ea7n ti\xeau th\u1ee5 l\xean 1 topic c\u1ee7a Kafka nh\u01b0 trong code Spark, b\u1ea1n c\xf3 th\u1ec3 commit n\xf3 m\u1ed9t c\xe1ch th\u1ee7 c\xf4ng v\xe0o m\u1ed9t \u0111\u01b0\u1eddng d\u1eabn \u1edf Zookeeper \u0111\u1ec3 ch\u1ee7 \u0111\u1ed9ng h\u01a1n trong vi\u1ec7c ki\u1ec3m so\xe1t."),(0,r.kt)("p",null,"Code c\u1ee7a c\u1ea3 b\xe0i vi\u1ebft b\u1ea1n \u0111\u1ecdc c\xf3 th\u1ec3 t\xecm th\u1ea5y \u1edf \u0111\xe2y: ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://github.com/lam1051999/spark_kafka_docker"},"https://github.com/lam1051999/spark_kafka_docker"))))}l.isMDXComponent=!0},4194:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/architecture-77309774b4b463947f6f1f3ef0a8f0dc.PNG"},2943:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/postgres-5773b8ab0a455f7ac006cae76556a096.PNG"},2178:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/spark-ui-1ecafa9bf152aeeb9696a63ce919b27e.PNG"}}]);