"use strict";(self.webpackChunklamtran_blog=self.webpackChunklamtran_blog||[]).push([[1861],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>k});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),c=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},p=function(e){var a=c(e.components);return r.createElement(l.Provider,{value:a},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},g=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(t),g=n,k=u["".concat(l,".").concat(g)]||u[g]||h[g]||o;return t?r.createElement(k,s(s({ref:a},p),{},{components:t})):r.createElement(k,s({ref:a},p))}));function k(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,s=new Array(o);s[0]=g;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i[u]="string"==typeof e?e:n,s[1]=i;for(var c=2;c<o;c++)s[c]=t[c];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},793:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var r=t(7462),n=(t(7294),t(3905));const o={slug:"mini-spark3-authorizer-part-2/",title:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",description:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",authors:"tranlam",tags:["Bigdata","Spark","Ranger","Apache"],image:"./images/banner.PNG"},s=void 0,i={permalink:"/blog/mini-spark3-authorizer-part-2/",editUrl:"https://github.com/LTranData/blogs/edit/main/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md",source:"@site/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md",title:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",description:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",date:"2023-05-01T00:00:00.000Z",formattedDate:"May 1, 2023",tags:[{label:"Bigdata",permalink:"/blog/tags/bigdata"},{label:"Spark",permalink:"/blog/tags/spark"},{label:"Ranger",permalink:"/blog/tags/ranger"},{label:"Apache",permalink:"/blog/tags/apache"}],readingTime:5.735,truncated:!0,authors:[{name:"Lam Tran",title:"Data Engineer",url:"https://github.com/LTranData",imageURL:"https://github.com/LTranData.png",key:"tranlam"}],frontMatter:{slug:"mini-spark3-authorizer-part-2/",title:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",description:"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",authors:"tranlam",tags:["Bigdata","Spark","Ranger","Apache"],image:"./images/banner.PNG"},prevItem:{title:"State Management In React",permalink:"/blog/state-management-react/"},nextItem:{title:"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation",permalink:"/blog/mini-spark3-authorizer-part-1/"}},l={image:t(5721).Z,authorsImageUrls:[void 0]},c=[{value:"Spark installation",id:"spark-installation",level:2},{value:"Build a mini Spark Session Extension for Ranger authorization",id:"build-a-mini-spark-session-extension-for-ranger-authorization",level:2},{value:"Room for improvement",id:"room-for-improvement",level:2}],p={toc:c};function u(e){let{components:a,...o}=e;return(0,n.kt)("wrapper",(0,r.Z)({},p,o,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"In the ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blog/mini-spark3-authorizer-part-1/"},"previous blog")),", I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blog/spark-catalyst-optimizer-and-spark-session-extension/"},"Spark Catalyst Optimizer"))," in order to archive authorization in Spark SQL with Ranger."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"banner",src:t(5721).Z,width:"751",height:"286"})),(0,n.kt)("h2",{id:"spark-installation"},"Spark installation"),(0,n.kt)("p",null,"Firstly, I will install Apache Spark 3.3.0 on my local machine. It is pretty easy with a few below steps."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Get Spark build with hadoop, you can find specific version here: https://archive.apache.org/dist/spark/\ncd ~\nwget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\ntar -xvf spark-3.3.0-bin-hadoop3.tgz\ncd spark-3.3.0-bin-hadoop3\n\n# Configure some environment variables\nexport SPARK_HOME=~/spark-3.3.0-bin-hadoop3\nexport PATH=$PATH:$SPARK_HOME/bin\n\n# Check if Spark is installed properly\nspark-shell\n")),(0,n.kt)("h2",{id:"build-a-mini-spark-session-extension-for-ranger-authorization"},"Build a mini Spark Session Extension for Ranger authorization"),(0,n.kt)("p",null,"The idea to make the authorizer is first inspired by ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://github.com/yaooqinn/spark-ranger"},"this Spark Ranger repository")),". Currently, this repository has been archived and it is only compatible with Spark 2.4 and below, which does not help our use case (we use Spark 3.3.0). After weeks of trying multiple solutions but not having a result, I end up customizing the repository to work with Spark 3.3.0 myself."),(0,n.kt)("p",null,"Spark makes a huge update to migrate from 2.4 to 3.0 (see detail ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-24-to-30"},"migration guide"))," and ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://spark.apache.org/releases/spark-release-3-0-0.html"},"Spark Release 3.0.0")),") which create many new features. So we need to add more code and configure the project dependencies correctly in order not to conflict with Spark 3 dependencies (since Ranger and Spark 3 use the same library ",(0,n.kt)("inlineCode",{parentName:"p"},"jersey")," to build RESTful Web Services, Ranger uses 1.x and Spark 3 uses 2.x)."),(0,n.kt)("p",null,"The idea is to create a Spark Session Extension to customize the logical plan optimization phase of Spark Catalyst Optimizer, and since the logical plan is represented as ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blog/spark-catalyst-optimizer-and-spark-session-extension/#1-treenode"},"TreeNode")),", it can be logical commands such as ",(0,n.kt)("inlineCode",{parentName:"p"},"CreateTableCommand"),", ",(0,n.kt)("inlineCode",{parentName:"p"},"DropTableCommand"),", ",(0,n.kt)("inlineCode",{parentName:"p"},"InsertIntoHiveTable"),",... In these commands, we can extract the name of the database, table, and columns,... out of them, we collect them and check if the current user has proper access to those resources by Ranger-provided APIs."),(0,n.kt)("p",null,"The extension code can be found at: ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://github.com/LTranData/mini_spark3_authorizer"},"https://github.com/LTranData/mini_spark3_authorizer")),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Build the project\ncd ~\ngit clone https://github.com/LTranData/mini_spark3_authorizer\ncd mini_spark3_authorizer/spark3-ranger-custom\nmvn clean package\n\n# Copy output jar to Spark jars folder\ncp target/spark-ranger-1.0-SNAPSHOT.jar $SPARK_HOME/jars\n\n# Download dependency jars\ncd $SPARK_HOME/jars\nwget https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar\nwget https://repo1.maven.org/maven2/com/kstruct/gethostname4j/1.0.0/gethostname4j-1.0.0.jar\nwget https://repo1.maven.org/maven2/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar\nwget https://repo1.maven.org/maven2/org/apache/kudu/kudu-spark3_2.12/1.16.0/kudu-spark3_2.12-1.16.0.jar\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar\n")),(0,n.kt)("p",null,"After we have all the additional jars in Spark, we start to configure some properties for the extension."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Get the template configuration file\ncd $SPARK_HOME/conf\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-security.xml .\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-audit.xml .\n\n# Modify the downloaded policy directory\nvi ranger-spark-security.xml\nranger.plugin.spark.policy.cache.dir=<your_spark_home>/security/policycache\nmkdir -p $SPARK_HOME/security/policycache\n\n# Copy template policy file. The filename need to be exactly below, because in the extension code, plugin appId = ranger_customized, in Ranger Admin UI, the service we are going to create will have the name = spark_sql\ncp ~/mini_spark3_authorizer/spark3-conf/security/policycache/ranger_customized_spark_sql.json $SPARK_HOME/security/policycache\n")),(0,n.kt)("p",null,"At the moment, we go to Ranger Admin UI ",(0,n.kt)("inlineCode",{parentName:"p"},"http://localhost:6080")," with ",(0,n.kt)("inlineCode",{parentName:"p"},"user/password = admin/YourPassword@123456"),". In the ",(0,n.kt)("inlineCode",{parentName:"p"},"HIVE")," plugin folder, create a service with the below input."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"service config",src:t(1880).Z,width:"1332",height:"711"})),(0,n.kt)("p",null,"We can test if the service policy is accessible through RESTful APIs or not and if it is downloadable."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'curl -ivk -H "Content-type:application/json" -u admin:YourPassword@123456 -X GET "http://localhost:6080/service/plugins/policies/download/spark_sql"\n')),(0,n.kt)("p",null,"Add user you are going to test under ",(0,n.kt)("inlineCode",{parentName:"p"},"Settings -> Users/Groups/Roles -> Add New User"),"."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"user config",src:t(7571).Z,width:"1520",height:"812"})),(0,n.kt)("p",null,"Add policy to the service ",(0,n.kt)("inlineCode",{parentName:"p"},"spark_sql"),"."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"policy config",src:t(6833).Z,width:"1330",height:"722"})),(0,n.kt)("p",null,"We create the database and table that we need."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'# Create a table to Spark metastore\nspark-shell\n\nscala> val df = spark.read.parquet("file:///Users/tranlammacbook/mini_spark3_authorizer/jobs.parquet")\nscala> df.printSchema\nroot\n |-- job_id: string (nullable = true)\n |-- company_id: string (nullable = true)\n |-- job_name: string (nullable = true)\n |-- taglist: string (nullable = true)\n |-- location: string (nullable = true)\n |-- three_reasons: string (nullable = true)\n |-- description: string (nullable = true)\nscala> spark.sql("CREATE DATABASE test;")\nscala> df.write.mode("overwrite").format("parquet").saveAsTable("test.jobs")\n')),(0,n.kt)("p",null,"Now we can test our extension. As in the policy configuration, user ",(0,n.kt)("inlineCode",{parentName:"p"},"lamtran")," only has permission to the columns ",(0,n.kt)("inlineCode",{parentName:"p"},"job_id, company_id, job_name")," of table ",(0,n.kt)("inlineCode",{parentName:"p"},"test.jobs"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'spark-shell --conf spark.sql.extensions=mini.spark3.authorizer.RangerSparkSQLExtension --conf spark.sql.proxy-user=lamtran\n\n# Check Spark configurations\nscala> spark.conf.get("spark.sql.extensions")\nres0: String = mini.spark3.authorizer.RangerSparkSQLExtension\nscala> spark.conf.get("spark.sql.proxy-user")\nres1: String = lamtran\n\n# Check permitted columns\nscala> spark.sql("SELECT job_id, company_id, job_name FROM test.jobs LIMIT 10").show(truncate=false)\n+----------------------------------------------------+--------------+----------------------------------------+\n|job_id                                              |company_id    |job_name                                |\n+----------------------------------------------------+--------------+----------------------------------------+\n|kms-technology:jrsr_qa_engineer_kms_labs_bonus      |kms-technology|(Jr/Sr) QA Engineer, KMS Labs - BONUS   |\n|kms-technology:engineering_manager_bonus            |kms-technology|Engineering Manager - BONUS             |\n|kms-technology:fullstack_mobile_mobilenodejs_kobiton|kms-technology|Fullstack Mobile (Mobile,NodeJs) Kobiton|\n|kms-technology:jrsrprincipal_java_developer_bonus   |kms-technology|(Jr/Sr/Principal) Java Developer- BONUS |\n|kms-technology:product_manager_kms_labs_bonus       |kms-technology|Product Manager, KMS Labs - BONUS       |\n|kms-technology:sr_it_business_analyst_english_bonus |kms-technology|Sr IT Business Analyst (English) - BONUS|\n|kms-technology:fullstack_dev_reactjsnodejs_kobiton  |kms-technology|Fullstack Dev (ReactJs,NodeJs) - Kobiton|\n|kms-technology:senior_ruby_on_rails_engineer_bonus  |kms-technology|Senior Ruby on Rails Engineer - BONUS   |\n|kms-technology:senior_data_engineer_bonus           |kms-technology|Senior Data Engineer - BONUS            |\n|kms-technology:srjr_fullstack_nodejsreactjs_bonus   |kms-technology|Sr/Jr Fullstack (NodeJS,ReactJS) - BONUS|\n+----------------------------------------------------+--------------+----------------------------------------+\n\n# Check columns that are not allowed to select\nscala> spark.sql("SELECT location, taglist, description FROM test.jobs LIMIT 10").show(truncate=false)\n23/07/01 21:35:19 ERROR RangerSparkAuthorizerExtension: \n+===============================+\n|Spark SQL Authorization Failure|\n|-------------------------------|\n|Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\n|-------------------------------|\n|Spark SQL Authorization Failure|\n+===============================+\n             \norg.apache.ranger.authorization.spark.authorizer.SparkAccessControlException: Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6(RangerSparkAuthorizer.scala:127)\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6$adapted(RangerSparkAuthorizer.scala:124)\n  at scala.collection.Iterator.foreach(Iterator.scala:943)\n  at scala.collection.Iterator.foreach$(Iterator.scala:943)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3(RangerSparkAuthorizer.scala:124)\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3$adapted(RangerSparkAuthorizer.scala:107)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.checkPrivileges(RangerSparkAuthorizer.scala:107)\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:58)\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:14)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n  at scala.collection.immutable.List.foldLeft(List.scala:91)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n  at scala.collection.immutable.List.foreach(List.scala:431)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\n  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:810)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:787)\n  ... 47 elided\n')),(0,n.kt)("p",null,"Now check the downloaded policy file."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd $SPARK_HOME/security/policycache\nls -lh\n-rw-r--r--  1 tranlammacbook  staff    23K Jul  1 21:32 ranger_customized_spark_sql.json\n")),(0,n.kt)("p",null,"We will see this is the new policy file and its content is refreshed every 10 seconds when the extension is in use."),(0,n.kt)("p",null,"Now, we can check the Ranger audit logs at ",(0,n.kt)("inlineCode",{parentName:"p"},"http://localhost:6080/index.html#!/reports/audit/bigData"),"."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"ranger audit",src:t(5587).Z,width:"2010",height:"1104"})),(0,n.kt)("p",null,"To check if it is pushed in Solr or not, go to ",(0,n.kt)("inlineCode",{parentName:"p"},"http://localhost:6083/solr/#/ranger_audits/query")," and click ",(0,n.kt)("strong",{parentName:"p"},"Execute Query"),"."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"solr audit",src:t(9029).Z,width:"1994",height:"1076"})),(0,n.kt)("h2",{id:"room-for-improvement"},"Room for improvement"),(0,n.kt)("p",null,"In this blog, we integrate a local Spark 3 with a standalone Ranger, but in production, Spark is often used in corporate with a Hadoop data cluster with Kerberos authentication enabled. Ranger will also sit in that Hadoop and do the authorization for many frameworks in Hadoop. Thus, there are a few more things that we need to implement."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Config Spark and Ranger to work with Hadoop, and run Spark job in cluster mode."),(0,n.kt)("li",{parentName:"ul"},"Policy refresher needs to be secure and use SPNego protocol (which use Kerberos keytab to generate token) to make RESTful API calls to Ranger service."),(0,n.kt)("li",{parentName:"ul"},"The user of the Spark application and the user for authorizing Ranger need to be the keytab principal user.")),(0,n.kt)("p",null,"I have also successfully implemented all the above functionalities in the production environment. If you need them, contact me on ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://www.linkedin.com/in/ltrandata/"},"Linkedin")),"."))}u.isMDXComponent=!0},5721:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/banner-f7f67c4c18838de8d49230557182b09d.PNG"},6833:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/policy_config-08227c613b3427511bf003c7d59f375e.PNG"},5587:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/ranger_audit-d860c70e75b2c34bfa9d8908cc4e5676.PNG"},1880:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/service_config-da6e13cecf4aa8cbb473060f8ead94ac.PNG"},9029:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/solr_audit-6b76b5e2a88887f2d199a842d4572cfb.PNG"},7571:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/user_config-445294a579b9ef8455f30a0632dfa49c.PNG"}}]);