"use strict";(self.webpackChunklamtran_blog=self.webpackChunklamtran_blog||[]).push([[866],{4612:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"db-schemachange/","metadata":{"permalink":"/blog/db-schemachange/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2025-07-13-db-schemachange/index.md","source":"@site/blog/2025-07-13-db-schemachange/index.md","title":"db-schemachange","description":"db-schemachange","date":"2025-07-13T00:00:00.000Z","formattedDate":"July 13, 2025","tags":[{"label":"Databases","permalink":"/blog/tags/databases"},{"label":"Cloud","permalink":"/blog/tags/cloud"},{"label":"Data Platform","permalink":"/blog/tags/data-platform"},{"label":"Database Change Management","permalink":"/blog/tags/database-change-management"}],"readingTime":16.85,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"db-schemachange/","title":"db-schemachange","description":"db-schemachange","authors":"tranlam","tags":["Databases","Cloud","Data Platform","Database Change Management"],"image":"./images/flyway-naming-convention.png"},"nextItem":{"title":"Cloud Native Data Platform","permalink":"/blog/cloud-native-data-platform/"}},"content":"`db-schemachange` is a simple, lightweight python based tool to manage database objects for Databricks, Snowflake, MySQL, Postgres, SQL Server, and Oracle. It\\nfollows an Imperative-style approach to Database Change Management (DCM) and was inspired by\\nthe [Flyway database migration tool](https://flywaydb.org). When combined with a version control system and a CI/CD\\ntool, database changes can be approved and deployed through a pipeline using modern software delivery practices. As such\\nschemachange plays a critical role in enabling Database (or Data) DevOps.\\n\\n![banner image](./images/flyway-naming-convention.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Installation options\\n\\nYou can install the `db-schemachange` package with all available connectors, or you can choose a specific one that suits your needs for a lighter installation.\\n\\n```bash\\npip install --upgrade db-schemachange # Install the package WITHOUT connectors\\npip install --upgrade \\"db-schemachange[all]\\" # Install the package with all connectors\\npip install --upgrade \\"db-schemachange[postgres]\\" # Install the package with Postgres connector\\npip install --upgrade \\"db-schemachange[sqlserver]\\" # Install the package with SQL Server connector\\npip install --upgrade \\"db-schemachange[mysql]\\" # Install the package with MySQL connector\\npip install --upgrade \\"db-schemachange[oracle]\\" # Install the package with Oracle connector\\npip install --upgrade \\"db-schemachange[snowflake]\\" # Install the package with Snowflake connector\\npip install --upgrade \\"db-schemachange[databricks]\\" # Install the package with Databricks connector\\n```\\n\\n## Project Structure\\n\\n```\\n(project_root)\\n|\\n|-- folder_1\\n    |-- V1.1.1__first_change.sql\\n    |-- V1.1.2__second_change.sql\\n    |-- R__sp_add_sales.sql\\n    |-- R__fn_get_timezone.sql\\n|-- folder_2\\n    |-- folder_3\\n        |-- V1.1.3__third_change.sql\\n        |-- R__fn_sort_ascii.sql\\n```\\n\\nThe `db-schemachange` folder structure is very flexible. The `project_root` folder is specified with the `-f`\\nor `--root-folder` argument. `db-schemachange` only pays attention to the filenames, not the paths. Therefore, under\\nthe `project_root` folder you are free to arrange the change scripts any way you see fit. You can have as many\\nsubfolders (and nested subfolders) as you would like.\\n\\n## Change Scripts\\n\\n### Versioned Script Naming\\n\\nVersioned change scripts follow a similar naming convention to that used\\nby [Flyway Versioned Migrations](https://flywaydb.org/documentation/migrations#versioned-migrations). The script name\\nmust follow this pattern (image taken\\nfrom [Flyway docs](https://flywaydb.org/documentation/migrations#versioned-migrations)):\\n\\n![Flyway naming conventions](./images/flyway-naming-convention.png)\\n\\nWith the following rules for each part of the filename:\\n\\n- **Prefix**: The letter \'V\' for versioned change\\n- **Version**: A unique version number with dots or underscores separating as many number parts as you like\\n- **Separator**: \\\\_\\\\_ (two underscores)\\n- **Description**: An arbitrary description with words separated by underscores or spaces (can not include two\\n  underscores)\\n- **Suffix**: .sql or .sql.jinja\\n\\nFor example, a script name that follows this convention is: `V1.1.1__first_change.sql`. As with Flyway, the unique\\nversion string is very flexible. You just need to be consistent and always use the same convention, like 3 sets of\\nnumbers separated by periods. Here are a few valid version strings:\\n\\n- 1.1\\n- 1_1\\n- 1.2.3\\n- 1_2_3\\n\\nEvery script within a database folder must have a unique version number. `db-schemachange` will check for duplicate version\\nnumbers and throw an error if it finds any. This helps to ensure that developers who are working in parallel don\'t\\naccidentally (re-)use the same version number.\\n\\n### Repeatable Script Naming\\n\\nRepeatable change scripts follow a similar naming convention to that used\\nby [Flyway Versioned Migrations](https://flywaydb.org/documentation/concepts/migrations.html#repeatable-migrations). The\\nscript name must follow this pattern (image taken\\nfrom [Flyway docs](https://flywaydb.org/documentation/concepts/migrations.html#repeatable-migrations)):\\n\\n![Flyway naming conventions repeatable](./images/flyway-repeatable-naming-convention.png)\\n\\ne.g:\\n\\n- R\\\\_\\\\_sp_add_sales.sql\\n- R\\\\_\\\\_fn_get_timezone.sql\\n- R\\\\_\\\\_fn_sort_ascii.sql\\n\\nAll repeatable change scripts are applied each time the utility is run, if there is a change in the file.\\nRepeatable scripts could be used for maintaining code that always needs to be applied in its entirety. e.g. stores\\nprocedures, functions and view definitions etc.\\n\\nJust like Flyway, within a single migration run, repeatable scripts are always applied after all pending versioned\\nscripts have been executed. Repeatable scripts are applied in alphabetical order of their description.\\n\\n### Always Script Naming\\n\\nAlways change scripts are executed with every run of `db-schemachange`. This is an addition to the implementation\\nof [Flyway Versioned Migrations](https://flywaydb.org/documentation/concepts/migrations.html#repeatable-migrations).\\nThe script name must follow this pattern:\\n\\n`A__Some_description.sql`\\n\\ne.g.\\n\\n- A\\\\_\\\\_add_user.sql\\n- A\\\\_\\\\_assign_roles.sql\\n\\nThis type of change script is useful for an environment set up after cloning. Always scripts are applied always last.\\n\\n### Rollback Script Naming\\n\\nRollback script supports reverting database changes after a failed deployment. The script name must follow this pattern: `RB_[V<version>|R|A]__Some_description.sql`. In other words, the Rollback filename should be `RB_<scrip_name>` where `<scrip_name>` is one of the three above script types.\\n\\ne.g.\\n\\n- RB_V0.0.1\\\\_\\\\_CREATE_TABLE.SQL\\n- RB_R\\\\_\\\\_CREATE_VIEW.SQL\\n- RB_A\\\\_\\\\_ASSIGN_ROLES.SQL\\n\\n### Script Requirements\\n\\n`db-schemachange` is designed to be very lightweight and not impose too many limitations. Each change script can have any\\nnumber of SQL statements within it and must supply the necessary context, like catalog/database and schema names. `db-schemachange` will simply run the contents of each script against\\nthe target database, in the correct order. After each script, Schemachange will execute \\"reset\\" the context (catalog/database, schema) to the values used to configure the connector.\\n\\n### Using Variables in Scripts\\n\\n`db-schemachange` supports the jinja engine for a variable replacement strategy. One important use of variables is to support\\nmultiple environments (dev, test, prod) in a single database by dynamically changing the database name during\\ndeployment. To use a variable in a change script, use this syntax anywhere in the script: `{{ variable1 }}`.\\n\\nTo pass variables to `db-schemachange`, check out the [Configuration](#configuration) section below. You can either use\\nthe `--vars` command line parameter or the YAML config file `schemachange-config.yml`. For the command line version you\\ncan pass variables like this: `--vars \'{\\"variable1\\": \\"value\\", \\"variable2\\": \\"value2\\"}\'`. This parameter accepts a flat\\nJSON object formatted as a string.\\n\\n> _Nested objects and arrays don\'t make sense at this point and aren\'t supported._\\n\\n`db-schemachange` will replace any variable placeholders before running your change script code and will throw an error if it\\nfinds any variable placeholders that haven\'t been replaced.\\n\\n#### Secrets filtering\\n\\nWhile many CI/CD tools already have the capability to filter secrets, it is best that any tool also does not output\\nsecrets to the console or logs. Schemachange implements secrets filtering in a number of areas to ensure secrets are not\\nwriten to the console or logs.\\n\\nA secret is either a standard variable that has been tagged as a secret or a parameter of connection config input that considered as a secret. This is determined using a naming convention and either of the following will tag a variable/conection parameter as a secret:\\n\\n1. The name has the word `secret`, `pwd`, `passwd`, `password`, or `token` in it.\\n   ```yaml\\n   config-version: 1\\n   vars:\\n     bucket_name: S3://...... # not a secret\\n     secret_key: 567576D8E # a secret\\n   ```\\n   ```yaml\\n   password: asDqTT@!#12 # a secret\\n   credentials_provider:\\n     client_id: wq5e121f-k952-4002-942e-tt24c1tww452 # not a secret\\n     client_secret: prtpw9c03tw2lwe3c89c2054lw2025tw9842 # a secret\\n   ```\\n2. The variable is a child of a key named `secrets`.\\n   ```yaml\\n   config-version: 1\\n   vars:\\n   secrets:\\n     my_key: 567576D8E # a secret\\n   aws:\\n     bucket_name: S3://...... # not a secret\\n     secrets:\\n       encryption_key: FGDSUUEHDHJK # a secret\\n       us_east_1:\\n         encryption_key: sdsdsd # a secret\\n   ```\\n\\n### Jinja templating engine\\n\\n`db-schemachange` uses the Jinja templating engine internally and\\nsupports: [expressions](https://jinja.palletsprojects.com/en/3.0.x/templates/#expressions), [macros](https://jinja.palletsprojects.com/en/3.0.x/templates/#macros), [includes](https://jinja.palletsprojects.com/en/3.0.x/templates/#include)\\nand [template inheritance](https://jinja.palletsprojects.com/en/3.0.x/templates/#template-inheritance).\\n\\nThese files can be stored in the root-folder but `db-schemachange` also provides a separate modules\\nfolder `--modules-folder`. This allows common logic to be stored outside of the main changes scripts.\\n\\nThe Jinja auto-escaping feature is disabled in `db-schemachange`, this feature in Jinja is currently designed for where the\\noutput language is HTML/XML. So if you are using `db-schemachange` with untrusted inputs you will need to handle this within\\nyour change scripts.\\n\\n## Change History Table\\n\\n`db-schemachange` records all applied changes scripts to the change history table. By default, `db-schemachange` will attempt to\\nlog all activities to the `METADATA.[SCHEMACHANGE].CHANGE_HISTORY` table, based on the database you are using. The name and location of the change history\\ntable can be overriden via a command line argument (`-c` or `--change-history-table`) or the `schemachange-config.yml`\\nfile (`change-history-table`). The value passed to the parameter can have a one, two, or three part name (e.g. \\"\\nTABLE_NAME\\", or \\"SCHEMA_NAME.TABLE_NAME\\", or \\" DATABASE_NAME.SCHEMA_NAME.TABLE_NAME\\"). This can be used to support\\nmultiple environments (dev, test, prod).\\n\\nBy default, `db-schemachange` will not try to create the change history table, and it will fail if the table does not exist.\\nThis behavior can be altered by passing in the `--create-change-history-table` argument or adding\\n`create-change-history-table: true` to the `schemachange-config.yml` file. Even with the `--create-change-history-table`\\nparameter, `db-schemachange` will not attempt to create the database for the change history table. That must be created\\nbefore running `db-schemachange`.\\n\\nThe structure of the `CHANGE_HISTORY` table is as follows:\\n\\n| Column Name    | Type          | Example                    |\\n| -------------- | ------------- | -------------------------- |\\n| VERSION        | VARCHAR(1000) | 1.1.1                      |\\n| DESCRIPTION    | VARCHAR(1000) | First change               |\\n| SCRIPT         | VARCHAR(1000) | V1.1.1\\\\_\\\\_first_change.sql |\\n| SCRIPT_TYPE    | VARCHAR(1000) | V                          |\\n| CHECKSUM       | VARCHAR(1000) | 38e5ba03b1a6d2...          |\\n| EXECUTION_TIME | BIGINT        | 4                          |\\n| STATUS         | VARCHAR(1000) | SUCCESS                    |\\n| BATCH_ID       | VARCHAR(1000) | 38e5ba03b1a6d2...          |\\n| BATCH_STATUS   | VARCHAR(1000) | SUCCESS                    |\\n| INSTALLED_BY   | VARCHAR(1000) | DATABASE_USER              |\\n| INSTALLED_ON   | TIMESTAMP     | 2020-03-17 12:54:33.123    |\\n\\nThere is a specific BATCH_ID associated with each deployment.\\n\\nA new row will be added to this table every time a change script has been applied to the database. `db-schemachange` will use\\nthis table to identify which changes have been applied to the database and will not apply the same version more than\\nonce, with BATCH_STATUS = IN_PROGRESS.\\n\\nAfter all scripts are applied, the BATCH_STATUS will be updated to SUCCESS. If any of the scripts failed, the deployment stopped and\\nBATCH_STATUS will be set to FAILED.\\n\\nIf you are running a `rollback` command, each script was rolled back will be updated with STATUS = ROLLED_BACK.\\nAfter all scripts are reverted, the BATCH_STATUS is set to ROLLED_BACK. If any of the rollback scripts failed, the BATCH_STATUS will be set to ROLLED_BACK_FAILED.\\n\\nHere is the current schema DDL for the change history table (found in the `schemachange/cli.py` script), in case you choose to create it manually and not use the `--create-change-history-table` parameter:\\n\\n```sql\\nCREATE TABLE IF NOT EXISTS METADATA.[SCHEMACHANGE].CHANGE_HISTORY\\n(\\n    VERSION VARCHAR(1000),\\n    DESCRIPTION VARCHAR(1000),\\n    SCRIPT VARCHAR(1000),\\n    SCRIPT_TYPE VARCHAR(1000),\\n    CHECKSUM VARCHAR(1000),\\n    EXECUTION_TIME BIGINT,\\n    STATUS VARCHAR(1000),\\n    BATCH_ID VARCHAR(1000),\\n    BATCH_STATUS VARCHAR(1000),\\n    INSTALLED_BY VARCHAR(1000),\\n    INSTALLED_ON TIMESTAMP\\n)\\n```\\n\\n## Configuration\\n\\n### db-schemachange configuration\\n\\nSchemachange-specific parameters can be supplied in two different ways (in order of priority):\\n\\n1. Command Line Arguments\\n2. YAML config file\\n\\n`vars` provided via command-line argument will be merged with vars provided via YAML config.\\n\\n#### CLI usage\\n\\n##### deploy\\n\\nThis is the main command that runs the deployment process.\\n\\n```bash\\nusage: schemachange deploy [-h] \\\\\\n  [--config-folder CONFIG_FOLDER] \\\\\\n  [--config-file-name CONFIG_FILE_NAME] \\\\\\n  [-f ROOT_FOLDER] \\\\\\n  [-m MODULES_FOLDER] \\\\\\n  [--vars VARS] \\\\\\n  [--db-type DB_TYPE] \\\\\\n  [--connections-file-path CONNECTIONS_FILE_PATH] \\\\\\n  [-c CHANGE_HISTORY_TABLE] \\\\\\n  [--create-change-history-table] \\\\\\n  [--query-tag QUERY_TAG] \\\\\\n  [-v] \\\\\\n  [-ac] \\\\\\n  [--dry-run]\\n```\\n\\n| Parameter                                                            | Description                                                                                                                                                                                                            |\\n| -------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| -h, --help                                                           | Show the help message and exit                                                                                                                                                                                         |\\n| --config-folder CONFIG_FOLDER                                        | The folder to look in for the schemachange config file (the default is the current working directory)                                                                                                                  |\\n| --config-file-name CONFIG_FILE_NAME                                  | The file name of the schemachange config file. (the default is schemachange-config.yml)                                                                                                                                |\\n| -f ROOT_FOLDER, --root-folder ROOT_FOLDER                            | The root folder for the database change scripts. The default is the current directory.                                                                                                                                 |\\n| -m MODULES_FOLDER, --modules-folder MODULES_FOLDER                   | The modules folder for jinja macros and templates to be used across mutliple scripts                                                                                                                                   |\\n| --vars VARS                                                          | Define values for the variables to replaced in change scripts, given in JSON format. Vars supplied via the command line will be merged with YAML-supplied vars (e.g. \'{\\"variable1\\": \\"value1\\", \\"variable2\\": \\"value2\\"}\') |\\n| -v, --verbose                                                        | Display verbose debugging details during execution. The default is \'False\'.                                                                                                                                            |\\n| --db-type                                                            | Database type to run schemachange against. Should be one of [DATABRICKS, MYSQL, ORACLE, POSTGRES, SNOWFLAKE, SQL_SERVER]                                                                                               |\\n| --connections-file-path CONNECTIONS_FILE_PATH                        | YAML file for connection detail such as username, password, database,...                                                                                                                                               |\\n| -c CHANGE_HISTORY_TABLE, --change-history-table CHANGE_HISTORY_TABLE | Used to override the default name of the change history table (which is METADATA.[SCHEMACHANGE].CHANGE_HISTORY)                                                                                                        |\\n| --create-change-history-table                                        | Create the change history table if it does not exist. The default is \'False\'.                                                                                                                                          |\\n| -ac, --autocommit                                                    | Enable autocommit feature for DML commands. The default is \'False\'.                                                                                                                                                    |\\n| --dry-run                                                            | Run schemachange in dry run mode. The default is \'False\'.                                                                                                                                                              |\\n| --query-tag                                                          | A string to include in the QUERY_TAG that is attached to every SQL statement executed.                                                                                                                                 |\\n\\n##### render\\n\\nThis subcommand is used to render a single script to the console. It is intended to support the development and\\ntroubleshooting of script that use features from the jinja template engine.\\n\\n```bash\\nusage: schemachange render [-h] \\\\\\n  [--config-folder CONFIG_FOLDER] \\\\\\n  [-f ROOT_FOLDER] \\\\\\n  [-m MODULES_FOLDER] \\\\\\n  [--vars VARS] \\\\\\n  [-v] script\\n```\\n\\n| Parameter                                          | Description                                                                                                                               |\\n| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\\n| --config-folder CONFIG_FOLDER                      | The folder to look in for the schemachange-config.yml file (the default is the current working directory)                                 |\\n| -f ROOT_FOLDER, --root-folder ROOT_FOLDER          | The root folder for the database change scripts                                                                                           |\\n| -m MODULES_FOLDER, --modules-folder MODULES_FOLDER | The modules folder for jinja macros and templates to be used across multiple scripts                                                      |\\n| --vars VARS                                        | Define values for the variables to replaced in change scripts, given in JSON format (e.g. {\\"variable1\\": \\"value1\\", \\"variable2\\": \\"value2\\"}) |\\n| -v, --verbose                                      | Display verbose debugging details during execution (the default is False)                                                                 |\\n\\n##### rollback\\n\\nThe command is the same as the `deploy` command, plus an additional required parameter `--batch-id` for the ID of the batch that we need to revert the changes. The batch ID information is only available through CLI, not the YAML config file, since the config file is more suitable for static configurations.\\n\\n```bash\\nusage: schemachange rollback [-h] \\\\\\n  [--config-folder CONFIG_FOLDER] \\\\\\n  [--config-file-name CONFIG_FILE_NAME] \\\\\\n  [-f ROOT_FOLDER] \\\\\\n  [-m MODULES_FOLDER] \\\\\\n  [--vars VARS] \\\\\\n  [--db-type DB_TYPE] \\\\\\n  [--connections-file-path CONNECTIONS_FILE_PATH] \\\\\\n  [-c CHANGE_HISTORY_TABLE] \\\\\\n  [--create-change-history-table] \\\\\\n  [--query-tag QUERY_TAG] \\\\\\n  [-v] \\\\\\n  [-ac] \\\\\\n  [--dry-run] \\\\\\n  [--batch-id BATCH_ID]\\n```\\n\\n#### YAML config file\\n\\nBy default, Schemachange expects the YAML config file to be named `schemachange-config.yml`, located in the current\\nworking directory. The YAML file name can be overridden with the\\n`--config-file-name` [command-line argument](#cli-usage). The folder can be overridden by using the\\n`--config-folder` [command-line argument](#cli-usage)\\n\\nHere is the list of available configurations in the `schemachange-config.yml` file:\\n\\n```yaml\\n# Database type\\ndb-type: MYSQL\\n\\n# Path to connection detail file\\nconnections-file-path: null\\n\\n# The root folder for the database change scripts\\nroot-folder: \\"/path/to/folder\\"\\n\\n# The modules folder for jinja macros and templates to be used across multiple scripts.\\nmodules-folder: null\\n\\n# Used to override the default name of the change history table (the default is METADATA.SCHEMACHANGE.CHANGE_HISTORY)\\nchange-history-table: null\\n\\n# Create the change history schema and table, if they do not exist (the default is False)\\ncreate-change-history-table: false\\n\\n# Define values for the variables to replaced in change scripts. vars supplied via the command line will be merged into YAML-supplied vars\\nvars:\\n  var1: \\"value1\\"\\n  var2: \\"value2\\"\\n  secrets:\\n    var3: \\"value3\\" # This is considered a secret and will not be displayed in any output\\n\\n# Enable autocommit feature for DML commands (the default is False)\\nautocommit: false\\n\\n# Display verbose debugging details during execution (the default is False)\\nverbose: false\\n\\n# Run schemachange in dry run mode (the default is False)\\ndry-run: false\\n\\n# A string to include in the QUERY_TAG that is attached to every SQL statement executed\\nquery-tag: \\"QUERY_TAG\\"\\n```\\n\\n### connections-config.yml\\n\\nConnection detail of the database passed in `--connections-file-path` CLI parameter or `connections-file-path` property in YAML config file. Please refer to [Authentication](#authentication)\\n\\n## Authentication\\n\\nSchemachange supports the many of the authentication methods supported by the each database connector.\\nPlease see below sample YAML file to pass in `--connections-file-path` parameter\\n\\n### Databricks\\n\\nAllowed parameters in https://docs.databricks.com/aws/en/dev-tools/python-sql-connector#connection-class and few other options\\n\\n```yaml\\nserver_hostname: \\"<ws_id>.cloud.databricks.com\\"\\nhttp_path: \\"/sql/1.0/warehouse/<warehouse_id>\\"\\naccess_token: \\"<access_token>\\"\\nauth_type: \\"<oauth>\\"\\ncredentials_provider:\\n  client_id: \\"<client_id>\\"\\n  client_secret: \\"<client_secret>\\"\\npassword: <password>\\nusername: <username>\\nsession_configuration: # Spark session configuration parameters\\n  spark.sql.variable.substitute: true\\nhttp_headers:\\n  - !!python/tuple [\\"header_1\\", \\"value_1\\"]\\n  - !!python/tuple [\\"header_2\\", \\"value_2\\"]\\ncatalog: <catalog>\\nschema: <schema>\\nuse_cloud_fetch: false\\nuser_agent_entry: \\"<application_name>\\"\\nuse_inline_params: false\\noauth_client_id: \\"<oauth_client_id>\\"\\noauth_redirect_port: 443\\n```\\n\\n### MySQL\\n\\nAllowed parameters in https://dev.mysql.com/doc/connector-python/en/connector-python-connectargs.html and few other options\\n\\n```yaml\\ndatabase: \\"<database>\\"\\nuser: \\"<user>\\"\\npassword: \\"<password>\\"\\nhost: \\"<host>\\"\\nport: 3306\\n# ...others\\n```\\n\\n### Oracle\\n\\nAllowed parameters in `oracledb/connection.py::connect` of Python Oracle connector package `oracledb`\\n\\n```yaml\\nuser: \\"<user>\\"\\npassword: \\"<password>\\"\\nhost: \\"<host>\\"\\nport: \\"<port>\\"\\nservice_name: \\"<service_name>\\"\\n# ...others\\n```\\n\\n### Postgres\\n\\nAllowed parameters in https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS and few other options in `psycopg/connection.py::connect` of `psycopg` package\\n\\n```yaml\\nhost: \\"<host>\\"\\nport: \\"<port>\\"\\ndbname: \\"<dbname>\\"\\nuser: \\"<user>\\"\\npassword: \\"<password>\\"\\n# ...others\\n```\\n\\n### Snowflake\\n\\nAllowed parameters in `snowflake/connector/connection.py::DEFAULT_CONFIGURATION` of `snowflake-connector-python` package\\n\\n```yaml\\nuser: \\"<user>\\"\\npassword: \\"<password>\\"\\ndatabase: \\"<database>\\"\\nschema: \\"<schema>\\"\\nwarehouse: \\"<warehouse>\\"\\nrole: \\"<role>\\"\\naccount: \\"<account>\\"\\n# ...others\\n```\\n\\n### SQL Server\\n\\nAllowed parameters in `pymssql/_pymssql.pyi::connect` of `pymssql` package\\n\\n```yaml\\nserver: \\"<server>\\"\\nuser: \\"<user>\\"\\npassword: \\"<password>\\"\\ndatabase: \\"<database>\\"\\n# ...others\\n```\\n\\n## Yaml Jinja support\\n\\nThe YAML config file supports the jinja templating language and has a custom function \\"env_var\\" to access environmental\\nvariables. Jinja variables are unavailable and not yet loaded since they are supplied by the YAML file. Customisation of\\nthe YAML file can only happen through values passed via environment variables.\\n\\n### env_var\\n\\nProvides access to environmental variables. The function can be used two different ways.\\n\\nReturn the value of the environmental variable if it exists, otherwise return the default value.\\n\\n```jinja\\n{{ env_var(\'<environmental_variable>\', \'default\') }}\\n```\\n\\nReturn the value of the environmental variable if it exists, otherwise raise an error.\\n\\n```jinja\\n{{ env_var(\'<environmental_variable>\') }}\\n```\\n\\n## Running schemachange\\n\\n### Prerequisites\\n\\nIn order to run schemachange you must have the following:\\n\\n- You will need to have a recent version of python 3 installed\\n- You will need to create the change history table used by schemachange in the database (\\n  see [Change History Table](#change-history-table) above for more details)\\n  - First, you will need to create a database/catalog to store your change history table (schemachange will not help you with\\n    this).\\n  - Second, you will need to create the change history schema and table. You can do this manually (\\n    see [Change History Table](#change-history-table) above for the DDL) or have schemachange create them by running\\n    it with the `--create-change-history-table` parameter (just make sure the user you\'re running\\n    schemachange with has privileges to create a schema and table in that database)\\n- You will need to create (or choose) a user account that has privileges to apply the changes in your change script\\n  - Don\'t forget that this user also needs the SELECT and INSERT privileges on the change history table\\n\\n### Running the Script\\n\\nschemachange is a single python script located at `schemachange/cli.py`. It can be executed as\\nfollows:\\n\\n```bash\\npython -m schemachange.cli [subcommand] [-h] \\\\\\n  [--config-folder CONFIG_FOLDER] \\\\\\n  [--config-file-name CONFIG_FILE_NAME] \\\\\\n  [-f ROOT_FOLDER] \\\\\\n  [-m MODULES_FOLDER] \\\\\\n  [--vars VARS] \\\\\\n  [--db-type DB_TYPE] \\\\\\n  [--connections-file-path CONNECTIONS_FILE_PATH] \\\\\\n  [-c CHANGE_HISTORY_TABLE] \\\\\\n  [--create-change-history-table] \\\\\\n  [--query-tag QUERY_TAG] \\\\\\n  [-v] \\\\\\n  [-ac] \\\\\\n  [--dry-run]\\n```\\n\\nOr if installed via `pip`, it can be executed as follows:\\n\\n```bash\\n# Build library from source\\npip install --upgrade build\\npip install --upgrade -r requirements.txt\\npython -m build\\npip install dist/db_schemachange-*-py3-none-any.whl\\n\\n# Or install via PyPI\\npip install --upgrade \\"db-schemachange[all]\\" # Install the package with all connectors\\npip install --upgrade \\"db-schemachange[databricks]\\" # Or install the package with specific connector\\n\\n# Run the command\\nschemachange [subcommand] [-h] \\\\\\n  [--config-folder CONFIG_FOLDER] \\\\\\n  [--config-file-name CONFIG_FILE_NAME] \\\\\\n  [-f ROOT_FOLDER] \\\\\\n  [-m MODULES_FOLDER] \\\\\\n  [--vars VARS] \\\\\\n  [--db-type DB_TYPE] \\\\\\n  [--connections-file-path CONNECTIONS_FILE_PATH] \\\\\\n  [-c CHANGE_HISTORY_TABLE] \\\\\\n  [--create-change-history-table] \\\\\\n  [--query-tag QUERY_TAG] \\\\\\n  [-v] \\\\\\n  [-ac] \\\\\\n  [--dry-run]\\n```\\n\\n1. Make sure you\'ve completed the [Prerequisites](#prerequisites) steps above\\n1. Get a copy of this schemachange repository (either via a clone or download)\\n1. Open a shell and change directory to your copy of the schemachange repository\\n1. Run schemachange (see [Running the Script](#running-the-script) above) with your database connection details and\\n   respective demo project as the root folder (make sure you use the full path)\\n\\n### Using Docker\\n\\n```bash\\ndocker run -it --rm \\\\\\n  --name schemachange-script \\\\\\n  -v \\"$PWD\\":/usr/src/schemachange \\\\\\n  -w /usr/src/schemachange \\\\\\n  python:3 /bin/bash -c \\"pip install --upgrade \'db-schemachange[all]\' && schemachange [subcommand] [-h] [--config-folder CONFIG_FOLDER] [--config-file-name CONFIG_FILE_NAME] [-f ROOT_FOLDER] [-m MODULES_FOLDER] [--vars VARS] [--db-type DB_TYPE] [--connections-file-path CONNECTIONS_FILE_PATH] [-c CHANGE_HISTORY_TABLE] [--create-change-history-table] [--query-tag QUERY_TAG] [-v] [-ac] [--dry-run]\\"\\n```\\n\\n## Maintainers\\n\\n- Lam Tran (@LTranData)\\n\\n## Demo\\n\\n```bash\\nschemachange deploy \\\\\\n  --verbose \\\\\\n  --vars \'{\\"secret\\":\\"abc\\"}\' \\\\\\n  --connections-file-path demo/mysql/config/connections-config.yml \\\\\\n  --config-folder demo/mysql/config \\\\\\n  --root-folder demo/mysql/scripts \\\\\\n  --db-type MYSQL \\\\\\n  --create-change-history-table\\n```\\n\\n## Links\\n\\n**[https://github.com/LTranData/db-schemachange](https://github.com/LTranData/db-schemachange)**\\n\\n**[https://pypi.org/project/db-schemachange](https://pypi.org/project/db-schemachange)**"},{"id":"cloud-native-data-platform/","metadata":{"permalink":"/blog/cloud-native-data-platform/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2025-02-20-cloud-native-data-platform/index.md","source":"@site/blog/2025-02-20-cloud-native-data-platform/index.md","title":"Cloud Native Data Platform","description":"Cloud Native Data Platform","date":"2025-02-20T00:00:00.000Z","formattedDate":"February 20, 2025","tags":[{"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"label":"Cloud","permalink":"/blog/tags/cloud"},{"label":"Data Platform","permalink":"/blog/tags/data-platform"}],"readingTime":3.91,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"cloud-native-data-platform/","title":"Cloud Native Data Platform","description":"Cloud Native Data Platform","authors":"tranlam","tags":["Kubernetes","Cloud","Data Platform"],"image":"./images/architecture.jpg"},"prevItem":{"title":"db-schemachange","permalink":"/blog/db-schemachange/"},"nextItem":{"title":"Understanding Snowflake micro-partitions","permalink":"/blog/understanding-snowflake-micro-partitions/"}},"content":"This platform leverages cloud-native technologies to build a flexible and efficient data pipeline. It supports various data ingestion, processing, and storage needs, enabling real-time and batch data analytics. The architecture is designed to handle structured, semi-structured, and unstructured data from diverse external sources.\\n\\n![banner image](./images/architecture.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Architecture\\n\\nThe platform is organized into distinct layers:\\n\\n* **Ingestion Layer:** Captures data from external sources using Debezium for streaming and Spark for batch.\\n* **Storage Layer:** Stores streaming data in Kafka and streaming/batch data in MinIO, there is a Hive metastore on top of the object storage with Delta Lake tables.\\n* **Transformation Layer:** Transforms and processes data using Spark, Trino, and dbt.\\n* **Provisioning Layer:** Expose data for multiple downstream applications through various connectors.\\n* **Service Layer:** Resource management, orchestration, data governance, and monitoring.\\n\\n## Getting Started\\n\\n```bash\\n# Create Kubernetes namespace\\nk create namespace data-platform\\n```\\n\\n### Install MinIO\\n\\n```bash\\n# Add repository\\nhelm repo add minio-operator https://operator.min.io\\nhelm search repo minio-operator\\nhelm repo update\\n\\n# Install client\\nbrew install minio/stable/mc\\n\\n# Download operator config for reference\\ncurl -sLo infra/services/minio/operator/values.yaml https://raw.githubusercontent.com/minio/operator/master/helm/operator/values.yaml\\n\\n# Download tenant config for reference\\ncurl -sLo infra/services/minio/tenant/values.yaml https://raw.githubusercontent.com/minio/operator/master/helm/tenant/values.yaml\\n# Make sure you configure `externalCertSecret` and `requestAutoCert` so that the server use Self-signed certificate instead of auto-generated certificate\\n\\n# Install server\\nmake -f scripts/minio/Makefile generate-self-signed-cert\\nmake -f scripts/minio/Makefile register-self-signed-cert\\nmake -f scripts/minio/Makefile install\\n\\n# Port forward for MinIO service and set up alias, & is to run it in background\\nk port-forward service/myminio-hl 9000 -n data-platform &\\nk port-forward service/myminio-console 9443 -n data-platform &\\n\\n# Because we are using the Self-signed certificate, hence specify flag --insecure here\\n# Alias for Tenant service\\nmc alias set myminio https://localhost:9000 minio minio123 --insecure\\n\\n# Create a bucket\\nmc mb myminio/mybucket --insecure\\nmc mb myminio/hive-warehouse --insecure\\n```\\n\\n### Install Spark cluster\\n\\n```bash\\n# Add repository\\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\\nhelm search repo spark-operator\\nhelm repo update\\n\\n# Download Spark config for reference\\ncurl -sLo infra/services/spark/values.yaml https://raw.githubusercontent.com/kubeflow/spark-operator/refs/heads/master/charts/spark-operator-chart/values.yaml\\n\\n# Install Spark Operator and build Spark application base image\\nmake -f scripts/spark/Makefile install\\nmake -f scripts/spark/Makefile build-spark-application-dockerfile\\n\\n# Running a Spark application to write file to MinIO with Delta Lake table format\\nmake -f scripts/spark/Makefile build-spark-write-minio-dockerfile\\nk apply -f pipeline/spark-write-minio/job.yaml\\n\\n# Release Docker images\\nmake -f scripts/spark/Makefile release-docker-images\\n\\n# Go to https://localhost:9443/browser/mybucket/user_data to view data files\\n```\\n\\n### Install Airflow\\n\\n```bash\\nhelm repo add apache-airflow https://airflow.apache.org\\nhelm search repo apache-airflow\\nhelm repo update\\n\\n# Download Airflow config for reference\\ncurl -sLo infra/services/airflow/operator/values.yaml https://raw.githubusercontent.com/apache/airflow/refs/heads/main/chart/values.yaml\\n\\n# Install Airflow Operator\\nmake -f scripts/airflow/Makefile build-custom-dockerfile\\nmake -f scripts/airflow/Makefile release-docker-images\\nmake -f scripts/airflow/Makefile install\\n\\n# Port forward for Airflow webserver\\nk port-forward service/airflow-operator-webserver 8080 -n data-platform &\\n```\\n\\n### Install Hive metastore\\n\\n```bash\\n# Download Postgres config for reference\\ncurl -sLo infra/services/hive/database/values.yaml https://raw.githubusercontent.com/bitnami/charts/refs/heads/main/bitnami/postgresql/values.yaml\\n\\n# Install Hive metastore\\nmake -f scripts/hive/Makefile build-metastore-custom-dockerfile\\nmake -f scripts/hive/Makefile build-schematool-custom-dockerfile\\nmake -f scripts/hive/Makefile release-docker-images\\nmake -f scripts/hive/Makefile install\\n\\n# Port forward for Hive metastore database and thrift\\nk port-forward service/hive-metastore-postgres-postgresql 5432 -n data-platform &\\nk port-forward service/hive-metastore 9083 -n data-platform &\\n\\n# export HADOOP_ROOT_LOGGER=DEBUG,console && hadoop fs -ls s3a://hive-warehouse/\\n# hadoop org.apache.hadoop.conf.Configuration\\n# hdfs getconf -confKey [key]\\n\\n# Create a Hive table to test Hive metastore\\nmake -f scripts/spark/Makefile build-spark-create-hive-table-dockerfile\\nk apply -f pipeline/spark-create-hive-table/job.yaml\\n```\\n\\n### Install Kafka\\n\\n```bash\\n# Download Kafka config for reference\\ncurl -sLo infra/services/kafka/operator/values.yaml https://raw.githubusercontent.com/bitnami/charts/refs/heads/main/bitnami/kafka/values.yaml\\n\\n# Install Kafka\\nmake -f scripts/kafka/Makefile generate-self-signed-cert-keystore-truststore\\nmake -f scripts/kafka/Makefile register-self-signed-cert-keystore-truststore\\nmake -f scripts/kafka/Makefile install\\n\\n# Port forward for Kafka service\\nk port-forward service/kafka-operator 9092 -n data-platform &\\n\\n# Create a client pod for accessing Kafka cluster data\\nmake -f scripts/kafka/Makefile create-kafka-client-pod\\nkafka-console-producer.sh \\\\\\n    --producer.config /tmp/client.properties \\\\\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\\\\n    --topic test\\n\\n# Consume those messages\\nkafka-console-consumer.sh \\\\\\n    --consumer.config /tmp/client.properties \\\\\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\\\\n    --topic test \\\\\\n    --from-beginning\\n\\n# List Kafka topics\\nkafka-topics.sh \\\\\\n    --command-config /tmp/client.properties \\\\\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\\\\n    --list\\n```\\n\\n### Install sources\\n\\nThis is the installation of source systems with databases so that we can integrate the CDC from these to our datalake with Kafka Connector\\n\\n```bash\\n# Install Postgres source\\nmake -f scripts/sources/Makefile install-postgres\\n\\n# Port forward for Postgres source\\nk port-forward service/postgres-source-postgresql 5432 -n data-platform &\\n```\\n\\n### Install Kafka Connect\\n\\n```bash\\nmake -f scripts/kafka-connect/Makefile install\\n\\n# Port forward for Kafka Connect REST endpoint\\nk port-forward service/kafka-connect-operator-cp-kafka-connect 8083 -n data-platform &\\n\\n# List all installed plugins\\ncurl -sS localhost:8083/connector-plugins\\n\\n# Create Postgres connector\\nmake -f scripts/kafka-connect/Makefile create-postgres-connector\\nmake -f scripts/kafka-connect/Makefile get-all-connectors\\n```\\n\\n### Install Trino\\n\\n```bash\\n# Add repository\\nhelm repo add trino https://trinodb.github.io/charts\\nhelm search repo trino\\nhelm repo update\\n\\n# Download operator config for reference\\ncurl -sLo infra/services/trino/operator/values.yaml https://raw.githubusercontent.com/trinodb/charts/refs/heads/main/charts/trino/values.yaml\\n\\n# Install Trino\\nmake -f scripts/trino/Makefile build-trino-custom-dockerfile\\nmake -f scripts/trino/Makefile release-docker-images\\nmake -f scripts/trino/Makefile install\\n\\n# Port forward for Trino\\nk port-forward service/trino-operator-trino 8089:8080 -n data-platform &\\n```\\n\\n## GitHub\\n\\n**[https://github.com/LTranData/data-platform](https://github.com/LTranData/data-platform)**"},{"id":"understanding-snowflake-micro-partitions/","metadata":{"permalink":"/blog/understanding-snowflake-micro-partitions/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2024-12-22-understanding-snowflake-micro-partitions/index.md","source":"@site/blog/2024-12-22-understanding-snowflake-micro-partitions/index.md","title":"Understanding Snowflake micro-partitions","description":"Understanding Snowflake micro-partitions","date":"2024-12-22T00:00:00.000Z","formattedDate":"December 22, 2024","tags":[{"label":"Snowflake","permalink":"/blog/tags/snowflake"},{"label":"Cloud","permalink":"/blog/tags/cloud"},{"label":"Data Platform","permalink":"/blog/tags/data-platform"}],"readingTime":6.65,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"understanding-snowflake-micro-partitions/","title":"Understanding Snowflake micro-partitions","description":"Understanding Snowflake micro-partitions","authors":"tranlam","tags":["Snowflake","Cloud","Data Platform"],"image":"./images/snowflake-micro-partitions.png"},"prevItem":{"title":"Cloud Native Data Platform","permalink":"/blog/cloud-native-data-platform/"},"nextItem":{"title":"Differences between Spark RDD, Dataframe and Dataset","permalink":"/blog/spark-rdd-dataframe-dataset/"}},"content":"Snowflake is one of the most popular data warehouse solutions nowaday because of the various features that it can provide you to build a complete data platform. Understanding the Snowflake storage layer not only helps us to have a deep dive into how it organizes the data under the hood but it is also crucial for performance optimization of your queries.\\n\\n![banner image](./images/snowflake-micro-partitions.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Partitioning in traditional Data Lake\\n\\nWhen working with traditional Data Lake solution such as Hadoop, we often organized partitions of tables in hierarchical folder structure with partition column as subfolders, data stored in files\u200b.\\n\\n![traditional partitioning](./images/traditional-partitioning.png)\\n\\nAlthough this approach is easy for visual exploration and query, there are many drawbacks\\n\\n- We need to predefine the partition columns for the table\u200b and always be mindful in table design and partitions selection\u200b at the first place. \\n\\n```sql\\nCREATE TABLE company_revenue (\\n    id INTEGER NOT NULL,\\n    revenue DECIMAL(38, 15) NOT NULL\\n) PARTITION BY (date INTEGER);\\n```\\n\\n- Overhead of maintenance: adding more partitioned columns to the table requires whole table rewrite and metadata changes, the more partitions the more metadata size and management burden,...\u200b\\n- Low performance when filtering on non-partitioned columns\u200b\\n- Suffer from the data skewness problem leading to low performance query even with partitioned data\u200b\\n- Not having the best query pushdown and pruning\u200b since there are likely only partition columns that can be pruned\\n\\n## Snowflake micro-partitions overview\\n\\nSnowflake Data Platform implements a powerful and unique form of partitioning, called micro-partitioning. Unlike traditional static partitioning schemes that require manual partition key definition, Snowflake\'s micro-partitions are automatically created during data ingestion based on natural sort order and file size optimization. This approach enables partition pruning, data clustering, and autonomous optimization without the overhead of partition management or the risk of partition skew.\\n\\n- Tables in Snowflake are divided into micro partitions automatically without overhead of manual maintenance\u200b\\n\\n![table micro partitions](./images/snowflake-micro-partitions.png)\\n\\n- Each micro partition contains 50 -> 500 MB of uncompressed data (~16 MB of compressed data), efficient for query pruning\u200b. There can be overlap between micro partitions in the value ranges it stores, so that they can be similar in size, data volume, to reduce data skewness problem\u200b\\n\\n![micro partitions size](./images/micro-partitions-size.png)\\n\\n- Data stored in columnar oriented format for efficient query projection\u200b. Snowflake decides the most efficient compression algorithm to use for each column based on the column datatype and statistics\u200b\\n\\n![columnar oriented format](./images/columnar-oriented-format.png)\\n\\nSnowflake stored metadata of each micro partition\u200b\\n- The range of values for each of the columns\u200b\\n- The number of distinct values\u200b\\n- Additional properties used for both optimization and efficient query processing\u200b\\n\\nThis information can help the query optimizer calculate the cost of each query plan and choose the best plan for processing the work units submitted to the virtual warehouse cluster.\\n\\n## How do Snowflake perform table updates?\u200b\\n\\nSnowflake micro partitions are immutable, they will not be modified once it is created. Data updates on the table will result in new micro partition creation, not modifying existing ones, with the changes applied compared with the old ones. The old micro partitions will either be destroyed immediately or remain for a certain amount of time based on the `DATA_RETENTION_TIME_IN_DAYS` parameter set on the table.\\n\\nThe immutable characteristic of micro partitions is also the reason that makes Time-Travel possible and easy to manage\u200b because of the ability to traverse back to the old version of data.\\n\\n![insert update delete](./images/insert-update-delete.png)\\n\\n- If we insert new records into the table, new micro partitions will be created\\n- If we update or delete certain records of the table, old micro partitions will be obsoleted and new micro partitions will be created with the changes applied\\n\\n## Clustering information of micro partitions\\n\\nDuring data ingestion into Snowflake tables, the platform automatically generates and maintains clustering metadata at the micro-partition level. This metadata contains statistical information about value distributions and data boundaries for each column within each micro-partition. \\n\\nThe query optimizer then leverages this fine-grained metadata to implement partition pruning, eliminating non-qualifying micro-partitions from the scan path based on query predicates.\\n\\nThe number of micro partitions that are needed to scan depends dramatically on how the micro partitions are organized. There are 2 pieces of information that can show how well the table is clustered\\n- Clustering Depth: the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table\\n- Clustering Width: The number of micro-partitions containing values that overlap with each other\\n\\n![clustering depth](./images/clustering-depth.png)\\n\\nThe smaller the clustering metrics are, the better clustered the table is with regards to the specified columns.\u200b Hence, the query that has filter/join conditions on that column will have better performance compare to the query on other columns.\\n\\nFor above picture, suppose that we have query `SELECT * FROM table_a WHERE column_a = \'h\'`, the scenario that `clustering depth = 5`, we will need to scan all 5 partitions to get the result. Otherwise, `clustering depth = 1`, we will only need to scan the partition that contains value range from `e -> j`.\\n\\nFor checking the clustering information, use [SYSTEM$CLUSTERING_INFORMATION](https://docs.snowflake.com/en/sql-reference/functions/system_clustering_information) function.\\n\\n![clustering information](./images/clustering-information.png)\\n\\nIn above case, try adding `DATE(INSERTED_TIME)` filter in every of your queries will boost the performance of READ query as a whole.\\n\\n## Clustered key and tips\u200b\\n\\nTo overcome the problem that a certain set of columns of the table have bad clustering information, Snowflake introduces the Clustering key concept. Normally, it is similar to indexes in an operational database. The clustering key in Snowflake is a subset of columns in a table (or expressions on a table) that are explicitly designated to co-locate the data in the table in the same micro-partitions\u200b.\\n\\nClustering key often provides the most benefits when\u200b\\n- Applied on the table that has infrequent DML (less re-clustering operation needed)\u200b\\n- Columns that are most actively used in selective filters\u200b\\n- Columns frequently used in join predicates\u200b\\n- Columns that have enough (medium) cardinality (number of distinct values of that column/number of records in the table) to not only enable effective pruning but also effectively group rows in the same micro-partitions\u200b\\n\\nFor the `SEGMENT_CODE` column of the previous section table, we can put the clustering key in order to re-organize the data distribution and boost the performance of the query using the filter/join conditions on that column.\\n\\nBe mindful that [Clustering key cost = 2 credits = 8$\u200b](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf).\\n\\n## Real life use case\\n\\nImagine you have a very large table in Snowflake with billions of records, you are required to remove 90% of the records from the table as part of the data archival process\\n- Easy \ud83d\ude1d just `DELETE FROM <table> WHERE <dead_condition>;\u200b`\\n- Wait \ud83e\udd14 did you forget anything important? Snowflake will try to search for the eligible micro partitions and create a new micro partition from the old one. It will destroy the old version immediately or not based on the `DATA_RETENTION_TIME_IN_DAYS` we set on the table\u200b. In the worst case, the records that need to be deleted are located in all table micro partitions then your query will scan the whole table and recreate lots of micro partitions\u200b.\\n- There need to be a better solution right? \ud83d\ude35\u200d\ud83d\udcab\u200b\\n- Wait \ud83d\ude32 query 10% records that have `<suriving_condition>` will be a lot faster right? So we can do\u200b\\n\\n```sql\\nCREATE <new_table> AS SELECT * FROM <table> WHERE <suriving_condition>;\u200b\\nALTER TABLE <table> RENAME TO <table_backup>;\u200b\\nALTER TABLE <new_table> RENAME TO <table>;\u200b\\nDROP TABLE <table_backup>;\u200b\\n```\\n\\nWith the above approach, 1st query will be much faster when it scans only 10% of the data\u200b. 2nd, 3rd, 4th are metadata queries so they are also fast\u200b. Using the trick, you will save a lot of time and Snowflake credits. Depending on the size of your data, the cost saving can be tens of times.\\n\\n## References\\n\\n[Micro-partitions & Data Clustering](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)\\n\\n[Understanding & using Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel)\\n\\n[The Impact of DML Operations with micro-partitions in Snowflake](https://www.linkedin.com/pulse/impact-dml-operations-micro-partitions-snowflake-minzhen-yang/)\\n\\n[Clustering Keys & Clustered Tables](https://docs.snowflake.com/en/user-guide/tables-clustering-keys)\\n\\n[Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf)"},{"id":"spark-rdd-dataframe-dataset/","metadata":{"permalink":"/blog/spark-rdd-dataframe-dataset/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2024-04-17-spark-rdd-dataframe-dataset/index.md","source":"@site/blog/2024-04-17-spark-rdd-dataframe-dataset/index.md","title":"Differences between Spark RDD, Dataframe and Dataset","description":"Differences between Spark RDD, Dataframe and Dataset","date":"2024-04-17T00:00:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Apache","permalink":"/blog/tags/apache"}],"readingTime":6.91,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"spark-rdd-dataframe-dataset/","title":"Differences between Spark RDD, Dataframe and Dataset","description":"Differences between Spark RDD, Dataframe and Dataset","authors":"tranlam","tags":["Bigdata","Spark","Apache"],"image":"./images/RDD-Dataframe-Dataset.svg"},"prevItem":{"title":"Understanding Snowflake micro-partitions","permalink":"/blog/understanding-snowflake-micro-partitions/"},"nextItem":{"title":"How Is Memory Managed In Spark?","permalink":"/blog/how-is-memory-managed-in-spark/"}},"content":"I have participated in fews technical interviews and have discussed with people topics around data engineering and things they have done in the past. Most of them are familiar with Apache Spark, obviously, one of the most adopted frameworks for big data processing. What I have been asked and what I often ask them is simple concepts around RDD, Dataframe, and Dataset and the differences between them. It sounds quite fundamental, right? Not really. If we have more closer look at them, there are lots of interesting things that can help us understand and choose which is the best suited for our project.\\n\\n![banner image](./images/RDD-Dataframe-Dataset.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## The overview\\n\\n<table class=\\"tg\\">\\n<thead>\\n  <tr>\\n    <th class=\\"tg-0pky\\"></th>\\n    <th class=\\"tg-0pky\\">RDD</th>\\n    <th class=\\"tg-0pky\\">Dataframe</th>\\n    <th class=\\"tg-0pky\\">Dataset</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Data representation</td>\\n    <td class=\\"tg-0pky\\">A distributed collection of data elements spread across many machines in the cluster. RDDs are a set of Java or Scala objects representing data</td>\\n    <td class=\\"tg-0pky\\">A distributed collection of data organized into named columns. It is conceptually equal to a table in a relational database</td>\\n    <td class=\\"tg-0pky\\">An extension of Dataframe API that provides the functionality of type-safe, object-oriented programming interface of the RDD API and performance benefits of the Catalyst query optimizer and off heap storage mechanism of a Dataframe API</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Data formats</td>\\n    <td class=\\"tg-0pky\\">Can easily and efficiently process data which is structured as well as unstructured. But unlike Dataframe and Dataset, RDD does not infer the schema of the ingested data and requires the user to specify it</td>\\n    <td class=\\"tg-0pky\\">It works only on structured and semi-structured data. It organizes the data in the named column. Dataframe allows the Spark to manage schema</td>\\n    <td class=\\"tg-0pky\\">It also efficiently processes structured and unstructured data. It represents data in the form of JVM objects of row or a collection of row object. Which is represented in tabular forms through encoders</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Data source APIs</td>\\n    <td class=\\"tg-0pky\\">Data source API allows that an RDD could come from any data source e.g. text file, a database via JDBC etc. and easily handle data with no predefined structure</td>\\n    <td class=\\"tg-0pky\\">Data source API allows data processing in different formats (AVRO, CSV, JSON, and storage system HDFS, HIVE tables, MySQL). It can read and write from various data sources that are mentioned above</td>\\n    <td class=\\"tg-0pky\\">Dataset API of Spark also support data from different sources</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Compile time type safety </td>\\n    <td class=\\"tg-0pky\\">RDD provides a familiar object-oriented programming style with compile-time type safety</td>\\n    <td class=\\"tg-0pky\\">If you are trying to access the column which does not exist in the table in such case Dataframe APIs does not support compile-time error. It detects attribute error only at runtime</td>\\n    <td class=\\"tg-0pky\\">It provides compile-time type safety</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Optimization</td>\\n    <td class=\\"tg-0pky\\">No inbuilt optimization engine is available in RDD. When working with structured data, RDDs cannot take advantages of Sparks advance optimizers. For example, Catalyst optimizer and Tungsten execution engine. Developers optimise each RDD on the basis of its attributes</td>\\n    <td class=\\"tg-0pky\\">Optimization takes place using catalyst optimizer which contains four phases optimization stages</td>\\n    <td class=\\"tg-0pky\\">It includes the concept of Dataframe Catalyst optimizer for optimizing query plan</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Serialization</td>\\n    <td class=\\"tg-0pky\\">Whenever Spark needs to distribute the data within the cluster or write the data to disk, it does so use Java serialization. The overhead of serializing individual Java and Scala objects is expensive and requires sending both data and structure between nodes</td>\\n    <td class=\\"tg-0pky\\">Spark Dataframe can serialize the data into off-heap storage (in memory) in binary format and then perform many transformations directly on this off heap memory because Spark understands the schema. There is no need to use java serialization to encode the data. It provides a Tungsten physical execution backend which explicitly manages memory and dynamically generates bytecode for expression evaluation</td>\\n    <td class=\\"tg-0pky\\">When it comes to serializing data, the Dataset API in Spark has the concept of an encoder which handles conversion between JVM objects to tabular representation. It stores tabular representation using Spark internal Tungsten binary format. Dataset allows performing the operation on serialized data and improving memory use. It allows on-demand access to individual attribute without deserializing the entire object</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Garbage collection</td>\\n    <td class=\\"tg-0pky\\">There is overhead for garbage collection that results from creating and destroying individual objects</td>\\n    <td class=\\"tg-0pky\\">Avoids the garbage collection costs in constructing individual objects for each row in the dataset</td>\\n    <td class=\\"tg-0pky\\">There is also no need for the garbage collector to destroy object because serialization takes place through Tungsten. That uses off heap data serialization</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Memory usage</td>\\n    <td class=\\"tg-0pky\\">Efficiency is decreased when serialization is performed individually on a java and scala object which takes lots of time</td>\\n    <td class=\\"tg-0pky\\">Use of off heap memory for serialization reduces the overhead. It generates byte code dynamically so that many operations can be performed on that serialized data. No need for deserialization for small operations</td>\\n    <td class=\\"tg-0pky\\">It allows performing an operation on serialized data and improving memory use. Thus it allows on-demand access to individual attribute without deserializing the entire object</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Schema projection</td>\\n    <td class=\\"tg-0pky\\">In RDD APIs use schema projection is used explicitly. Hence, we need to define the schema (manually)</td>\\n    <td class=\\"tg-0pky\\">Auto-discovering the schema from the files and exposing them as tables through the Hive Meta store. We did this to connect standard SQL clients to our engine. And explore our dataset without defining the schema of our files</td>\\n    <td class=\\"tg-0pky\\">Auto discover the schema of the files because of using Spark SQL engine</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Aggregation</td>\\n    <td class=\\"tg-0pky\\">RDD API is slower to perform simple grouping and aggregation operations</td>\\n    <td class=\\"tg-0pky\\">Dataframe API is very easy to use. It is faster for exploratory analysis, creating aggregated statistics on large datasets</td>\\n    <td class=\\"tg-0pky\\">Dataset is fast on performing aggregation operations on large amount of data</td>\\n  </tr>\\n  <tr>\\n    <td class=\\"tg-0pky\\">Use case</td>\\n    <td class=\\"tg-0pky\\">You need fine-grained control, low-level transformation over data operations<br/>Your data is unstructured, like text streams or media<br/>You prefer functional programming constructs for data manipulation over domain-specific functions<br/>Defining a schema (like columnar format) isn\'t important during processing<br/>You prioritize direct access to data by index or position rather than named columns\\n    </td>\\n    <td class=\\"tg-0pky\\">You want high-level data manipulation, rich semantics and powerful abstractions, making data processing more intuitive<br/>You prefer domain-specific APIs for tasks like filtering, mapping, aggregation (averages, sums), and SQL-like queries<br/>You have more efficient data access using column names (instead of indexes) and leverage columnar storage for faster processing<br/>The API remains consistent across various Spark libraries, simplifying development<br/>Your transformations are complex and need the help of Spark optimizers for improving performance</td>\\n    <td class=\\"tg-0pky\\">You prefer features from Dataframe and also higher degree of type-safety at compile time</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n**Dataset is not available in PySpark since Python is dynamically typed programming language.*\\n\\n## More about the performance comparision\\n\\nWhen PySpark interact with RDD, at the driver, `SparkContext` will use `Py4J` to launch a JVM and initiate `JavaSparkContext` and each transformed RDD will associate with `PythonRDD` objects in Java. When the tasks is distributed to worker nodes, `PythonRDD` objects run Python subprocesses using pipes, send both code and data to be processed within Python. While this approach allows PySpark to distribute the processing of the data to multiple Python subprocesses on multiple workers, the overall operation will cause a lot of context switches and communications between Java and Python, so there are more overhead to run the code and therefore it is slow when interact with RDD in Python.\\n\\n![pyspark rdd](./images/pyspark-RDD.svg)\\n\\nOn the other hand, Spark Dataframe have a significant advantage - their execution is automatically optimized by a query optimizer component.\\n\\nBefore any computations are performed on a DataFrame, the Catalyst Optimizer analyzes the sequence of operations used to construct the DataFrame. It then generates an optimized physical execution plan. The optimizer leverages its understanding of the operation semantics and the data structure to make intelligent decisions that can reorganize and optimize the execution plan, leading to more efficient computations compared to executing the operations naively. You can find more about Catalyst Optimizer in **[Spark Catalyst Optimizer And Spark Session Extension](/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md)**.\\n\\n![performance rdd dataframe](./images/performance-RDD-Dataframe.png)\\n\\n## References\\n\\n[A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets](https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\\n\\n[RDD vs. DataFrame vs. Dataset](https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset)\\n\\n[Improving PySpark Performance Beyond the JVM](https://www.slideshare.net/hkarau/improving-pyspark-performance-spark-performance-beyond-the-jvm)\\n\\n[How does PySpark work? \u2014 step by step (with pictures)](https://medium.com/analytics-vidhya/how-does-pyspark-work-step-by-step-with-pictures-c011402ccd57)\\n\\n[Introducing DataFrames in Apache Spark for Large Scale Data Science](https://www.databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)"},{"id":"how-is-memory-managed-in-spark/","metadata":{"permalink":"/blog/how-is-memory-managed-in-spark/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2023-07-07-how-is-memory-managed-in-spark/index.md","source":"@site/blog/2023-07-07-how-is-memory-managed-in-spark/index.md","title":"How Is Memory Managed In Spark?","description":"How Is Memory Managed In Spark?","date":"2023-07-07T00:00:00.000Z","formattedDate":"July 7, 2023","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Apache","permalink":"/blog/tags/apache"}],"readingTime":7.92,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"how-is-memory-managed-in-spark/","title":"How Is Memory Managed In Spark?","description":"How Is Memory Managed In Spark?","authors":"tranlam","tags":["Bigdata","Spark","Apache"],"image":"./images/banner.PNG"},"prevItem":{"title":"Differences between Spark RDD, Dataframe and Dataset","permalink":"/blog/spark-rdd-dataframe-dataset/"},"nextItem":{"title":"State Management In React","permalink":"/blog/state-management-react/"}},"content":"Spark is an in-memory data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute tasks across multiple computers. Spark applications are memory heavy, hence, it is obvious that memory management plays a very important role in the whole system.\\n\\n![banner image](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Spark executor\\n\\n![cluster overview](./images/cluster-overview.PNG)\\n\\nSpark sends application code (defined by JAR or Python files passed to SparkContext) to each executor which will launch a JVM process for code execution. There are two types of memory in JVM.\\n\\n- **On-Heap memory:** refers to objects that will be present in the Java heap (and also subject to GC).\\n- **Off-Heap memory:** refers to (serialized) objects that are managed by EHCache, but stored outside the heap (and also not subject to GC).\\n\\nThe Off-Heap store is used to avoid the overhead of GC on a heap that is several Megabytes or Gigabytes large. It is slightly slower than the On-Heap memory, but still faster than the disk store.\\n\\n## Spark memory manager\\n\\nBefore Spark 1.6, a simple scheme for memory management was adopted, which is Static Memory Manager (SMM). The size of Storage Memory and Execution Memory and other memory is fixed during application execution and it has been deprecated because of the lack of flexibility.\\n\\nFrom Spark 1.6+, Spark came up with Unified Memory Manager (UMM) with dynamic memory allocation, shared by storage and execution. Thus, when Execution Memory is not used, the Storage Memory can borrow all the available memory and vice versa, by calling acquireMemory() to make changes to memory pools. Therefore, UMM has lots of advantages compared to SMM.\\n\\n- Memory can be switched between Storage Memory and Execution Memory.\\n- When our application has no cache, all memory can be used by execution and thus prevent data spilling to disk.\\n- The application will be able to spend a minimum amount for Storage Memory for cached data and let the execution borrow the remaining.\\n- Dynamically improve performance without requiring the user to configure the memory portion for each manually.\\n\\n### On-Heap Memory\\n\\nThe size of the On-Heap memory can be configured either by passing `--executor-memory` to command lines or setting `spark.executor.memory` to the Spark application, in the same format as JVM memory strings with a size unit suffix (\\"k\\", \\"m\\", \\"g\\" or \\"t\\") (e.g. 512m, 2g). This amount of memory can be breakdown into the below types.\\n\\n![on heap overview](./images/on-heap-overview.PNG)\\n\\nDefault values of those configurations in Spark v3.3.0\\n\\n```bash\\nspark.executor.memory=1g\\nspark.memory.fraction=0.6\\nspark.memory.storageFraction=0.5\\nUsable Memory = On-Heap Memory - Reserved Memory\\n```\\n\\n#### Researved memory\\n\\nReserved Memory is the memory reserved for the system and is used to store Spark\'s internal objects. Its size is hardcoded `private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024` in `org.apache.spark.memory.UnifiedMemoryManager`. If you want to make any modifications, you need to change the Spark source code and recompile it. Spark will require On-Heap memory greater or equal to 1.5 times of Reserved Memory or it will fail to initialize Spark session.\\n\\n```bash\\nspark-shell --conf spark.executor.memory=300m\\n\\njava.lang.IllegalArgumentException: Executor memory 314572800 must be at least 471859200. Please increase executor memory using the --executor-memory option or spark.executor.memory in Spark configuration.\\n\\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:229)\\n    ...\\n```\\n\\n#### User memory\\n\\nUser Memory is the memory used to store user-defined data structures, any UDFs created by the user, the data needed for RDD conversion operations, etc. This memory segment is not managed by Spark and Spark will not be aware of/maintain it.\\n\\n```bash\\nUser Memory = Usable Memory * (1 - spark.memory.fraction)\\n```\\n\\n#### Spark memory\\n\\n```bash\\nSpark Memory = Usable Memory * spark.memory.fraction\\n```\\n\\nThis memory pool is managed by Spark. Divided into two types of memory\\n\\n- **Execution Memory:** Used for processing tasks, storing objects required during the execution of the tasks. When this pool has no space left, it will spill to the disk. Execution Memory tends to remain shorter than Storage Memory since it will be evicted immediately after each operation, making space for the next ones.\\n\\n```bash\\nStorage Memory = Spark Memory * (1 - spark.memory.storageFraction)\\n```\\n\\n- **Storage Memory:** used for storing the cached data (data with persist option MEMORY in it), broadcast variables, and data deserialization. When this region is full, cache data will be either written to disk or recomputed based on configuration. Spark also clears space for new cache requests by eliminating the old cache objects with the Least Recently Used (LRU) mechanism.\\n\\n```bash\\nStorage Memory = Spark Memory * spark.memory.storageFraction\\n```\\n\\n#### Dynamic memory allocation between Storage Memory and Execution Memory\\n\\n- Storage Memory can borrow space from Execution Memory only if blocks are not used in Execution Memory.\\n- Execution Memory can also borrow space from Storage Memory if blocks are not used in Storage Memory.\\n- If blocks from Execution Memory are used by Storage Memory, and Execution needs more memory, it can forcefully evict the excess blocks occupied by Storage Memory.\\n- If blocks from Storage Memory are used by Execution Memory and Storage needs more memory, it cannot forcefully evict the excess blocks occupied by Execution Memory, it will end up having less memory area. It will wait until Spark releases the excess blocks stored by Execution Memory and then occupies them.\\n\\n### Off-Heap Memory\\n\\nMost Spark operations happened entirely in On-Heap memory and utilize the mighty help of GC that sometimes can cause GC overhead. To minimize this effect, Spark introduces the Off-Heap memory for certain operations, which will reduce the impact of GC in the application.\\n\\nOff-Heap memory means allocating memory objects (serialized to a byte array) to memory outside the heap of the JVM, which is directly managed by the operating system. This memory does not bound to GC but calls the Java API (sun.misc.Unsafe) for unsafe operations such as C which uses malloc() to use operating system memory.\\n\\nData accessing in this region can be slightly slower than accessing the On-Heap memory, but still faster than disk, and the user has to manually deal with managing the allocated memory. Data on Off-Heap memory can still be persisted even when the executor getting killed (data cache on On-Heap memory would be gone).\\n\\nThis memory region is disabled by default but can be enabled by setting these configurations.\\n\\n```bash\\nspark.memory.offHeap.enabled = true (false by default)\\nspark.memory.offHeap.size = ?g (0 by default)\\n```\\n\\nOff-Heap memory includes only Storage Memory and Execution Memory, which will be distributed in the following manner.\\n\\n![off heap overview](./images/off-heap-overview.PNG)\\n\\nTherefore, the total memory of Storage Memory or Execution Memory will be the sum of each in both On-Heap and Off-Heap memories.\\n\\n## Spark memory calculation example\\n\\nDespite we pass `spark.executor.memory` to On-Heap memory, the maximum amount of memory that the JVM will attempt to use will be slightly smaller than `spark.executor.memory`, which will be calculated with the below Java program.\\n\\n```java\\npublic class Helper {\\n    public static void main(String[] args) {\\n        long maxMem = Runtime.getRuntime().maxMemory();\\n        System.out.println(maxMem);\\n    }\\n}\\n```\\n\\n```bash\\nspark.executor.memory=1024 (as 1GB in MB)\\njava -Xmx${spark_executor_memory}m -cp target/calculate-1.0-SNAPSHOT.jar Helper\\n954728448 (which is 0.88916015625 GB)\\n```\\n\\nA small Python program to calculate the memory of each memory category, with the help of the Java code above.\\n\\n```python\\n# MB will be the smallest unit\\nfrom distutils.util import strtobool\\nimport subprocess\\n\\ndef get_valid_input(message, f, error_message):\\n    amount = None\\n    while amount is None:\\n        try:\\n            amount = f(input(message))\\n            return amount\\n        except ValueError:\\n            print(error_message)\\n\\ndef get_jvm_max_mem(mem):\\n    command = [\\"java\\", f\\"-Xmx{int(mem)}m\\", \\"-cp\\", \\"calculate/target/calculate-1.0-SNAPSHOT.jar\\", \\"Helper\\"]\\n    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\\n    text = p.stdout.read()\\n    ret = p.wait()\\n    if ret == 0:\\n        return float(text.decode(\\"utf-8\\"))\\n\\nif __name__ == \\"__main__\\":\\n    RESERVED_SYSTEM_MEMORY = 300\\n    SPARK_MEMORY_FRACTION=0.6\\n    SPARK_MEMORY_STORAGEFRACTION=0.5\\n    GB_TO_MB_RATE = 1024\\n    spark_executor_memory = get_valid_input(\\"Amount of spark.executor.memory (in GB): \\", float, \\"Invalid value for spark.executor.memory, must be a number\\") * GB_TO_MB_RATE\\n    spark_executor_memory = get_jvm_max_mem(spark_executor_memory) / pow(GB_TO_MB_RATE, 2)\\n    spark_memory_offheap_enabled = get_valid_input(\\"Option spark.memory.offHeap.enabled: \\", strtobool, \\"Invalid value for spark.memory.offHeap.enabled, must be a boolean string (true, false, True, False,...)\\")\\n    if spark_memory_offheap_enabled:\\n        spark_memory_offheap_size = get_valid_input(\\"Amount of spark.memory.offHeap.size (in GB): \\", float, \\"Invalid value for spark.memory.offHeap.size, must be a number\\") * GB_TO_MB_RATE\\n    on_heap_user_memory = (spark_executor_memory - RESERVED_SYSTEM_MEMORY) * (1 - SPARK_MEMORY_FRACTION)\\n    on_heap_spark_memory = (spark_executor_memory - RESERVED_SYSTEM_MEMORY) * SPARK_MEMORY_FRACTION\\n    on_heap_spark_storage_memory = on_heap_spark_memory * SPARK_MEMORY_STORAGEFRACTION\\n    on_heap_spark_execution_memory = on_heap_spark_memory * (1 - SPARK_MEMORY_STORAGEFRACTION)\\n    total_spark_memory = on_heap_spark_memory\\n    print(\\"\\\\n\\")\\n    print(f\\"------------------ On-Heap Memory: {spark_executor_memory} MB ------------------\\")\\n    print(f\\"Researved Memory: {RESERVED_SYSTEM_MEMORY} MB\\")\\n    print(f\\"User Memory: {on_heap_user_memory} MB\\")\\n    print(f\\"Spark Memory: {on_heap_spark_memory} MB\\")\\n    print(f\\"\\\\tStorage Memory: {on_heap_spark_storage_memory} MB\\")\\n    print(f\\"\\\\tExecution Memory: {on_heap_spark_execution_memory} MB\\")\\n    if spark_memory_offheap_enabled:\\n        off_heap_spark_storage_memory = spark_memory_offheap_size * SPARK_MEMORY_STORAGEFRACTION\\n        off_heap_spark_execution_memory = spark_memory_offheap_size * (1 - SPARK_MEMORY_STORAGEFRACTION)\\n        print(\\"\\\\n\\")\\n        print(f\\"------------------ Off-Heap Memory: {spark_memory_offheap_size} MB ------------------\\")\\n        print(f\\"Storage Memory: {off_heap_spark_storage_memory} MB\\")\\n        print(f\\"Execution Memory: {off_heap_spark_execution_memory} MB\\")\\n        total_spark_memory += spark_memory_offheap_size\\n    print(\\"\\\\n\\")\\n    print(f\\"------------------ Total Spark Memory (Spark Memory + Off-Heap Memory): {total_spark_memory} MB ({total_spark_memory / GB_TO_MB_RATE} GB) ------------------\\")\\n```\\n\\nSo for an application with `spark.executor.memory=1g`, `spark.memory.offHeap.enabled=true`, `spark.memory.offHeap.size=512m`.\\n\\n```bash\\npython calculate.py\\n\\nAmount of spark.executor.memory (in GB): 1\\nOption spark.memory.offHeap.enabled: true\\nAmount of spark.memory.offHeap.size (in GB): 0.5\\n\\n\\n------------------ On-Heap Memory: 910.5 MB ------------------\\nResearved Memory: 300 MB\\nUser Memory: 244.20000000000002 MB\\nSpark Memory: 366.3 MB\\n        Storage Memory: 183.15 MB\\n        Execution Memory: 183.15 MB\\n\\n\\n------------------ Off-Heap Memory: 512.0 MB ------------------\\nStorage Memory: 256.0 MB\\nExecution Memory: 256.0 MB\\n\\n\\n------------------ Total Spark Memory (Spark Memory + Off-Heap Memory): 878.3 MB (0.85771484375 GB) ------------------\\n```\\n\\nWe start a spark shell with the following configurations.\\n\\n```bash\\nspark-shell --conf spark.executor.memory=1g --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=512m\\n```\\n\\nThen go to `http://localhost:4040/executors/`.\\n\\n![memory calculation](./images/memory-calculation.PNG)\\n\\nWe can see that the total amount of Spark memory is exactly like our calculation.\\n\\nThat is how we calculate the memory in Spark. The source code can be found at: **[https://github.com/LTranData/spark_memory_calculator](https://github.com/LTranData/spark_memory_calculator)**. See you in the next blogs.\\n\\n## References\\n\\n[Spark Memory Management](https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794)\\n\\n[Dive into Spark memory](https://luminousmen.com/post/dive-into-spark-memory)"},{"id":"state-management-react/","metadata":{"permalink":"/blog/state-management-react/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2023-06-26-state-management-react/index.md","source":"@site/blog/2023-06-26-state-management-react/index.md","title":"State Management In React","description":"State Management In React","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"Web development","permalink":"/blog/tags/web-development"},{"label":"React","permalink":"/blog/tags/react"},{"label":"Html","permalink":"/blog/tags/html"},{"label":"Css","permalink":"/blog/tags/css"}],"readingTime":10.43,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"state-management-react/","title":"State Management In React","description":"State Management In React","authors":"tranlam","tags":["Web development","React","Html","Css"],"image":"./images/showcase.PNG"},"prevItem":{"title":"How Is Memory Managed In Spark?","permalink":"/blog/how-is-memory-managed-in-spark/"},"nextItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","permalink":"/blog/mini-spark3-authorizer-part-2/"}},"content":"State is a very crucial part of React applications which will help update the information of the React components to change UI accordingly and make our application interactive with clients. In this article, I will walk you through the usage of state, its characteristic, and how we can use state in an efficient way.\\n\\n![showcase image](./images/showcase.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Virtual DOM\\n\\nIf you ever work with `React` or `Vue`, you probably know about a concept called Virtual DOM which those libraries/frameworks used to update the real DOM tree in the browser. Virtual DOM in React is a programming concept where the representation of a UI is kept in memory and synced with the real DOM.\\n\\n![virtual dom](./images/virtual_dom.PNG)\\n\\nIn the picture above, whenever we have a UI in the browser on the left, we can inspect that UI and we can see the real DOM tree which contains several Html elements such as `div`, `ul`, `li`,... React will maintain a JavaScript object on the right to describe things on the real DOM and updates made to that object will be synchronized to the real DOM and modify the UI accordingly.\\n\\nThus, the Virtual DOM is a lightweight version of the real DOM, it provides a mechanism that abstracts manual DOM manipulations away from the developers, helping us to write more predictable code to interact with the real DOM.\\n\\n## React state\\n\\nState is a plain JavaScript object used by React to represent a piece of information about the component\'s current situation. We modify state to manipulate the Virtual DOM.\\n\\n![state change](./images/state_change.PNG)\\n\\nWhenever state changes, it will modify some properties of the Virtual DOM. React will compare the new Virtual DOM with the old one to detect those changes and will synchronize them to the real DOM. The interesting thing here is that update is only applied at the node where there is an actual change. So, how can React do the diffing algorithm in an efficient way?\\n\\n## Diffing algorithm\\n\\nDiffing is the algorithm that React uses in order to find differences between two Virtual DOM trees and update efficiently the real DOM. In this section, I will just explain very high-level rules used by this algorithm.\\n\\nWhen diffing two trees, React will first compare the two root elements.\\n\\n### Rule #1\\n\\nTwo elements of different types will produce different trees.\\n\\n![rule 1](./images/rule_1.PNG)\\n\\nAs shown in the picture, going from `<div>` to `<span>` will lead to full rebuild, its children will get unmounted and have their state destroyed.\\n\\n### Rule #2\\n\\nWith the DOM elements of the same type, but different attributes, React knows to only modify the attribute that have been changed.\\n\\nIn this example, when the `className` attribute of the `<input>` is changed, React will only update that attribute instead of rebuilding the whole element.\\n\\n![rule 2 1](./images/rule_2_1.PNG)\\n\\nWith object value attribute, React updates only the properties that have been changed.\\n\\n![rule 2 2](./images/rule_2_2.PNG)\\n\\n### Rule #3\\n\\nThis rule is about detecting the differences while recursing on children elements.\\n\\n![rule 3 1](./images/rule_3_1.PNG)\\n\\nReact iterates over both lists of lists of children at the same time, and generates mutation whenever there\'s a difference. As shown in above example, React will know to just generate the `List item 3` and append to the `ul` element.\\n\\nSo what if we prepend the list element instead of append? Because of iterating two lists at the same time and comparing, React will assume that all of those elements have been changed and rebuild all of them, which is a very bad case.\\n\\n![rule 3 2](./images/rule_3_2.PNG)\\n\\nIn order to get rid of the above case, we come to the `rule #4`\\n\\n### Rule #4\\n\\nDevelopers can hint at which elements may be stable across different renders with a `key` prop, so that elements with the same key will be compared with each other\\n\\n![rule 4](./images/rule_4.PNG)\\n\\nIn the picture above, we add a `key` property to each `li` element so React will know to pair check the elements with the `key=\\"item-1\\"` and `key=\\"item-2\\"` and only add the element with `key=\\"item-3\\"` to the new DOM\\n\\n## State usage with useState hook\\n\\nThe `useState` hook is a built-in React hook that allows you to manage state in a functional component.\\n\\nConsider an example below\\n\\n```js\\n// define React state in functional components\\nconst [count, setCount] = useState(0);\\n\\n// update state\\nsetCount(count + 1);\\nsetCount((prevCount) => prevCount + 1);\\n```\\n\\n`useState` returns an array contains the state (`count`) and a function used to update the state (`setCount`). The set function will take either the new value for the state or a callback function that returns new value as the argument.\\n\\nThere are some considerations when using state\\n\\n- **`setState` function is asynchronous:** consider the below example, whenever we click the button to increase the count, we see the console logs the old value of state before it is updated. Because `setState` function is asynchronous, it will be pulled to an event loop, move to the lifecycle `Call stack -> WebAPIs -> Callback Queue -> Call stack -> being executed -> Pop out the stack` (you can refer to **[Javascript event loop](https://www.webdevolution.com/blog/Javascript-Event-Loop-Explained)**), but basically, in JavaScript, in the same code block, the synchronous code will always run before the asynchronous code, that\'s why we see the console logs the value of state before it is updated.\\n\\nCode: `UseStateM1.jsx`, tab `Mistake 1`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM1.jsx\\"\\n></iframe>\\n\\n- **Changes to state should be made by `setState` function:** in this example, we can see that the count actually changes in the console log but the UI is not updated. Because we modify the state value directly and it will not cause the rerender of the component. In this case, we need to update state value by `setState` function: `setCount(count + 3)`.\\n\\nCode: `UseStateM2.jsx`, tab `Mistake 2`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM2.jsx\\"\\n></iframe>\\n\\n- **State should be treated as immutable:** each time we use `setState` function, we need to pass a brand new value to the function in order to rerender the component. React uses Shallow Comparision to check if the state is changed or not. In JavaScript, with primitive datatypes such as numbers, strings,... SC will compare their values, with object datatypes such as object, array,... SC will compare their references. Consider the example below, when we modify the state object directly, the component will not be rerendered and the UI will not be updated because the state actually holds the reference to the user object, not its value. In order to fix this, we use the spread operator to create a new object to make a new reference, and update properties in that object `{ ...prevUser, name: \\"No one\\", age: 30 }`, and our component will work as expected.\\n\\nCode: `UseStateM3.jsx`, tab `Mistake 3`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM3.jsx\\"\\n></iframe>\\n\\n- **Pass function as argument to `setState` whenever the state value depends on its previous value:** in the example below, some might expect the value of `count` will increase 2 at a time, but it actually increases 1 because `setState` is an asynchronous function, it will not be executed immediately but pulled to the event loop, and those two `setState` functions will receive the same value `count = 0`. To fix this, we need to use `setCount((prev) => prev + 1)`, we make the output value depends on the previous value so no matter the order of execution of that two `setState`, the value will be increased by 2 at a time.\\n\\nCode: `UseStateM4.jsx`, tab `Mistake 4`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM4.jsx\\"\\n></iframe>\\n\\n## Update components in different DOM tree branches\\n\\nState is often used within the body of a component and modify information about the component or its children, so, how can we sit in a component and modify the UI of other components that are in different scope?\\n\\n![update different dom tree branches](./images/update_different_dom_tree_branches.PNG)\\n\\n### Using callback functions\\n\\nIn this approach, we are using callback functions in the child component to update state of the parent component, we also pass the state value from the parent to another child component to update its UI.\\n\\nCode: `ByCallback.jsx`, tab `By Callback`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByCallback.jsx\\"\\n></iframe>\\n\\nIn the code above, we define state in the `Root` component and pass the functions used to update state to `RootLeftLeft`, and pass the state value to `RootRightRight`. Now, we can sit in the `RootLeftLeft` and modify the UI of `RootRightRight` by updating the state in their common parent component.\\n\\nPros of this approach\\n\\n- It does what we want, which is to update the UI of the component from other scopes.\\n- Easy to use and pretty straightforward, use the React built-in hooks.\\n\\nCons\\n\\n- It will cause a lot of rerenders each time because the `Root` component is being rerendered, and so do its children.\\n- Ugly code when we have many duplicates and we also need to pass the props in the intermediate components in the way from `Root` component to our target components.\\n\\n### Using React context\\n\\nBy using this approach, we can get rid of the ugly code problem in the first approach.\\n\\nCode: `ByContext.jsx`, tab `By Context`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByContext.jsx\\"\\n></iframe>\\n\\nWe define the `CountContext` context and provide it to our application part, we register the state value and the functions used to update state to the context, then, we use it directly in our target components, by using React `useContext` hook.\\n\\nEven we don\'t have the smelly code anymore, this approach still cannot solve the performance problem when all components of the tree are still being rerendered.\\n\\n### Using Redux\\n\\nRedux is a powerful state management library that will help us avoid rerendering too many components.\\n\\nRedux has a concept called a single source of truth when we maintain our whole application state in a single store.\\n\\nThe state is Redux is read-only as we cannot mutate directly, the only way to change state is to emit an action.\\n\\nChanges will be made by pure functions, so reducers are the pure functions, their output will only depend on the inputs and will not cause any side effects.\\n\\nCode: `ByRedux.jsx`, tab `By Redux`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByRedux.jsx\\"\\n></iframe>\\n\\nIn this example, we define `store` as our single source of truth, with a reducer to manage the count value and provide it within our application part. We create a slice and define functions to mutate the value in `store`. Action creators can be extracted from the slice and then they will be used everywhere within the context that `store` is provided.\\n\\nIn this way, the only component that gets rerendered is `RootRightRight`, when its props changed over time.\\n\\n## Performance considerations when interacting with state\\n\\nThere are some useful hooks/functions used to cache/memorize things related to state to avoid components from rerendering, which are\\n\\n- `React.memo()` for memorizing React components.\\n- `useMemo()` for memorizing some values.\\n- `useCallback()` for memorizing some callback functions.\\n\\nI will apply theses to the **[ByContext](#52-using-react-context)** example above.\\n\\nCode: `Memo.jsx`, tab `Memo`\\n\\n<iframe\\n  loading=\\"lazy\\"\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/Memo.jsx\\"\\n></iframe>\\n\\nI use `useCallback` to memorize the callback functions used to update state, and prevent them from being recreated every time the `Root` component rerenders. Because the state value `count` changes over time so I will not cache that value.\\n\\nI also use `React.memo()` to memorize the `RootLeft` component to prevent its rerendering when `Root` component rerenders (can also apply to other static components in the code, such as `RootRightLeft`,...).\\n\\nNow, when we hit the increment/decrement buttons, we can see that event `Root` component rerenders, `RootLeft` and `RootLeftLeft` will not be rerendered.\\n\\nThis is pretty much about state management in React. Hope you enjoy reading it. See you in the next blogs.\\n\\n## References\\n\\n[Reconciliation](https://legacy.reactjs.org/docs/reconciliation.html)\\n\\n[What is Diffing Algorithm ?](https://www.geeksforgeeks.org/what-is-diffing-algorithm/)\\n\\n[Javascript Event Loop Explained](https://www.webdevolution.com/blog/Javascript-Event-Loop-Explained)"},{"id":"mini-spark3-authorizer-part-2/","metadata":{"permalink":"/blog/mini-spark3-authorizer-part-2/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md","source":"@site/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md","title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","description":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","date":"2023-05-01T00:00:00.000Z","formattedDate":"May 1, 2023","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Ranger","permalink":"/blog/tags/ranger"},{"label":"Apache","permalink":"/blog/tags/apache"}],"readingTime":5.735,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mini-spark3-authorizer-part-2/","title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","description":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","authors":"tranlam","tags":["Bigdata","Spark","Ranger","Apache"],"image":"./images/banner.PNG"},"prevItem":{"title":"State Management In React","permalink":"/blog/state-management-react/"},"nextItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","permalink":"/blog/mini-spark3-authorizer-part-1/"}},"content":"In the **[previous blog](/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md)**, I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of **[Spark Catalyst Optimizer](/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md)** in order to archive authorization in Spark SQL with Ranger.\\n\\n![banner](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Spark installation\\n\\nFirstly, I will install Apache Spark 3.3.0 on my local machine. It is pretty easy with a few below steps.\\n\\n```bash\\n# Get Spark build with hadoop, you can find specific version here: https://archive.apache.org/dist/spark/\\ncd ~\\nwget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\\ntar -xvf spark-3.3.0-bin-hadoop3.tgz\\ncd spark-3.3.0-bin-hadoop3\\n\\n# Configure some environment variables\\nexport SPARK_HOME=~/spark-3.3.0-bin-hadoop3\\nexport PATH=$PATH:$SPARK_HOME/bin\\n\\n# Check if Spark is installed properly\\nspark-shell\\n```\\n\\n## Build a mini Spark Session Extension for Ranger authorization\\n\\nThe idea to make the authorizer is first inspired by **[this Spark Ranger repository](https://github.com/yaooqinn/spark-ranger)**. Currently, this repository has been archived and it is only compatible with Spark 2.4 and below, which does not help our use case (we use Spark 3.3.0). After weeks of trying multiple solutions but not having a result, I end up customizing the repository to work with Spark 3.3.0 myself.\\n\\nSpark makes a huge update to migrate from 2.4 to 3.0 (see detail **[migration guide](https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-24-to-30)** and **[Spark Release 3.0.0](https://spark.apache.org/releases/spark-release-3-0-0.html)**) which create many new features. So we need to add more code and configure the project dependencies correctly in order not to conflict with Spark 3 dependencies (since Ranger and Spark 3 use the same library `jersey` to build RESTful Web Services, Ranger uses 1.x and Spark 3 uses 2.x).\\n\\nThe idea is to create a Spark Session Extension to customize the logical plan optimization phase of Spark Catalyst Optimizer, and since the logical plan is represented as **[TreeNode](/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md#1-treenode)**, it can be logical commands such as `CreateTableCommand`, `DropTableCommand`, `InsertIntoHiveTable`,... In these commands, we can extract the name of the database, table, and columns,... out of them, we collect them and check if the current user has proper access to those resources by Ranger-provided APIs.\\n\\nThe extension code can be found at: **[https://github.com/LTranData/mini_spark3_authorizer](https://github.com/LTranData/mini_spark3_authorizer)**.\\n\\n```bash\\n# Build the project\\ncd ~\\ngit clone https://github.com/LTranData/mini_spark3_authorizer\\ncd mini_spark3_authorizer/spark3-ranger-custom\\nmvn clean package\\n\\n# Copy output jar to Spark jars folder\\ncp target/spark-ranger-1.0-SNAPSHOT.jar $SPARK_HOME/jars\\n\\n# Download dependency jars\\ncd $SPARK_HOME/jars\\nwget https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar\\nwget https://repo1.maven.org/maven2/com/kstruct/gethostname4j/1.0.0/gethostname4j-1.0.0.jar\\nwget https://repo1.maven.org/maven2/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar\\nwget https://repo1.maven.org/maven2/org/apache/kudu/kudu-spark3_2.12/1.16.0/kudu-spark3_2.12-1.16.0.jar\\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar\\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar\\n```\\n\\nAfter we have all the additional jars in Spark, we start to configure some properties for the extension.\\n\\n```bash\\n# Get the template configuration file\\ncd $SPARK_HOME/conf\\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-security.xml .\\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-audit.xml .\\n\\n# Modify the downloaded policy directory\\nvi ranger-spark-security.xml\\nranger.plugin.spark.policy.cache.dir=<your_spark_home>/security/policycache\\nmkdir -p $SPARK_HOME/security/policycache\\n\\n# Copy template policy file. The filename need to be exactly below, because in the extension code, plugin appId = ranger_customized, in Ranger Admin UI, the service we are going to create will have the name = spark_sql\\ncp ~/mini_spark3_authorizer/spark3-conf/security/policycache/ranger_customized_spark_sql.json $SPARK_HOME/security/policycache\\n```\\n\\nAt the moment, we go to Ranger Admin UI `http://localhost:6080` with `user/password = admin/YourPassword@123456`. In the `HIVE` plugin folder, create a service with the below input.\\n\\n![service config](./images/service_config.PNG)\\n\\nWe can test if the service policy is accessible through RESTful APIs or not and if it is downloadable.\\n\\n```bash\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies/download/spark_sql\\"\\n```\\n\\nAdd user you are going to test under `Settings -> Users/Groups/Roles -> Add New User`.\\n\\n![user config](./images/user_config.PNG)\\n\\nAdd policy to the service `spark_sql`.\\n\\n![policy config](./images/policy_config.PNG)\\n\\nWe create the database and table that we need.\\n\\n```bash\\n# Create a table to Spark metastore\\nspark-shell\\n\\nscala> val df = spark.read.parquet(\\"file:///Users/tranlammacbook/mini_spark3_authorizer/jobs.parquet\\")\\nscala> df.printSchema\\nroot\\n |-- job_id: string (nullable = true)\\n |-- company_id: string (nullable = true)\\n |-- job_name: string (nullable = true)\\n |-- taglist: string (nullable = true)\\n |-- location: string (nullable = true)\\n |-- three_reasons: string (nullable = true)\\n |-- description: string (nullable = true)\\nscala> spark.sql(\\"CREATE DATABASE test;\\")\\nscala> df.write.mode(\\"overwrite\\").format(\\"parquet\\").saveAsTable(\\"test.jobs\\")\\n```\\n\\nNow we can test our extension. As in the policy configuration, user `lamtran` only has permission to the columns `job_id, company_id, job_name` of table `test.jobs`.\\n\\n```bash\\nspark-shell --conf spark.sql.extensions=mini.spark3.authorizer.RangerSparkSQLExtension --conf spark.sql.proxy-user=lamtran\\n\\n# Check Spark configurations\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = mini.spark3.authorizer.RangerSparkSQLExtension\\nscala> spark.conf.get(\\"spark.sql.proxy-user\\")\\nres1: String = lamtran\\n\\n# Check permitted columns\\nscala> spark.sql(\\"SELECT job_id, company_id, job_name FROM test.jobs LIMIT 10\\").show(truncate=false)\\n+----------------------------------------------------+--------------+----------------------------------------+\\n|job_id                                              |company_id    |job_name                                |\\n+----------------------------------------------------+--------------+----------------------------------------+\\n|kms-technology:jrsr_qa_engineer_kms_labs_bonus      |kms-technology|(Jr/Sr) QA Engineer, KMS Labs - BONUS   |\\n|kms-technology:engineering_manager_bonus            |kms-technology|Engineering Manager - BONUS             |\\n|kms-technology:fullstack_mobile_mobilenodejs_kobiton|kms-technology|Fullstack Mobile (Mobile,NodeJs) Kobiton|\\n|kms-technology:jrsrprincipal_java_developer_bonus   |kms-technology|(Jr/Sr/Principal) Java Developer- BONUS |\\n|kms-technology:product_manager_kms_labs_bonus       |kms-technology|Product Manager, KMS Labs - BONUS       |\\n|kms-technology:sr_it_business_analyst_english_bonus |kms-technology|Sr IT Business Analyst (English) - BONUS|\\n|kms-technology:fullstack_dev_reactjsnodejs_kobiton  |kms-technology|Fullstack Dev (ReactJs,NodeJs) - Kobiton|\\n|kms-technology:senior_ruby_on_rails_engineer_bonus  |kms-technology|Senior Ruby on Rails Engineer - BONUS   |\\n|kms-technology:senior_data_engineer_bonus           |kms-technology|Senior Data Engineer - BONUS            |\\n|kms-technology:srjr_fullstack_nodejsreactjs_bonus   |kms-technology|Sr/Jr Fullstack (NodeJS,ReactJS) - BONUS|\\n+----------------------------------------------------+--------------+----------------------------------------+\\n\\n# Check columns that are not allowed to select\\nscala> spark.sql(\\"SELECT location, taglist, description FROM test.jobs LIMIT 10\\").show(truncate=false)\\n23/07/01 21:35:19 ERROR RangerSparkAuthorizerExtension: \\n+===============================+\\n|Spark SQL Authorization Failure|\\n|-------------------------------|\\n|Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\\n|-------------------------------|\\n|Spark SQL Authorization Failure|\\n+===============================+\\n             \\norg.apache.ranger.authorization.spark.authorizer.SparkAccessControlException: Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6(RangerSparkAuthorizer.scala:127)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6$adapted(RangerSparkAuthorizer.scala:124)\\n  at scala.collection.Iterator.foreach(Iterator.scala:943)\\n  at scala.collection.Iterator.foreach$(Iterator.scala:943)\\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3(RangerSparkAuthorizer.scala:124)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3$adapted(RangerSparkAuthorizer.scala:107)\\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.checkPrivileges(RangerSparkAuthorizer.scala:107)\\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:58)\\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:14)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\\n  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\\n  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\\n  at scala.collection.immutable.List.foldLeft(List.scala:91)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\\n  at scala.collection.immutable.List.foreach(List.scala:431)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\\n  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\\n  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\\n  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\\n  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:810)\\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:787)\\n  ... 47 elided\\n```\\n\\nNow check the downloaded policy file.\\n\\n```bash\\ncd $SPARK_HOME/security/policycache\\nls -lh\\n-rw-r--r--  1 tranlammacbook  staff    23K Jul  1 21:32 ranger_customized_spark_sql.json\\n```\\n\\nWe will see this is the new policy file and its content is refreshed every 10 seconds when the extension is in use.\\n\\nNow, we can check the Ranger audit logs at `http://localhost:6080/index.html#!/reports/audit/bigData`.\\n\\n![ranger audit](./images/ranger_audit.PNG)\\n\\nTo check if it is pushed in Solr or not, go to `http://localhost:6083/solr/#/ranger_audits/query` and click **Execute Query**.\\n\\n![solr audit](./images/solr_audit.PNG)\\n\\n## Room for improvement\\n\\nIn this blog, we integrate a local Spark 3 with a standalone Ranger, but in production, Spark is often used in corporate with a Hadoop data cluster with Kerberos authentication enabled. Ranger will also sit in that Hadoop and do the authorization for many frameworks in Hadoop. Thus, there are a few more things that we need to implement.\\n\\n- Config Spark and Ranger to work with Hadoop, and run Spark job in cluster mode.\\n- Policy refresher needs to be secure and use SPNego protocol (which use Kerberos keytab to generate token) to make RESTful API calls to Ranger service.\\n- The user of the Spark application and the user for authorizing Ranger need to be the keytab principal user.\\n\\nI have also successfully implemented all the above functionalities in the production environment. If you need them, contact me on **[Linkedin](https://www.linkedin.com/in/ltrandata/)**."},{"id":"mini-spark3-authorizer-part-1/","metadata":{"permalink":"/blog/mini-spark3-authorizer-part-1/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md","source":"@site/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md","title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","description":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","date":"2023-04-30T00:00:00.000Z","formattedDate":"April 30, 2023","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Ranger","permalink":"/blog/tags/ranger"},{"label":"Apache","permalink":"/blog/tags/apache"}],"readingTime":4.125,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mini-spark3-authorizer-part-1/","title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","description":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","authors":"tranlam","tags":["Bigdata","Spark","Ranger","Apache"],"image":"./images/banner.PNG"},"prevItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","permalink":"/blog/mini-spark3-authorizer-part-2/"},"nextItem":{"title":"Spark Catalyst Optimizer And Spark Session Extension","permalink":"/blog/spark-catalyst-optimizer-and-spark-session-extension/"}},"content":"Spark and Ranger are widely used by many enterprises because of their powerful features. Spark is an in-memory data processing framework and Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform. Thus, Ranger can be used to do authorization for Spark SQL and this blog will walk you through the integration of those two frameworks. This is the first part of the series, where we install the Ranger framework on our machine, and additionally, Apache Solr for auditing.\\n\\n![banner](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Build process\\n\\nThis installation comes with these below components.\\n\\n| Component   | Version      |\\n| ----------- | ------------ |\\n| MacOS       | M1/M2        |\\n| MySQL       | 8.0.33       |\\n| Java        | OpenJDK-8-jdk|\\n| Python      | 2.7.18       |\\n| Maven       | 3.6.3        |\\n| Ranger      | 2.0.0        |\\n| Solr        | 7.7.1        |\\n\\nI install the framework on MacOS, but it is similar when it comes to any other Unix or Linux distributions.\\n\\nFirstly, you need to have MySQL, Jdk8, Python 2.7, and Maven installed on your system, follow instructions from anywhere to make sure these components work as expected.\\n\\nAfter that, you can download the source code of Ranger (I use release version 2.0.0) and start to build the source code.\\n\\n```bash\\n# Get source code\\nwget https://downloads.apache.org/ranger/2.0.0/apache-ranger-2.0.0.tar.gz\\ntar -xvf apache-ranger-2.0.0.tar.gz\\ncd apache-ranger-2.0.0\\n\\n# Build source code, it will output the ranger-2.0.0-admin.tar.gz in this repository in target/ folder\\nmvn clean compile package install assembly:assembly -Dmaven.test.skip=true -Drat.skip=true -Dpmd.skip=true -Dfindbugs.skip=true -Dspotbugs.skip=true -Dcheckstyle.skip=true\\n```\\n\\n## Ranger Admin installation\\n\\nAfter the build process, you will have the `ranger-2.0.0-admin.tar.gz` file in the `target/` folder, you can go to the installation step of Ranger Admin. Our target is running a standalone Ranger with Solr for auditing then `ranger-2.0.0-admin.tar.gz` is enough in this case.\\n\\n```bash\\n# Extract the tar file\\ncp ranger-2.0.0-admin.tar.gz ~\\ncd ~\\ntar -xvf ranger-2.0.0-admin.tar.gz\\ncd ranger-2.0.0-admin\\n```\\n\\nWe use MySQL as the database for Ranger and store all the information, setup step needs to connect to MySQL to initialize the database and tables. Thus, we need to have a MySQL connector to connect to MySQL from the setup code.\\n\\n```bash\\n# Install mysql-connector\\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.26.tar.gz\\ntar -xvf mysql-connector-java-8.0.26.tar.gz\\nmv mysql-connector-java-8.0.26/mysql-connector-java-8.0.26.jar mysql-connector-java.jar\\n```\\n\\nNow we are ready to edit the Ranger configuration.\\n\\n```bash\\nvi install.properties\\nSQL_CONNECTOR_JAR=<path_to_mysql_connector>/mysql-connector-java.jar\\n\\ndb_name=ranger\\ndb_user=admin\\ndb_password=password12\\n\\nrangerAdmin_password=YourPassword@123456\\nrangerTagsync_password=YourPassword@123456\\nrangerUsersync_password=YourPassword@123456\\nkeyadmin_password=YourPassword@123456\\n\\naudit_solr_urls=http://localhost:6083/solr/ranger_audits\\n\\nunix_user=<your_mac_user>\\nunix_user_pwd=<your_mac_user_password>\\nunix_group=<your_mac_group>\\n\\nRANGER_PID_DIR_PATH=$PWD/var/run/ranger\\n```\\n\\nCurrently, the configuration for our use case is completed, we now run the setup scripts.\\n\\n```bash\\n# Comment setup user/group because it is currently compatible with Linux\\nvi setup.sh # then comment #setup_unix_user_group\\n\\n# Setup scripts run python2\\npyenv local 2.7.18\\n\\n# After updating the required properties, run setup.sh\\n./setup.sh\\n```\\n\\nOnce the setup scripts are done, you will see this output `Installation of Ranger PolicyManager Web Application is completed.`, that means it is successful. Then, we can run our Ranger Admin service.\\n\\n```bash\\n~/ranger-2.0.0-admin/ews/ranger-admin-services.sh start\\n\\n# Check logs\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/access_log.*\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/catalina.out\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/ranger-admin-*.log\\n```\\n\\nTo access Ranger Admin UI, go to `http://localhost:6080` with `user/password = admin/YourPassword@123456`.\\n\\n![ranger home](./images/ranger_home.PNG)\\n\\nAdditional notes.\\n\\n```bash\\n# Configure policy with\\npolicy.download.auth.users=<your_user>\\ntag.download.auth.users=<your_user>\\n\\n# We can test the policy with Rest APIs provided by Ranger\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies\\" # to get all policies\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies/download/dev_hive\\" # to get specific policy by service name\\n\\n# Stop ranger admin\\n~/ranger-2.0.0-admin/ews/ranger-admin-services.sh stop \\n```\\n\\n## Solr installation for auditing\\n\\nCurrently, Solr and Elasticsearch have supported sources for audit stores with Ranger. I will install Solr as it is built-in supported and does not require a good amount of infrastructure. This is also a standalone Solr which has no dependency on Zookeeper.\\n\\nIn the same Ranger build that we had done earlier, we would find an installation setup for enabling Solr audits. We also want to change some installation configurations for our specific use case.\\n\\n```bash\\ncd ~/ranger-2.0.0-admin/contrib/solr_for_audit_setup\\n\\n# Change config\\nvi install.properties\\nSOLR_USER=<your_mac_user>\\nSOLR_GROUP=<your_mac_group>\\n\\nSOLR_INSTALL=true\\n\\nJAVA_HOME=<your_java_home>\\nSOLR_DOWNLOAD_URL=http://archive.apache.org/dist/lucene/solr/7.7.1/solr-7.7.1.tgz\\n\\nSOLR_INSTALL_FOLDER=<your_prefix_folder>/data/solr\\nSOLR_RANGER_HOME=<your_prefix_folder>/data/solr/ranger_audit_server\\nSOLR_RANGER_DATA_FOLDER=<your_prefix_folder>/data/solr/ranger_audit_server/data\\nSOLR_LOG_FOLDER=<your_prefix_folder>/var/log/solr/ranger_audits\\n```\\n\\nAfter that, we can run the setup scripts.\\n\\n```bash\\n# Setup directory\\nmkdir -p <SOLR_INSTALL_FOLDER>\\n\\n# Setup scripts run python2\\npyenv local 2.7.18\\n\\n# After updating the required properties, run setup.sh\\nsudo ./setup.sh\\n\\n# Change user:group for Solr directory\\nsudo chown -R <your_mac_user>:<your_mac_group> ~/ranger-2.0.0-admin/contrib\\n\\n# Add write permission to Solr schema\\nchmod +w ~/ranger-2.0.0-admin/contrib/solr_for_audit_setup/data/solr/ranger_audit_server/ranger_audits/conf/managed-schema\\n```\\n\\nOnce the setup scripts are completed, we can start the Solr service.\\n\\n```bash\\n# Instructions for start/stop Solr\\ncat <SOLR_RANGER_HOME>/install_notes.txt\\n\\n# Start Solr\\n~/ranger-2.0.0-admin/contrib/solr_for_audit_setup/data/solr/ranger_audit_server/scripts/start_solr.sh\\n\\n# Stop Solr\\n~/ranger-2.0.0-admin/contrib/solr_for_audit_setup/data/solr/ranger_audit_server/scripts/stop_solr.sh\\n```\\n\\nGo to `http://localhost:6083`, you can see the information about your Solr service.\\n\\n![solr home](./images/solr_home.PNG)\\n\\nIf you don\'t want to build the source code yourself then go to this repository, I have included the output file of the build process at **[https://github.com/LTranData/ranger_build_output](https://github.com/LTranData/ranger_build_output)**.\\n\\nThis is all about installing Ranger with Solr for auditing. In the next blog, I will talk about how we can customize Spark to get a policy from Ranger and do the authorization on Spark SQL."},{"id":"spark-catalyst-optimizer-and-spark-session-extension/","metadata":{"permalink":"/blog/spark-catalyst-optimizer-and-spark-session-extension/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md","source":"@site/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md","title":"Spark Catalyst Optimizer And Spark Session Extension","description":"Spark Catalyst Optimizer And Spark Session Extension","date":"2023-01-07T00:00:00.000Z","formattedDate":"January 7, 2023","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Apache","permalink":"/blog/tags/apache"}],"readingTime":14.595,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"spark-catalyst-optimizer-and-spark-session-extension/","title":"Spark Catalyst Optimizer And Spark Session Extension","description":"Spark Catalyst Optimizer And Spark Session Extension","authors":"tranlam","tags":["Bigdata","Spark","Apache"],"image":"./images/spark-catalyst-optimizer.JPG"},"prevItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","permalink":"/blog/mini-spark3-authorizer-part-1/"},"nextItem":{"title":"MySQL series - Indexing","permalink":"/blog/mysql-series-mysql-indexing/"}},"content":"Spark catalyst optimizer is located at the core of Spark SQL with the purpose of optimizing structured queries expressed in SQL or through DataFrame/Dataset APIs, minimizing application running time and costs. When using Spark, often people see the catalyst optimizer as a black box, when we assume that it works mysteriously without really caring what happens inside it. In this article, I will go in depth of its logic, its components, and how the Spark session extension participates to change the Catalyst\'s plans.\\n\\n![spark catalyst optimizer](./images/spark-catalyst-optimizer.JPG)\\n\\n\x3c!--truncate--\x3e\\n\\n## TreeNode\\n\\nThe main components in Catalyst are represented as tree nodes, which are inherited from class `TreeNode`, or its subclasses. Class `TreeNode` has a set of child nodes with the attribute `children`, datatype `Seq[BaseType]`, therefore, one `TreeNode` can have 0 or more child nodes. These objects are immutable and manipulated using functional transformations, making the debug optimizer easier and parallel operations more predictable.\\n\\nThe two important classes are `LogicalPlan` and `SparkPlan` are both subclasses of `QueryPlan`, the class inherits directly from `TreeNode`. In the above Catalyst diagram, the first 3 components are logical plans, the nodes in the logical plan are usually logical operators such as `CreateTableCommand`, `Filter`, `Project`,... the two components behind are spark plans (physical plans), nodes are usually low-level operators like `ShuffledHashJoinExec`, `SortMergeJoinExec`, `BroadcastHashJoinExec`, `FileSourceScanExec`,...\\n\\nLeaf nodes will read data from sources, storage, memory ,... and the root node of the tree is the outermost operator and returns the result of the calculation.\\n\\n## Rules\\n\\nTo manipulate the TreeNode we use rules, rules are actually components that transforms the tree, from one tree to another. In the rule, we implement the logic that transforms the TreeNode, which often uses the pattern matching in Scala to find the corresponding matches in its subtree and replace it with other constructs. Trees provide transformation functions that can apply this pattern matching to transform trees like `transform`, `transformDown`, `transformUp`,...\\n\\n```scala\\npackage org.apache.spark.sql.catalyst.trees\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\\n   * When `rule` does not apply to a given node it is left unchanged.\\n   * Users should not expect a specific directionality. If a specific directionality is needed,\\n   * transformDown or transformUp should be used.\\n   *\\n   * @param rule the function used to transform this nodes children\\n*/\\ndef transform(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformDown(rule)\\n}\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\\n   * When `rule` does not apply to a given node it is left unchanged.\\n   * Users should not expect a specific directionality. If a specific directionality is needed,\\n   * transformDown or transformUp should be used.\\n   *\\n   * @param rule   the function used to transform this nodes children\\n   * @param cond   a Lambda expression to prune tree traversals. If `cond.apply` returns false\\n   *               on a TreeNode T, skips processing T and its subtree; otherwise, processes\\n   *               T and its subtree recursively.\\n   * @param ruleId is a unique Id for `rule` to prune unnecessary tree traversals. When it is\\n   *               UnknownRuleId, no pruning happens. Otherwise, if `rule` (with id `ruleId`)\\n   *               has been marked as in effective on a TreeNode T, skips processing T and its\\n   *               subtree. Do not pass it if the rule is not purely functional and reads a\\n   *               varying initial state for different invocations.\\n*/\\ndef transformWithPruning(cond: TreePatternBits => Boolean,\\nruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n: BaseType = {\\n    transformDownWithPruning(cond, ruleId)(rule)\\n}\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to it and all of its\\n   * children (pre-order). When `rule` does not apply to a given node it is left unchanged.\\n   *\\n   * @param rule the function used to transform this nodes children\\n*/\\ndef transformDown(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformDownWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\\n}\\n\\ndef transformDownWithPruning(cond: TreePatternBits => Boolean,\\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n  : BaseType = {\\n    /* More code */\\n}\\n\\ndef transformUp(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformUpWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\\n}\\n\\ndef transformUpWithPruning(cond: TreePatternBits => Boolean,\\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n  : BaseType = {\\n    /* More code */\\n}\\n\\n/* ... */\\n```\\n\\nHere is a simple example of using transform and partn matching to transform one Treenode to another\\n\\n```scala\\npackage com.tranlam\\n\\nimport org.apache.spark.sql.catalyst.expressions.{Add, BinaryOperator, Expression, IntegerLiteral, Literal, Multiply, Subtract, UnaryMinus}\\nimport org.apache.spark.SparkConf\\nimport org.apache.spark.sql.SparkSession\\n\\nobject TestTransform {\\n  def main(args: Array[String]): Unit = {\\n    val sparkConf = new SparkConf().setAppName(\\"test_transform\\").setMaster(\\"local[*]\\")\\n    val spark = SparkSession.builder().config(sparkConf).getOrCreate()\\n    val firstExpr: Expression = UnaryMinus(Multiply(Subtract(Literal(11), Literal(2)), Subtract(Literal(9), Literal(5))))\\n    val transformed: Expression = firstExpr transformDown {\\n      case BinaryOperator(l, r) => Add(l, r)\\n      case IntegerLiteral(i) if i > 5 => Literal(1)\\n      case IntegerLiteral(i) if i < 5 => Literal(0)\\n    }\\n    println(firstExpr) // -((11 - 2) * (9 - 5))\\n    println(transformed) // -((1 + 0) + (1 + 5))\\n    spark.sql(s\\"SELECT ${firstExpr.sql}\\").show()\\n    spark.sql(s\\"SELECT ${transformed.sql}\\").show()\\n  }\\n}\\n```\\n\\nIn the above example, the transformDown function is used, which traverses the nodes of a tree and uses pattern matching to return a different result. If the node is a binary operator like Multiply, Subtract, it will convert to Add. If node is an integer constant greater than 5, it will change to 1, constant less than 5 will change to 0, a constant of 5 will keep the same value.\\n\\n## Catalyst Operations in Spark SQL\\n\\nSpark Catalyst uses tree transformations in four main phases: (1) logical plan analysis to traverse the relations in that plan, (2) logical plan optimization, (3) physical planning, (4) code generation to compile the query into Java bytecode.\\n\\n### Parsing and Analyzing\\n\\n![spark catalyst parseing analyzing](./images/catalyst-pipeline-parsing-analyzing.PNG)\\n\\nIn this phase, Catalyst rules and Catalog objects will be used by Spark SQL to check if the relations in our query exist or not, relation properties such as columns, column names are also checked, the syntax of the query is examined and then resolve those relations.\\n\\nFor example, looking at the query plan below, Spark SQL will first transform the query into a parsed tree called an \\"unresolved logical plan\\" with undefined attributes and datatypes, not yet assigned to a specific table (or alias). Then it will\\n\\n- Search for relation by name from Catalog object.\\n- Mapping properties as columns of input with found relations.\\n- Decide which properties should point to the same value to assign it a unique ID (for the purpose of later optimizing expressions like `col = col`).\\n- Cast expressions of a specific datatype (for example, we won\'t know the return datatype of `col * 2` until col is resolved and the datatype is determined).\\n\\n```sql\\nSELECT * FROM test.describe_abc;\\n\\n== Parsed Logical Plan ==\\n\'Project [*]\\n+- \'UnresolvedRelation [test, describe_abc], [], false\\n\\n== Analyzed Logical Plan ==\\nid: int, name: string\\nProject [id#5833, name#5834]\\n+- SubqueryAlias spark_catalog.test.describe_abc\\n   +- Relation test.describe_abc[id#5833,name#5834] parquet\\n\\n== Optimized Logical Plan ==\\nRelation test.describe_abc[id#5833,name#5834] parquet\\n\\n== Physical Plan ==\\n*(1) ColumnarToRow\\n+- FileScan parquet test.describe_abc[id#5833,name#5834] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://bigdataha/user/hive/warehouse/test.db/describe_abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string>\\n```\\n\\n### Logical plan optimizations\\n\\n![spark LP optimization](./images/catalyst-pipeline-LP-optimization.PNG)\\n\\nCatalyst applies standard optimization rules to the logical plan analyzed in the previous step, with cached data. This section includes rules like\\n\\n- Constant folding: removes expressions that compute a value that we can define before the code runs, for example, with the expression `y = x * 2 * 2`, compiler will not generate 2 multiply instructions, it will first replace the constants before the values \u200b\u200bcan be computed `y = x * 4`.\\n- Predicate pushdown: push down parts of the query to where the data is stored, filter large amounts of data, improve network traffic.\\n- Projection: read only selected columns, less columns will be passed from the storage to Spark, significantly efficient with columnar file format such as Parquet.\\n- Boolean expression simplification: eg. A and (A or B) = A(A+B) = A.A + A.B = A + A.B = A.(1+B) = A\\n- Many other rules,\u2026\\n\\nSpark\'s Catalyst optimizer will include batches of rules, some of which can exist in multiple batches. Usually these batch rules will be run once on that plan, however, there are some batches that will run repeatedly until a certain number of passes.\\n\\n### Physical planning\\n\\n![spark PP planning](./images/catalyst-pipeline-PP-planning.PNG)\\n\\nSpark SQL takes a logical plan and generates one or more physical plans, then it chooses the appropriate physical plan based on the cost models. Cost models typically rely on relational statistics, quantifying statistics flowing into a node in a TreeNode such as\\n\\n- Size of data flowing into node.\\n- Number of records per table.\\n- Statistical indexes related to columns such as: number of distinct values and nulls, minimum and maximum value, average and maximum length of the values, an equi-height histogram of the values,...\\n\\nSome Spark SQL approaches to this cost model\\n\\n- Size-only approach: only uses statistics about the physical size of the data flowing into the node, also take the number of records index in some cases.\\n- Cost-based approach: statistics related to column level information for Aggregate, Filter, Join, Project nodes (note, cost-based approach is only applicable to nodes of this type, with other types of nodes, it will revert to using the size-only approach), improving the size and number of records for those nodes.\\n\\nThe cost-based approach is chosen if we set `spark.sql.cbo.enabled=true`. Besides, table and column statistics also need to be collected so that Spark can calculate based on it, by running **[ANALYZE](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html)**\\n\\n### Code generation\\n\\n![spark codegen](./images/catalyst-pipeline-codegen.PNG)\\n\\nAfter selecting the right physical plan to run, Catalyst will compile a tree of plans that support codegen into a single Java function, to Java bytecode to run on drivers and executors. This codegen greatly improves running speed when Spark SQL often works on in-memory datasets, data processing is often tied to the CPU.\\n\\n- Eliminates virtual function calls and thus, reduces the number of CPU instructions.\\n- Leverages CPU registers for intermediate data.\\n\\nCatalyst relies on a Scala feature, quasiquotes, to simplify this part of the codegen (quasiquotes allow building abstract syntax trees (ASTs), which then input the Scala compiler to generate bytecode).\\n\\n## Spark session extension\\n\\nSpark session extension is an extension of Spark that allows us to customize parts of the Catalyst optimizer so that it works in each of our contexts.\\n\\n### Custom parser rule\\n\\nAs shown above, initially our query will have to go through the parsing set to check the validity of the query. Spark provides an interface that we can implement at this stage `ParserInterface`\\n\\n```scala\\npackage org.apache.spark.sql.catalyst.parser\\n\\n@DeveloperApi\\ntrait ParserInterface {\\n  @throws[ParseException](\\"Text cannot be parsed to a LogicalPlan\\")\\n  def parsePlan(sqlText: String): LogicalPlan\\n\\n  @throws[ParseException](\\"Text cannot be parsed to an Expression\\")\\n  def parseExpression(sqlText: String): Expression\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a TableIdentifier\\")\\n  def parseTableIdentifier(sqlText: String): TableIdentifier\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a FunctionIdentifier\\")\\n  def parseFunctionIdentifier(sqlText: String): FunctionIdentifier\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a multi-part identifier\\")\\n  def parseMultipartIdentifier(sqlText: String): Seq[String]\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a schema\\")\\n  def parseTableSchema(sqlText: String): StructType\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a DataType\\")\\n  def parseDataType(sqlText: String): DataType\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a LogicalPlan\\")\\n  def parseQuery(sqlText: String): LogicalPlan\\n}\\n```\\n\\nWe will implement that interface and inject this rule into Spark job as follows\\n\\n```scala\\ncase class CustomerParserRule(sparkSession: SparkSession, delegateParser: ParserInterface) extends ParserInterface {\\n  /* Overwrite those methods here */\\n}\\n\\nval customerParserRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectParser(CustomerParserRule)\\n}\\n```\\n\\n### Custom analyzer rule\\n\\nAnalyzer rule includes several types of rules such as resolution rule, check rule. These rules are injected through functions\\n\\n- `injectResolutionRule`: inject our rules for the resolution phase.\\n- `injectPostHocResolutionRule`: run our rules after the resolution phase.\\n- `injectCheckRule`: add rules to check some logic of logical plans, for example, we want to check business logic, or check which rules have finished running,...\\n\\nTo inject resolution rule, we extend an abstract class of Spark `Rule[LogicalPlan]`\\n\\n```scala\\ncase class CustomAnalyzerResolutionRule(sparkSession: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    /* Code for resolution rule */\\n  }\\n}\\n\\nval customAnalyzerResolutionRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectResolutionRule(CustomAnalyzerResolutionRule)\\n}\\n```\\n\\nTo inject check rule, we inherit the class `Function1[LogicalPlan, Unit]`\\n\\n```scala\\ncase class CustomAnalyzerCheckRule(sparkSession: SparkSession) extends (LogicalPlan => Unit) {\\n  override def apply(plan: LogicalPlan): Unit = {\\n    /* Code for check rule */\\n  }\\n}\\n\\nval customAnalyzerCheckRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectCheckRule(CustomAnalyzerCheckRule)\\n}\\n```\\n\\n### Custom optimization\\n\\nTo customize the logical plan optimization phase, we will inherit the abstract class `Rule[LogicalPlan]`\\n\\n```scala\\ncase class CustomOptimizer(sparkSession: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    /* Code for custom logical optimier */\\n  }\\n}\\n\\nval customOptimizerFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectOptimizerRule(CustomOptimizer)\\n}\\n```\\n\\n### Custom physical planning\\n\\nTo configure the running strategy for Spark Catalyst optimizer, we inherit the abstract class `SparkStrategy` and implement the method `apply` of that class\\n\\n```scala\\ncase class CustomStrategy(sparkSession: SparkSession) extends SparkStrategy {\\n  override def apply(plan: LogicalPlan): Seq[SparkPlan] = {\\n    /* Code for custom spark strategy/physical planning */\\n  }\\n}\\n\\nval customStrategyFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectPlannerStrategy(CustomStrategy)\\n}\\n```\\n\\n### Example code to configuree logical plan optimization phase in Catalyst optimizer\\n\\nIn this section, I will make an example of changing logical plan optimization phase with Spark extension. A simple extension with code as below\\n\\n```scala\\n/* class CustomProjectFilterExtension ======================================= */\\npackage extensions\\nimport org.apache.spark.sql.SparkSession\\nimport org.apache.spark.sql.catalyst.plans.logical._\\nimport org.apache.spark.sql.catalyst.rules.Rule\\n// create an extension that\\ncase class CustomProjectFilterExtension(spark: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    val fixedPlan = plan transformDown {\\n      case Project(expression, Filter(condition, child)) =>\\n          Filter(condition, child)\\n    }\\n    fixedPlan\\n  }\\n}\\n\\n/* class AllExtensions ======================================= */\\npackage extensions\\nimport org.apache.spark.sql.SparkSessionExtensions\\n// inject the extension to SparkSessionExtensions\\nclass AllExtensions extends (SparkSessionExtensions => Unit) {\\n  override def apply(ext: SparkSessionExtensions): Unit = {\\n    ext.injectOptimizerRule(CustomProjectFilterExtension)\\n  }\\n}\\n```\\n\\nThe above class `CustomProjectFilterExtension` transforms Filter (row filter), Project (select column while scanning file) operators to only Filter. Then, even though we have selected the column, it still scans all the columns of the file in the storage.\\n\\nCompile project\\n\\n```bash\\n# compile jar file\\nmvn clean package && mvn dependency:copy-dependencies\\n```\\n\\n#### When not applying extension\\n\\nWe initialize `spark-shell` without passing extension\\n\\n```bash\\n# initialize spark-shell\\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr \' \' \',\'),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar\\n\\n# check spark.sql.extensions\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = null\\n\\n# explain a query that contains Filter and Project operators\\nscala> spark.sql(\\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel=\'Resort Hotel\') a\\").explain(extended = true)\\n\\n== Parsed Logical Plan ==\\n\'Project [\'hotel, \'is_canceled]\\n+- \'SubqueryAlias a\\n   +- \'Project [*]\\n      +- \'Filter (\'hotel = Resort Hotel)\\n         +- \'UnresolvedRelation [test, hotel_bookings], [], false\\n\\n== Analyzed Logical Plan ==\\nhotel: string, is_canceled: bigint\\nProject [hotel#0, is_canceled#1L]\\n+- SubqueryAlias a\\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\\n      +- Filter (hotel#0 = Resort Hotel)\\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Optimized Logical Plan ==\\nProject [hotel#0, is_canceled#1L]\\n+- Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n   +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Physical Plan ==\\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint>\\n```\\n\\nWe see that `Optimized Logical Plan` there are both Project and Filter operations, because we filter `WHERE hotel=\'Resort Hotel\'` and project `SELECT hotel, is_canceled`. Therefore, in the physical plan, it only scans 2 columns `FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L]`.\\n\\n#### When applying extension\\n\\n```bash\\n# initialize spark-shell with extension\\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr \' \' \',\'),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar --conf spark.sql.extensions=extensions.AllExtensions\\n\\n# check spark.sql.extensions\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = extensions.AllExtensions\\n\\n# explain a query that contains Filter and Project operators\\nscala> spark.sql(\\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel=\'Resort Hotel\') a\\").explain(extended = true)\\n\\n== Parsed Logical Plan ==\\n\'Project [\'hotel, \'is_canceled]\\n+- \'SubqueryAlias a\\n   +- \'Project [*]\\n      +- \'Filter (\'hotel = Resort Hotel)\\n         +- \'UnresolvedRelation [test, hotel_bookings], [], false\\n\\n== Analyzed Logical Plan ==\\nhotel: string, is_canceled: bigint\\nProject [hotel#0, is_canceled#1L]\\n+- SubqueryAlias a\\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\\n      +- Filter (hotel#0 = Resort Hotel)\\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Optimized Logical Plan ==\\nFilter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Physical Plan ==\\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint,lead_time:bigint,arrival_date_year:bigint,arrival_date_mon...\\n```\\n\\nAt this point, `Optimized Logical Plan` there is no longer Project operator, but only Filter operator, so that when it comes to the physical plan step, it scans all the columns in the table `FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields]`.\\n\\nAbove, I have specifically presented the components of Spark Catalyst optimizer and how to write spark session extensions to intervene to change Catalyst\'s plans, there are also specific code examples to demonstrate this. In the next article, I will present one more part that is a new feature in Spark 3.0, which is Spark Adaptive Query Execution, a feature that improves Spark job speed at runtime.\\n\\n## References\\n\\n[Deep Dive into Spark SQL\'s Catalyst Optimizer](https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\\n\\n[Spark Catalyst Pipeline: A Deep Dive Into Spark\u2019s Optimizer](https://www.unraveldata.com/resources/catalyst-analyst-a-deep-dive-into-sparks-optimizer/)\\n\\n[Extending Apache Spark Catalyst for Custom Optimizations](https://medium.com/@pratikbarhate/extending-apache-spark-catalyst-for-custom-optimizations-9b491efdd24f)"},{"id":"mysql-series-mysql-indexing/","metadata":{"permalink":"/blog/mysql-series-mysql-indexing/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-10-15-mysql-indexing/index.md","source":"@site/blog/2022-10-15-mysql-indexing/index.md","title":"MySQL series - Indexing","description":"MySQL series - Indexing","date":"2022-10-15T00:00:00.000Z","formattedDate":"October 15, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blog/tags/my-sql"},{"label":"Database","permalink":"/blog/tags/database"},{"label":"Data Engineering","permalink":"/blog/tags/data-engineering"},{"label":"Indexing","permalink":"/blog/tags/indexing"}],"readingTime":10.33,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-indexing/","title":"MySQL series - Indexing","description":"MySQL series - Indexing","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Indexing"],"image":"./images/indexing.PNG"},"prevItem":{"title":"Spark Catalyst Optimizer And Spark Session Extension","permalink":"/blog/spark-catalyst-optimizer-and-spark-session-extension/"},"nextItem":{"title":"MySQL series - Multiversion concurrency control","permalink":"/blog/mysql-series-mysql-mvcc/"}},"content":"Indexing is a method to make queries faster, which is a very important part of improving performance. For large data tables, precise indexing will increase the query speed as a whole, however, this is often not taken into account in the table design process. This article talks about the types of indexes and how to properly index them.\\n\\n![Indexing](./images/indexing.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Types of index\\n\\nThere are many types of indexes designed for different purposes. Remember, indexes are implemented at the storage engine layer, not at the server layer, so they behave differently in different storage engines. The types of indexes in this article are mainly about indexes in InnoDB.\\n\\n### B-tree index\\n\\nB-tree index uses a balanced tree to store its data, almost all MySQL storage engines support this type of index (or its variant), for example, the NDB Cluster storage engine uses the data structure T-tree for indexing, InnoDB uses B+ tree,...\\n\\nIn B-tree, all values \u200b\u200bare sorted, and leaves are equally spaced from the root of the tree. Below figure is a description of the B-tree data structure.\\n\\n![B Tree](./images/BTree.PNG)\\n\\nB-trees provide the ability to search, access sequential data, insert and delete with logarithmic time complexity ${O(log(n))}$. At the root node, there will be pointers to the child nodes, when we query, the storage engine will know the appropriate subnode branch to browse by looking at the values \u200b\u200bin the node pages, containing the upper and lower threshold information, child nodes in that page. At the leaf page layer, pointers point to data instead of other pages.\\n\\nIn the image above, we only see a node page and leaf pages. In fact, the B-tree has many layers of node pages between the root node and the leaf nodes, the size of the tree depends on the size of the indexed table.\\n\\n#### Adaptive hash index\\n\\nWhen index values \u200b\u200bare accessed with high frequency, InnoDB will build a hash index for them in memory on top of the B-tree index, making it possible to find this hash value very quickly and efficiently. This mode is automatic by InnoDB, however, you can still disable adaptive hash index if you want.\\n\\n#### Types of query that are efficient with B-tree index\\n\\nB-tree indexes work well with exact-value, range, or value-prefix query types. These queries are best when we use them on the leftmost column in the indexed set of columns.\\n\\n```sql\\nCREATE TABLE People (\\n     last_name varchar(50) not null,\\n     first_name varchar(50) not null,\\n     dob date not null,\\n     KEY `idx_full_col` (last_name, first_name, dob)\\n) ENGINE=InnoDB;\\n```\\n\\n- Exact match: when the columns in the index are queried to match a certain value, for example `WHERE last_name = \'lam\' AND first_name = \'tran\' AND dob = \'1999-05-10\'`. This type of query will return results very quickly.\\n- Match the leftmost column: for example, if we query to find people with `last_name = \'lam\'`.\\n- Match the first part of the left most column: For example, when we find the person whose last_name starts with the letter \'L\'.\\n- Match a range of values: when we need to get the set of people whose last_name is between \'anh\' and \'lam\'.\\n- Match the leftmost column and a range of the next column values: for example, when we need information about people last_name is \'lam\' and first_name starts with \'t\'.\\n\\n#### Drawbacks of B-tree index\\n\\n- It won\'t really help when the query condition doesn\'t start with the leftmost column, nor is it good when the query finds people whose last_name ends with a specific letter.\\n- Queries that skip some columns also don\'t take full advantage of the index. For example when looking for people `last_name = \'lam\' AND dob = \'1999-05-10\'` with no condition on first_name.\\n- Indexes of this type will not take advantage of the columns behind the range matching column. For example, the query people `last_name = \'lam\' AND first_name LIKE \'t%\' AND dob = \'1999-05-10\'` will only apply the index on the last_name and first_name columns. For columns with less distinct data, we can overcome this by enumerating all values \u200b\u200binstead of accessing the range of values.\\n\\nThus, the order of the columns in the index is really important, you need to consider the query goal of the application before indexing the columns.\\n\\n### Full-text index\\n\\nThe full-text index searches for keywords in the text string instead of comparing the field\'s value directly. It aids in searching rather than judging what type the data matches. When a column has a full-text index, we can still type a B-tree index on that column.\\n\\n```sql\\nCREATE TABLE tutorial (\\n    id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,\\n    title VARCHAR(200),\\n    description TEXT,\\n    FULLTEXT `idx_full_text` (title,description)\\n) ENGINE=InnoDB;\\n```\\n\\nThe full-text index is used by syntax `MATCH() AGAINST()` with the parameter of `MATCH()` are columns to search, separated by commas. The parameter of `AGAINST()` is a string to search and type of search to perform.\\n\\n#### Types of full-text index\\n\\n- Natural language search: this mode will interpret the search string as a phrase in natural human language. This mode does not count stopwords as well as words shorter than the minimum number of characters (default is 3 characters with InnoDB).\\n- Boolean search: interprets the search string using special query language rules. The string contains all the words to be searched, it can also contain special operators for advanced searches, such as a word that needs to appear in the string, or a word that is weighted heavier or lighter. Stop words will be ignored in this mode.\\n- Query expansion: is a variation of natural language search. The words in the most relevant rows returned will be added to the search string, and the search will be repeated. The query will return rows in the second search.\\n\\nI won\'t go into each type in detail, because I rarely use the full-text index.\\n\\n## Benefits of indexing\\n\\nSome benefits of indexing\\n\\n- Index helps server save time for browsing and querying.\\n- Index helps the server avoid operations such as sorting data or creating temporary tables.\\n- Index turns random disk access into sequential access, improving read speed\\n\\nSome criteria to evaluate index\\n\\n- Index needs to arrange related rows, closer together.\\n- The sorted rows should be exactly what your application queries need.\\n- Index needs to contain all the columns that your application query filters.\\n\\n## Indexing strategies\\n\\nCreating the right indexes will greatly improve your query speed, which in turn makes your application more responsive to users.\\n\\n### Prefix index for text field\\n\\nConsider Index Selectivity is the ratio between the number of different column values \u200b\u200b/ total records of the table. For columns with high Index Selectivity, indexing on these fields is very effective because MySQL will remove more records when filtering on those columns. For long text fields, we cannot index the whole column length because MySQL won\'t allow that, so we need to find a good enough prefix of that field to index and it will give us a good enough performance.\\n\\nTry with the product data below, we list the top ten sellers that appear the most\\n\\n```sql\\nselect productVendor, count(1) c from `classicmodels`.`products_index`\\ngroup by productVendor\\norder by c desc\\nLIMIT 10;\\n\\n+--------------------------------------------------+----+\\n| productVendor                                    | c  |\\n+--------------------------------------------------+----+\\n| Pressure and Safety Relief Valve                 | 10 |\\n| NEC United Solutions                             |  9 |\\n| SunGard Data Systems                             |  8 |\\n| Zhengzhou Esunny Information Technology Co.,Ltd. |  8 |\\n| Spring Support                                   |  8 |\\n| Ball and Plug Valve                              |  7 |\\n| LSAW Pipe                                        |  7 |\\n| Wood Mackenzie Ltd                               |  7 |\\n| Heat Recovery Steam Generator                    |  7 |\\n| Carbon Steel Flange                              |  7 |\\n+--------------------------------------------------+----+\\n```\\n\\nTry to calculate the frequency of occurrence of length 3 prefix with the field `productVendor`\\n\\n```sql\\nselect LEFT(productVendor, 3), count(1) c from `classicmodels`.`products_index`\\ngroup by LEFT(productVendor, 3)\\norder by c desc\\nLIMIT 10;\\n\\n+------------------------+----+\\n| LEFT(productVendor, 3) | c  |\\n+------------------------+----+\\n| Sha                    | 44 |\\n| Car                    | 16 |\\n| Sun                    | 15 |\\n| Zhe                    | 13 |\\n| Gas                    | 12 |\\n| Sto                    | 11 |\\n| Pre                    | 11 |\\n| Col                    | 11 |\\n| She                    |  9 |\\n| Hea                    |  9 |\\n+------------------------+----+\\n```\\n\\nWe see that the frequency of occurrence of length 3 prefix is a lot more compare to full column values, which equates to fewer distinct values, which equates to a much smaller Index Selectivity. So prefix 3 is not a good choice\\n\\nLet\'s calculate the Index Selectivity with various prefix lengths\\n\\n```sql\\nselect COUNT(DISTINCT LEFT(productVendor, 3))/COUNT(1) AS selectivity_3,\\nCOUNT(DISTINCT LEFT(productVendor, 4))/COUNT(1) AS selectivity_4,\\nCOUNT(DISTINCT LEFT(productVendor, 5))/COUNT(1) AS selectivity_5,\\nCOUNT(DISTINCT LEFT(productVendor, 6))/COUNT(1) AS selectivity_6,\\nCOUNT(DISTINCT LEFT(productVendor, 7))/COUNT(1) AS selectivity_7,\\nCOUNT(DISTINCT LEFT(productVendor, 8))/COUNT(1) AS selectivity_8,\\nCOUNT(DISTINCT LEFT(productVendor, 9))/COUNT(1) AS selectivity_9,\\nCOUNT(DISTINCT LEFT(productVendor, 10))/COUNT(1) AS selectivity_10,\\nCOUNT(DISTINCT LEFT(productVendor, 11))/COUNT(1) AS selectivity_11,\\nCOUNT(DISTINCT productVendor)/COUNT(1) AS selectivity\\nfrom `classicmodels`.`products_index`;\\n\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n| selectivity_3 | selectivity_4 | selectivity_5 | selectivity_6 | selectivity_7 | selectivity_8 | selectivity_9 | selectivity_10 | selectivity_11 | selectivity |\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n|        0.1982 |        0.2164 |        0.2218 |        0.2236 |        0.2236 |        0.2273 |        0.2309 |         0.2491 |         0.2509 |      0.2600 |\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n```\\n\\nWe see that the selectivity prefix 11 is very close to the column selectivity value, and is also quite suitable for long text fields like this column, so choosing prefix 11 will balance the size of the index as well as the speed of the query.\\n\\n```sql\\nALTER TABLE `classicmodels`.`products_index` ADD KEY (productVendor(11));\\n```\\n\\n### Index on multiple columns\\n\\nSome mistakes when indexing is indexing each column separately, and creating indexes for all columns in the WHERE statement.\\n\\n```sql\\nCREATE TABLE t (\\n     c1 INT,\\n     c2 INT,\\n     c3 INT,\\n     KEY(c1),\\n     KEY(c2),\\n     KEY(c3)\\n);\\n```\\n\\nSeparate indexes like the one above will usually not optimize performance very much in most situations, because then MySQL can use a tactic called index merge. Index merge will use all the indexes in the query, scan the indexes simultaneously, then merge the results again.\\n\\n- Union index will be used for OR condition\\n- Intersection index will be used for AND condition\\n- Union of intersection index for the union of both 2\\n\\nHere is an example query on 2 index fields but MySQL uses index merge\\n\\n```sql\\nmysql> explain select * from `classicmodels`.`products_index` where productVendor = \'Infor Global Solutions\' OR productScale = \'1:10\'\\\\G\\n*************************** 1. row ***************************\\n           id: 1\\n  select_type: SIMPLE\\n        table: products_index\\n   partitions: NULL\\n         type: index_merge\\npossible_keys: productVendor,productScale\\n          key: productVendor,productScale\\n      key_len: 14,12\\n          ref: NULL\\n         rows: 33\\n     filtered: 100.00\\n        Extra: Using sort_union(productVendor,productScale); Using where\\n```\\n\\nSome considerations when query encounters index merge\\n\\n- If the server intersects the index (AND condition on the indexes), it means that you can create an index containing all the columns related to each other, not each index for each column.\\n- If the server union index (OR condition on the indexes), check if those columns have high Index Selectivity, if the Index Selectivity in some columns is low, it means that the column has few different values, that is, the scan index returns more records for the merge operations that follow it, consuming more CPU and memory. Sometimes, rewriting the query with the UNION statement gives better results than when the server unions the indexes in the index merge.\\n\\nWhen you see the index merge in the EXPLAIN statement, review the query and table structure to check if the current design is optimal.\\n\\n### Choose the correct order of columns to index\\n\\nWhen our index contains many columns, the order of columns in that index is very important, because in B-tree index, the index will be sorted **[from the leftmost column to the next columns](#113-drawbacks-of-b-tree-index)**. Therefore, we often choose the columns with the highest Index Selectivity as the leftmost column, order the columns in descending order of Index Selectivity, so that our overall index has high selectivity.\\n\\n```sql\\nselect count(distinct productVendor)/count(1),\\n\\tcount(distinct productScale)/count(1)\\nfrom `classicmodels`.`products_index`;\\n\\n+----------------------------------------+---------------------------------------+\\n| count(distinct productVendor)/count(1) | count(distinct productScale)/count(1) |\\n+----------------------------------------+---------------------------------------+\\n|                                 0.2600 |                                0.0145 |\\n+----------------------------------------+---------------------------------------+\\n```\\n\\nIn the above example, if we index 2 columns `productVendor` and `productScale`, we will usually take `productVendor` as the leftmost column\\n\\n```sql\\nalter table `classicmodels`.`products_index` add key (productVendor, productScale);\\n```\\n\\nSome more considerations about the index to pay attention to such as clustered index, covering index, remove redundant, unused indexes, ... I would like to mention in another article."},{"id":"mysql-series-mysql-mvcc/","metadata":{"permalink":"/blog/mysql-series-mysql-mvcc/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-10-07-mysql-multiversion-concurrency-control/index.md","source":"@site/blog/2022-10-07-mysql-multiversion-concurrency-control/index.md","title":"MySQL series - Multiversion concurrency control","description":"MySQL series - Multiversion concurrency control","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blog/tags/my-sql"},{"label":"Database","permalink":"/blog/tags/database"},{"label":"Data Engineering","permalink":"/blog/tags/data-engineering"},{"label":"Transaction","permalink":"/blog/tags/transaction"}],"readingTime":2.025,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-mvcc/","title":"MySQL series - Multiversion concurrency control","description":"MySQL series - Multiversion concurrency control","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Transaction"],"image":"./images/overall.PNG"},"prevItem":{"title":"MySQL series - Indexing","permalink":"/blog/mysql-series-mysql-indexing/"},"nextItem":{"title":"MySQL series - Transaction In MySQL","permalink":"/blog/mysql-series-mysql-transaction/"}},"content":"Usually storage engines do not use a simple row lock mechanism, to achieve good performance in a highly concurrent read and write environment, storage engines implement row locking with a certain complexity, the method is often used, is multiversion concurrency control (MVCC).\\n\\n![MVCC Overall](./images/overall.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction to MVCC\\n\\nMVCC is used in many types of relational databases, it helps us to lock as little data as possible when doing many transactions at once, it can allow us to not be locked when reading data and only lock the necessary rows when writing data.\\n\\n## MVCC in InnoDB\\n\\nMVCC works by taking snapshots of data at some point in time, so a transaction can see the same data no matter how fast or long it takes. However, it also causes different transactions to see different data views of the same table at the same time.\\n\\n![MVCC Detail Example](./images/detail.PNG)\\n\\nInnoDB will assign a transaction id to a transaction every time it starts reading some data. The changes of a record in that transaction will be written to the undo log for data revert, and the rollback pointer of that transaction will point to the location of that undo log. When another session starts reading the mutated record above, InnoDB compares the transaction id of the record with the data view that new session read. If the record is in a state that is invisible to other transactions (e.g. a transaction that changes that record has not been committed), the undo log will be applied on the data view until the record becomes available again, readable by other transactions.\\n\\nAll undo logs recorded are copied to the redo log because they are used for data recovery in the event of a system failure. The size of the undo log and redo log also affects the ability to perform read and write in environments with high concurrent read and write.\\n\\nWhile the benefit is that we are never locked when reading, the storage engine needs to store more data with each record, do more control work, and perform more operations.\\n\\n## Isolation level with MVCC\\n\\nMVCC is only available with REPEATABLE READ and READ COMMITTED modes. MVCC is not compatible with READ UNCOMMITTED because queries will not read records whose version does not match the transaction\'s version. MVCC is not compatible with SERIALIZABLE because of its read locking (you can find **[four isolation levels](/blog/2022-10-06-mysql-transaction/index.md#3-four-isolation-level-in-highly-concurrent-read-and-write-environments)** in my previous blog)."},{"id":"mysql-series-mysql-transaction/","metadata":{"permalink":"/blog/mysql-series-mysql-transaction/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-10-06-mysql-transaction/index.md","source":"@site/blog/2022-10-06-mysql-transaction/index.md","title":"MySQL series - Transaction In MySQL","description":"MySQL series - Transaction In MySQL","date":"2022-10-06T00:00:00.000Z","formattedDate":"October 6, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blog/tags/my-sql"},{"label":"Database","permalink":"/blog/tags/database"},{"label":"Data Engineering","permalink":"/blog/tags/data-engineering"},{"label":"Transaction","permalink":"/blog/tags/transaction"}],"readingTime":5.31,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-transaction/","title":"MySQL series - Transaction In MySQL","description":"MySQL series - Transaction In MySQL","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Transaction"],"image":"./images/transaction.JPEG"},"prevItem":{"title":"MySQL series - Multiversion concurrency control","permalink":"/blog/mysql-series-mysql-mvcc/"},"nextItem":{"title":"MySQL series - MySQL Architecture Overview","permalink":"/blog/mysql-series-mysql-architecture/"}},"content":"![Poster](./images/transaction.JPEG)\\n\\nThe next article in the MySQL series is about transactions. A very common operation in MySQL in particular and relational databases in general. Let\'s go to the article.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is transaction?\\n\\nA transaction is a set of SQL statements put together as a unit of work. If the database successfully runs all SQL statements in that group, it is considered successful. If one of the SQL commands fails, all the SQL commands that have been run or not run will have no effect on the database. An example of a set of SQL statements wrapped in a transaction follows\\n\\n```sql\\n    1  START  TRANSACTION;\\n    2  SELECT balance FROM checking WHERE customer_id = 10233276;\\n    3  UPDATE checking SET balance = balance - 200.00 WHERE customer_id = 10233276;\\n    4  UPDATE savings SET balance = balance - 200.00 WHERE customer_id = 10233276;\\n    5  COMMIT;\\n```\\n\\nTransactions are started by START TRANSACTION and are usually closed by COMMIT (confirm transaction) or ROLLBACK (return to pre-transaction state). If the ${4^{th}}$ statement fails, the ${3^{rd}}$ statement will be rolledback and nothing will happen to affect the old data.\\n\\n## Four data preservation properties in relational database\\n\\n![ACID](./images/acid.PNG)\\n\\nEvery system needs to satisfy four ACID properties to ensure data preservation\\n\\n- Atomicity\\n\\nTransaction needs to act as a unit of work. Either all SQL statements in the transaction are applied or none are applied.\\n\\n- Consistency\\n\\nDatabase needs to be consistent, only being moved from one consistent state to another. The example above, if the error occurs after running the ${3^{rd}}$ statement, the checking account will not lose 200$ when the transaction has not been committed. The total money in the two accounts before and after the transaction remains the same.\\n\\n- Isolation\\n\\nThe result of this transaction will be invisible to other transactions when this transaction is not finished, not committed. For example, when transaction 1 is running between ${3^{rd}}$ and ${4^{th}}$ statements above, another transaction that summarizes the balances of the accounts will still see 200$ in the checking account. When a transaction is uncommitted, no changes will affect the database.\\n\\n- Durability\\n\\nOnce committed, the changes made by the transaction will be permanent, the changes need to be recorded to ensure that the data is not lost if the system fails.\\n\\n## Four isolation level in highly concurrent read and write environments\\n\\nThere are 4 isolation levels related to transactions\\n\\n- READ UNCOMMITTED\\n\\nIn this mode, transactions can see the results of other uncommitted transactions. This mode does not perform much faster than many other modes but easily causes problems when reading wrong data.\\n\\n- READ COMMITTED\\n\\nThe default mode of most databases (but not MySQL), it will lose some of the ACID Isolation properties, this transaction will be visible to changes by other transactions committed after this transaction starts, however changes to this transaction remain invisible until it is committed. This can cause two identical read statements in a transaction to return two different datasets.\\n\\n- REPEATABLE READ\\n\\nThis mode is the default of MySQL. It ensures that within the same transaction, the same read statements will return the same result. But there will also be a small problem that if we select a range of values, another transaction inserts a new record in that range, we will see that new record. Storage engines like InnoDB, XtraDB solve this problem by creating multiple versions of a record that manage concurrent reads and writes.\\n\\n- SERIALIZABLE\\n\\nThis mode solves the problem of reading a range of values \u200b\u200babove by running transactions in order. This mode will lock all the rows it reads, a lot of timeouts and locking occur frequently, concurrent reads and writes will be reduced.\\n\\n![Isolation Level](./images/isolation_levels.PNG)\\n\\n## Transaction deadlock\\n\\nDeadlock occurs when two or more transactions lock the same resources, creating a cycle of dependency\\n\\n```sql\\n-- Transaction 1\\n    START TRANSACTION;\\n    UPDATE StockPrice SET close = 45.50 WHERE stock_id = 4 and date = \u20182020-05-01\u2019;\\n    UPDATE StockPrice SET close = 19.80 WHERE stock_id = 3 and date = \u20182020-05-02\u2019;\\n    COMMIT;\\n-- Transaction 2\\n    START TRANSACTION;\\n    UPDATE StockPrice SET high = 20.12 WHERE stock_id = 3 and date = \u20182020-05-02\u2019;\\n    UPDATE StockPrice SET high = 47.20 WHERE stock_id = 4 and date = \u20182020-05-01\u2019;\\n    COMMIT;\\n```\\n\\nAfter these two transactions finish running the first command, when running the second command. The records with the corresponding id of this transaction are being locked by another transaction, as well as another transaction that is locked by this transaction. InnoDB will return an error if a dependency circle is detected. The way InnoDB handles deadlock is that it will rollback the transaction with the fewest locked rows.\\n\\n![Deadlock](./images/deadlock.JPEG)\\n\\n## Transaction logging\\n\\nTransaction logging makes transaction execution more efficient. Instead of updating directly to the disk table every time there is a change, it updates to the copy of the data in memory. Then the transaction log will be written to disk with append mode, this operation is very fast because only sequential I/O is required in disk, more cost-effective, after a while these changes will be applied to the actual data on disk. Because this log is written on disk, it will be durable, if the system fails after writing the transaction log to disk but before updating the changes to the main data, the storage engine can still recover those changes.\\n\\n![Transaction Log](./images/transaction_log.PNG)\\n\\n## Autocommit\\n\\nBy default, INSERT, UPDATE, and DELETE statements are wrapped in temporary transactions and committed as soon as they run, this is AUTOCOMMIT mode. To enable this mode run the sentence SET AUTOCOMMIT = 1; otherwise, SET AUTOCOMMIT = 0. Some special commands can commit a transaction while in an open transaction, such as DDL statements. We can set the isolation level for MySQL by running the SET TRANSACTION ISOLATION LEVEL command, after running this isolation level will take effect in subsequent transactions. You can set it in the configuration file for the whole server, or just set it in your session\\n\\n```sql\\nSET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;\\n```\\n\\nWe should not process tables with different storage engines in the same transaction, because there are some storage engines that will not support data rollback (MyISAM storage engine), if some error occurs during transaction execution, only some tables will be rolled back causing loss of consistency.\\n\\nThis is the end of the article, see you in the next blogs."},{"id":"mysql-series-mysql-architecture/","metadata":{"permalink":"/blog/mysql-series-mysql-architecture/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-10-04-mysql-architecture/index.md","source":"@site/blog/2022-10-04-mysql-architecture/index.md","title":"MySQL series - MySQL Architecture Overview","description":"MySQL series - MySQL Architecture Overview","date":"2022-10-04T00:00:00.000Z","formattedDate":"October 4, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blog/tags/my-sql"},{"label":"Database","permalink":"/blog/tags/database"},{"label":"Data Engineering","permalink":"/blog/tags/data-engineering"}],"readingTime":4.435,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-architecture/","title":"MySQL series - MySQL Architecture Overview","description":"MySQL series - MySQL Architecture Overview","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering"],"image":"./images/architecture.PNG"},"prevItem":{"title":"MySQL series - Transaction In MySQL","permalink":"/blog/mysql-series-mysql-transaction/"},"nextItem":{"title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","permalink":"/blog/spark-kafka-docker/"}},"content":"Hello everyone, recently, I did some research in MySQL because I think whoever doing data engineering should go in-depth with a certain relational database. Once you get a deep understanding of one RDBMS, you can easily learn the other RDBMS since they have many similarities. For the next few blogs, I will have a series about MySQL, and this is the first article.\\n\\n\x3c!--truncate--\x3e\\n\\n## MySQL architecture components\\n\\nMySQL is widely used not only in small applications but also in large enterprises, thanks to the features of its flexible architecture.\\n\\n![Architecture](./images/architecture.PNG)\\n\\nThe top layer is the Clients layer, this layer is usually not unique to MySQL. They are services like connection handling, authentication, security, etc.\\n\\nThe second layer is the layer that contains the code for query analysis, optimization and contains built-in functions that interact with the database such as dates, times, math, encryption, etc. All features are compatible with many storage engines like stored procedures, triggers, views, etc.\\n\\nThe third layer is storage engines, which are responsible for storing and retrieving data in MySQL. Each storage engine has its own good and bad sides. The MySQL server interacts with them by the storage engine API, which contains many low-level functions, operations such as starting a transaction, finding records with the corresponding primary key. Storage engines only respond to requests from the server, while parsing queries are made at the second layer.\\n\\n## Connection and security management\\n\\nWith the default configuration, each connection from the client will occupy one thread, and queries will run in that thread. The server will have a cache of threads ready to use, so they won\'t need to be created and destroyed every time there is a new connection from the client.\\n\\nWhen the client connects, the server will need to authenticate that connection based on the host, username, and password. After connecting, the server will check whether the client has permissions to specific database resources (eg, SELECT permission on which table on which database,\u2026).\\n\\n## MySQL optimizer\\n\\n![Overall](./images/overall.PNG)\\n\\nWhen running, MySQL will\\n\\n- Looking in the query cache to check if the query\'s results can be found, it will return the results immediately, otherwise it performs the next steps. The memory size of the query cache is assigned in the variable `query_cache_size`, if this variable is updated, MySQL will clear all cached queries one by one and re-initialize the query cache (this can be time consuming).\\n- Parse the queries into a tree containing the query\'s information. The query can be completely rewritten, the order in which the tables are read will be different, how the index should be selected, etc. We can interfere with that analysis by using hints to determine the order. What will it be like to run? At this time, the parser will build a parse tree for the query, besides, it also checks the syntax of the query.\\n- The preprocessor will check some other constraints such as this table, column or database exist or not, the user\'s authority to query to which resources.\\n- Then, the parse tree will be passed through an optimizer to convert it into a query execution plan. A MySQL query can be run in many ways, the optimizer will try to optimize the cost as much as possible (unit is 4KB data page), this cost can be seen by running `SHOW STATUS LIKE \'Last_query_cost\';`. The optimizer doesn\'t really care which storage engines are used, but the choice of storage engine has a big impact on how well the server optimizes the queries because the server needs information from storage engines such as the statistics of the tables, the cost of performing the operations, how the index is supported or the computing power of the storage engines to run more optimally. The optimizer may not be able to choose the best plan to run because the statistics from the storage engines are not absolute, the cost metric may not be equivalent to the cost of running the query, MySQL will try to reduce the cost but not the speed of the query, or user-defined functions will not be evaluated by the optimizer.\\n- The Query execution plan is a tree that contains each step to generate the results for the query, the server will perform those steps many times until there are no more records to retrieve. Query execution engine communicates with storage engines by storage engine APIs to perform operations according to the query execution plan.\\n- MySQL storage engines are a management system with each database being a subpath in that filesystem\'s data path. When creating a table, the table information is stored in the .frm file (for example, the table `users` is stored in the .frm file named `user.frm`).\\n- Next, the query will be run and return the results to the client. MySQL also stores the results of the query in the query cache.\\n\\nFinally, re-caching the results of frequently used queries can improve performance. Previously, MySQL had a query cache in its architecture, as a bottleneck in highly concurrent read and write environments, this query cache in new versions has been deprecated, instead, we often use other data cache methods such as Redis, ...\\n\\nThe above is an overview of the MySQL architecture and the process of running a query. See you in the next posts."},{"id":"spark-kafka-docker/","metadata":{"permalink":"/blog/spark-kafka-docker/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-09-11-spark-kafka-docker/index.md","source":"@site/blog/2022-09-11-spark-kafka-docker/index.md","title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","description":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","date":"2022-09-11T00:00:00.000Z","formattedDate":"September 11, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Apache","permalink":"/blog/tags/apache"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":8.675,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"spark-kafka-docker/","title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","description":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","authors":"tranlam","tags":["Bigdata","Spark","Apache","Kafka","Docker"],"image":"./images/architecture.PNG"},"prevItem":{"title":"MySQL series - MySQL Architecture Overview","permalink":"/blog/mysql-series-mysql-architecture/"},"nextItem":{"title":"Create A Standalone Spark Cluster With Docker","permalink":"/blog/spark-cluster-docker/"}},"content":"![Architecture](./images/architecture.PNG)\\n\\nHi guys, I\'m back after a long time without writing anything. Today, I want to share about how to create a Spark Streaming pipeline that consumes data from Kafka, everything is built on Docker.\\n\\n\x3c!--truncate--\x3e\\n\\n## Design overview\\n\\nThe model is containerized by Docker. Includes the following components\\n\\n- Producer: is a Kafka Producer that produces fake data about an user information using Java Faker and produce messages onto Kafka.\\n- Kafka cluster: includes brokers to store data and Zookeeper to manage those brokers.\\n- Spark cluster: is a Spark cluster consisting of 3 nodes: 1 driver and 2 workers to consume data from Kafka.\\n- Schema Registry: provides a restful interface to store and retrieve schemas, helping Kafka producers and consumers work together according to standards. Since the two ends of producing and consuming messages from two Kafka ends are independent, the consumer does not need to know how the producer sends the message with the format, the Schema Registry acts as an intermediary for the two parties to register the message format with each other, avoiding system errors.\\n- Postgres: is the database to provide configurations for the Spark Streaming application and in this article is also the place to store the streaming data after processing by Spark.\\n\\n## Build necessary Docker images and containers\\n\\n### Create a Spark cluster\\n\\nAs in the **[previous article](/blog/2022-01-01-spark-cluster-docker/index.md)** I wrote about how to build a Spark cluster on Docker, in this article I take advantage of that cluster. However, there is a slight change, leaving out some things to fit this article. You can find the build image script **[at my repository](https://github.com/LTranData/spark_kafka_docker/tree/main/spark_cluster)**. So we have the necessary images for the Spark cluster. Here is the container configuration in docker-compose.yml\\n\\n```yml\\nspark-master:\\n  image: spark-master\\n  container_name: spark-master\\n  ports:\\n    - 8080:8080\\n    - 7077:7077\\n    - 4040:4040\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\nspark-worker-1:\\n  image: spark-worker\\n  container_name: spark-worker-1\\n  environment:\\n    - SPARK_WORKER_CORES=1\\n    - SPARK_WORKER_MEMORY=1024m\\n  ports:\\n    - 18081:8081\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\n  depends_on:\\n    - spark-master\\nspark-worker-2:\\n  image: spark-worker\\n  container_name: spark-worker-2\\n  environment:\\n    - SPARK_WORKER_CORES=1\\n    - SPARK_WORKER_MEMORY=1024m\\n  ports:\\n    - 28081:8081\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\n  depends_on:\\n    - spark-master\\n```\\n\\n### Add Zookeeper, Kafka, Postgres, Schema Registry containers\\n\\nNext will be on Zookeeper, Kafka, Postgres and Schema Registry containers\\n\\n```yml\\nzookeeper:\\n  image: confluentinc/cp-zookeeper:3.3.1\\n  container_name: zookeeper\\n  ports:\\n    - \\"2181:2181\\"\\n  environment:\\n    ZOOKEEPER_CLIENT_PORT: 2181\\n    ZOOKEEPER_TICK_TIME: 2000\\nkafka:\\n  image: confluentinc/cp-kafka:3.3.1\\n  container_name: kafka\\n  depends_on:\\n    - zookeeper\\n  ports:\\n    - \\"29092:29092\\"\\n  environment:\\n    KAFKA_BROKER_ID: 1\\n    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\\n    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\\n    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\\n    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\\n    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\\n\\ndb:\\n  image: postgres\\n  container_name: db-postgres\\n  volumes:\\n    - ./data/db:/var/lib/postgresql/data\\n  ports:\\n    - \\"5432:5432\\"\\n  environment:\\n    - POSTGRES_NAME=postgres\\n    - POSTGRES_USER=postgres\\n    - POSTGRES_PASSWORD=postgres\\n\\nschema-registry:\\n  image: confluentinc/cp-schema-registry:3.3.1\\n  container_name: schema-registry\\n  depends_on:\\n    - zookeeper\\n    - kafka\\n  ports:\\n    - \\"8081:8081\\"\\n  environment:\\n    SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181\\n    SCHEMA_REGISTRY_HOST_NAME: schema-registry\\n```\\n\\nTo sum up, we have a complete **[docker-compose.yml](https://github.com/LTranData/spark_kafka_docker/blob/main/spark_ex/docker-compose.yml)** file. Then we start the containers with\\n\\n```bash\\ndocker-compose up -d\\n```\\n\\nNote, this starts all containers at once, some Kafka and Schema Registry instances will fail because it depends on Zookeeper. Wait for the Zookeeper container to finish up and then restart the Kafka container and the Schema Registry (you can also check the Zookeeper service by implementing some healthcheck techniques).\\n\\n## Create a Kafka Producer that produce fake data using Java Faker\\n\\nNext, we create a Kafka Producer to fire dummy data in Java. First, we need to create a schema on the Schema Registry. Because the Schema Registry provides a restful interface, we can easily interact with it by calling GET, POST,... The schema we use in this article will have the following form.\\n\\n```json\\n{\\n  \\"namespace\\": \\"com.cloudurable.phonebook\\",\\n  \\"type\\": \\"record\\",\\n  \\"name\\": \\"Employee\\",\\n  \\"doc\\": \\"Represents an Employee at a company\\",\\n  \\"fields\\": [\\n    { \\"name\\": \\"id\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person id\\" },\\n    { \\"name\\": \\"firstName\\", \\"type\\": \\"string\\", \\"doc\\": \\"The persons given name\\" },\\n    { \\"name\\": \\"nickName\\", \\"type\\": [\\"null\\", \\"string\\"], \\"default\\": null },\\n    { \\"name\\": \\"lastName\\", \\"type\\": \\"string\\" },\\n    { \\"name\\": \\"age\\", \\"type\\": \\"int\\", \\"default\\": -1 },\\n    { \\"name\\": \\"emails\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person email\\" },\\n    {\\n      \\"name\\": \\"phoneNumber\\",\\n      \\"type\\": {\\n        \\"type\\": \\"record\\",\\n        \\"name\\": \\"PhoneNumber\\",\\n        \\"fields\\": [\\n          { \\"name\\": \\"areaCode\\", \\"type\\": \\"string\\" },\\n          { \\"name\\": \\"countryCode\\", \\"type\\": \\"string\\", \\"default\\": \\"\\" },\\n          { \\"name\\": \\"prefix\\", \\"type\\": \\"string\\" },\\n          { \\"name\\": \\"number\\", \\"type\\": \\"string\\" }\\n        ]\\n      }\\n    },\\n    { \\"name\\": \\"status\\", \\"type\\": \\"string\\" }\\n  ]\\n}\\n```\\n\\nFirst, to POST this schema to the Schema Registry, we must convert this schema to escaped json, visit **[this json escape website](https://www.freeformatter.com/json-escape.html)** and get the string version. Then use the POST method to push the schema as follows\\n\\n```bash\\ncurl -X POST -H \\"Content-Type: application/vnd.schemaregistry.v1+json\\" \\\\\\n  --data \'{\\"schema\\": \\"{\\\\\\"namespace\\\\\\": \\\\\\"com.cloudurable.phonebook\\\\\\",\\\\\\"type\\\\\\": \\\\\\"record\\\\\\",\\\\\\"name\\\\\\": \\\\\\"Employee\\\\\\",\\\\\\"doc\\\\\\" : \\\\\\"Represents an Employee at a company\\\\\\",\\\\\\"fields\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"id\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The person id\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"firstName\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The persons given name\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"nickName\\\\\\", \\\\\\"type\\\\\\": [\\\\\\"null\\\\\\", \\\\\\"string\\\\\\"], \\\\\\"default\\\\\\" : null},{\\\\\\"name\\\\\\": \\\\\\"lastName\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"age\\\\\\",  \\\\\\"type\\\\\\": \\\\\\"int\\\\\\", \\\\\\"default\\\\\\": -1},{\\\\\\"name\\\\\\": \\\\\\"emails\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The person email\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"phoneNumber\\\\\\",  \\\\\\"type\\\\\\":{ \\\\\\"type\\\\\\": \\\\\\"record\\\\\\",   \\\\\\"name\\\\\\": \\\\\\"PhoneNumber\\\\\\",\\\\\\"fields\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"areaCode\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"countryCode\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"default\\\\\\" : \\\\\\"\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"prefix\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"number\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"}]}},{\\\\\\"name\\\\\\": \\\\\\"status\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"}]}\\"}\' \\\\\\n  http://localhost:8081/subjects/personinformation-value/versions\\n```\\n\\nAfter that, GET back to check if the schema is up or not\\n\\n```bash\\ncurl -X GET http://localhost:8081/subjects/personinformation-value/versions/ // check all versions\\ncurl -X GET http://localhost:8081/subjects/personinformation-value/versions/1 // check schema version 1\\n```\\n\\nNow that Kafka is up, the schema is on the Schema Registry, the rest is to push the message to that topic. Write a **[kafka producer](https://github.com/LTranData/spark_kafka_docker/tree/main/KafkaClient)** class as follows, and run, then the data will be uploaded to Kafka with the above chema.\\n\\n```java\\npackage kafkaclient;\\n\\nimport com.github.javafaker.Faker;\\nimport io.confluent.kafka.serializers.KafkaAvroSerializerConfig;\\nimport org.apache.avro.Schema;\\nimport org.apache.avro.generic.GenericData;\\nimport org.apache.avro.generic.GenericRecord;\\nimport org.apache.kafka.clients.producer.KafkaProducer;\\nimport org.apache.kafka.clients.producer.Producer;\\nimport io.confluent.kafka.serializers.KafkaAvroSerializer;\\nimport org.apache.kafka.clients.producer.ProducerConfig;\\nimport org.apache.kafka.clients.producer.ProducerRecord;\\nimport org.apache.kafka.common.serialization.StringSerializer;\\n\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.Properties;\\n\\npublic class KafkaProducerExample {\\n    private final static String TOPIC = \\"personinformation\\";\\n    private final static String BOOTSTRAP_SERVERS = \\"localhost:29092\\";\\n    private final static String SCHEMA_REGISTRY_URL = \\"http://localhost:8081\\";\\n    private final static String LOCAL_SCHEMA_PATH = \\"src/main/resources/person.avsc\\";\\n    private final static Schema schema;\\n\\n    private final static int nPersons = 1000;\\n\\n    static {\\n        try {\\n            schema = new Schema.Parser().parse(new File(LOCAL_SCHEMA_PATH));\\n        } catch (IOException e) {\\n            throw new RuntimeException(e);\\n        }\\n    }\\n\\n    private static Producer<String, GenericRecord> createProducer(){\\n        Properties props = new Properties();\\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);\\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());\\n        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\\n\\n        return new KafkaProducer<>(props);\\n    }\\n\\n    static void runProducer() {\\n        final Producer<String, GenericRecord> producer = createProducer();\\n        Faker faker = new Faker();\\n\\n        for (int i = 0; i < nPersons; i ++){\\n            String id = faker.idNumber().valid();\\n            String firstName = faker.name().firstName();\\n            String nickName = faker.name().username();\\n            String lastName = faker.name().lastName();\\n            int age = faker.number().numberBetween(18, 90);\\n            String emails = faker.internet().safeEmailAddress();\\n            String areaCode = String.valueOf(faker.number().numberBetween(200, 500));\\n            String countryCode = String.valueOf(faker.number().numberBetween(80, 85));\\n            String prefix = String.valueOf(faker.number().numberBetween(400, 600));\\n            String number = String.valueOf(faker.number().numberBetween(1234, 6789));\\n\\n            GenericRecord phoneNumber = new GenericData.Record(schema.getField(\\"phoneNumber\\").schema());\\n            phoneNumber.put(\\"areaCode\\", areaCode);\\n            phoneNumber.put(\\"countryCode\\", countryCode);\\n            phoneNumber.put(\\"prefix\\", prefix);\\n            phoneNumber.put(\\"number\\", number);\\n\\n            StatusEnum status = StatusEnum.getRandomStatus();\\n\\n            GenericRecord personInfo = new GenericData.Record(schema);\\n            personInfo.put(\\"id\\", id);\\n            personInfo.put(\\"firstName\\", firstName);\\n            personInfo.put(\\"nickName\\", nickName);\\n            personInfo.put(\\"lastName\\", lastName);\\n            personInfo.put(\\"age\\", age);\\n            personInfo.put(\\"emails\\", emails);\\n            personInfo.put(\\"phoneNumber\\", phoneNumber);\\n            personInfo.put(\\"status\\", status.toString());\\n\\n            ProducerRecord<String, GenericRecord> data = new ProducerRecord<String, GenericRecord>(TOPIC, String.format(\\"%s %s %s\\", firstName, lastName, nickName), personInfo);\\n            producer.send(data);\\n            System.out.println(\\"Send successfully!!!\\");\\n            try {\\n                Thread.sleep(2000);\\n            }catch (Exception e){\\n                e.printStackTrace();\\n            }\\n        }\\n    }\\n\\n    public static void main(String[] args) {\\n        try {\\n            runProducer();\\n        }catch (Exception e){\\n            e.printStackTrace();\\n        }\\n    }\\n}\\n```\\n\\nAbove, every 2 seconds we will push 1 message to Kafka, pushing a total of 1000 messages.\\n\\n## Submit Spark job\\n\\n### Configure the Postgres database\\n\\nBefore we can run the job, we need to configure Postgres with the following tables\\n\\n- Configuration for Spark applications\\n\\n```sql\\nCREATE TABLE spark_launcher_config (\\n    id serial primary  key,\\n    \\"desc\\" varchar(1000) NULL,\\n    app_name varchar(255) NULL,\\n    properties text,\\n    created timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\'),\\n    modified timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\')\\n)\\n\\nINSERT INTO public.spark_launcher_config\\n    (id, \\"desc\\", app_name, properties, created, modified)\\n    VALUES(2, \'kafka_ingest\', \'ingest_avro_from_kafka\', \'{\\n    \\"appname\\": \\"ingest_avro_from_kafka\\",\\n    \\"master\\": \\"spark://spark-master:7077\\",\\n    \\"duration\\": \\"10\\",\\n    \\"groupId\\": \\"ingest_avro_from_kafka\\",\\n    \\"zookeeper.hosts\\": \\"zookeeper:2181\\",\\n    \\"checkpoint\\": \\"./spark_checkpoint/ingest_avro_from_kafka\\",\\n    \\"zookeeper.timeout\\": \\"40000\\",\\n    \\"spark.sql.shuffle.partitions\\": \\"10\\",\\n    \\"spark.sql.sources.partitionOverwriteMode\\": \\"dynamic\\",\\n    \\"spark.sql.hive.verifyPartitionPath\\": \\"true\\",\\n    \\"spark.streaming.kafka.maxRatePerPartition\\": 10000,\\n    \\"_kafka_.bootstrap.servers\\": \\"kafka:9092\\",\\n    \\"_kafka_.group.id\\": \\"ingest_avro_from_kafka\\",\\n    \\"_kafka_.auto.offset.reset\\": \\"earliest\\",\\n    \\"_kafka_.max.poll.interval.ms\\": 5000000,\\n    \\"_kafka_.max.poll.records\\": 10000,\\n    \\"_kafka_.schema.registry.url\\": \\"http://schema-registry:8081\\",\\n    \\"_kafka_.auto.commit\\": \\"false\\",\\n    \\"_kafka_.session.timeout.ms\\": \\"50000\\",\\n    \\"_kafka_.heartbeat.interval.ms\\": \\"25000\\",\\n    \\"_kafka_.request.timeout.ms\\": \\"50000\\"\\n    }\', \'2022-04-12 09:35:27.511\', \'2022-04-12 09:35:27.511\');\\n```\\n\\n- Topic consumption configuration table\\n\\n```sql\\nCREATE TABLE spark_ingest_config (\\n    id serial primary key,\\n    app_name varchar(255) not null unique,\\n    type varchar(255)  NULL,\\n    \\"order\\" int NULL,\\n    topic varchar(255) not null unique,\\n    status int not null DEFAULT 0,\\n    fields text,\\n    temp_view_first varchar(255)  NULL,\\n    sql_parser text,\\n    prd_id varchar(255)  NULL,\\n    keys varchar(255)  NULL,\\n    path_hdfs varchar(255) NOT NULL,\\n    table_dest varchar(255) NOT NULL,\\n    impala_driver varchar(255) null DEFAULT \'\',\\n    impala_url varchar(255) null DEFAULT \'\',\\n    kafka_msg_type kkmt DEFAULT \'avro_flat\',\\n    json_schema text,\\n    repartition_des int not null DEFAULT 1,\\n    msg_type mst DEFAULT \'NOT_DEFINE\',\\n    created timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\'),\\n    modified timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\')\\n)\\n\\nINSERT INTO public.spark_ingest_config\\n(id, app_name, \\"type\\", \\"order\\", topic, status, fields, temp_view_first, sql_parser, prd_id, keys, path_hdfs, table_dest, impala_driver, impala_url, kafka_msg_type, json_schema, repartition_des, msg_type, created, modified)\\nVALUES(1, \'ingest_avro_from_kafka\', \'insert\', 0, \'personinformation\', 1, \'firstName,\\nnickName,\\nlastName,\\nage,\\nemails,\\nphoneNumber,\\nstatus\', \'ingest_avro_from_kafka_personinformation\', \'select\\n\\tcast(firstName as STRING) as firstName,\\n\\tcast(nickName as STRING) as nickName,\\n\\tcast(lastName as STRING) as lastName,\\n\\tcast(age as INT) as age,\\n\\tcast(emails as STRING) as emails,\\n\\tcast(concat(phoneNumber.countryCode, \\"-\\", phoneNumber.areaCode, \\"-\\", phoneNumber.prefix, \\"-\\", phoneNumber.number) as STRING) as phoneNumber,\\n\\tcast(status as STRING) as status\\nfrom ingest_avro_from_kafka_personinformation\', \'\', \'\', \'\', \'personinformation\', \'\', \'\', \'avro_flat\'::public.\\"kkmt\\", \'\', 1, \'NOT_DEFINE\'::public.\\"mst\\", \'2022-04-06 19:59:41.745\', \'2022-04-06 19:59:41.745\');\\n```\\n\\n- Streaming data table\\n\\n```sql\\ncreate table personinformation (\\n\\tfirstName varchar(250) not null,\\n\\tnickName varchar(250) not null,\\n\\tlastName varchar(250) not null,\\n\\tage integer not null,\\n\\temails varchar(250) not null,\\n\\tphoneNumber varchar(250) not null,\\n\\tstatus varchar(10) not null\\n);\\n```\\n\\n### Spark application configuration\\n\\nYou can find the full source code at **[my Spark Streaming example repository](https://github.com/LTranData/spark_kafka_docker/tree/main/spark_ex)**. Compile the project by running\\n\\n```bash\\nsh run.sh\\n```\\n\\nWhen all containers are running stable, Kafka has the data, we access the shell of the Spark master container\\n\\n```bash\\ndocker exec -it spark-master bash\\n```\\n\\nAfter entering the shell, you continue to run the command below to submit the Spark job\\n\\n```bash\\n$SPARK_HOME/bin/spark-submit --jars $(echo /execution_files/dependency/*.jar | tr \' \' \',\') --class com.tranlam.App /execution_files/spark_ex-1.0-SNAPSHOT.jar --app-name ingest_avro_from_kafka --jdbc-url \\"jdbc:postgresql://db:5432/postgres?user=postgres&password=postgres\\"\\n```\\n\\nSo there is already a Spark job that consumes Kafka data. Visit **[http://localhost:4040/streaming](http://localhost:4040/streaming)** to see the batches running\\n\\n![Architecture](./images/spark-ui.PNG)\\n\\nIn Postgres, query the table `personinformation` we get the data as desired\\n\\n![Postgres](./images/postgres.PNG)\\n\\nAbove is the steps for building a basic Spark streaming pipeline to stream data from Kafka. Another thing to note is that instead of committing the offset of the consumptions to a Kafka topic like in above code, you can manually commit it to a path in Zookeeper for more proactive control.\\n\\nThe code of the whole article you read can be found at: **[https://github.com/LTranData/spark_kafka_docker](https://github.com/LTranData/spark_kafka_docker)**"},{"id":"spark-cluster-docker/","metadata":{"permalink":"/blog/spark-cluster-docker/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2022-01-01-spark-cluster-docker/index.md","source":"@site/blog/2022-01-01-spark-cluster-docker/index.md","title":"Create A Standalone Spark Cluster With Docker","description":"Create A Standalone Spark Cluster With Docker","date":"2022-01-01T00:00:00.000Z","formattedDate":"January 1, 2022","tags":[{"label":"Bigdata","permalink":"/blog/tags/bigdata"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Apache","permalink":"/blog/tags/apache"},{"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":6.415,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"spark-cluster-docker/","title":"Create A Standalone Spark Cluster With Docker","description":"Create A Standalone Spark Cluster With Docker","authors":"tranlam","tags":["Bigdata","Spark","Apache","Docker"],"image":"./images/cluster-overview.PNG"},"prevItem":{"title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","permalink":"/blog/spark-kafka-docker/"},"nextItem":{"title":"M\u1ed9t s\u1ed1 c\xe2u h\u1ecfi ph\u1ecfng v\u1ea5n AI/ML","permalink":"/blog/ai-interview-questions/"}},"content":"![Cluster Overview](./images/cluster-overview.PNG)\\n\\nLately, I\'ve spent a lot of time teaching myself how to build Hadoop clusters, Spark, Hive integration, and more. This article will write about how you can build a Spark cluster for data processing using Docker, including 1 master node and 2 worker nodes, the cluster type is standalone cluster (maybe the upcoming articles I will do about Hadoop cluster and integrated resource manager is Yarn). Let\'s go to the article.\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview of a Spark cluster\\n\\nApache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute tasks across multiple computers. It was design for fast computing and use RAM for caching and processing data.\\n\\nIt provides flexibility and scalability, created to improve the performance of MapReduce but at a much higher speed: 100 times faster than Hadoop when data is stored in memory and 10 times faster when accessed CD driver.\\n\\nSpark does not have a file system of its own, but it can interact with many types of storage systems and can be used to integrate with Hadoop. Below is an overview of the structure of a Spark application.\\n\\n![Cluster Overview](./images/cluster-overview.PNG)\\n\\nEach time we submit a Spark application, it will create a driver program at the master node, which then create a SparkContext object. To be able to run in a cluster, SparkContext need to connect to a cluster resource manager, that could be Spark\u2019s standalone cluster manager, Mesos or Yarn. Once the SparkContext get the connection, it will have specific RAM and CPU resources of the worker nodes in the cluster.\\n\\nEach worker node will receive the code and tasks from the driver, compute and process the data.\\n\\nThe master node will be responsible for scheduling all the tasks and send those to the worker nodes so it\u2019s ideal when we put it in the same network area with all the worker nodes to achieve low latency between requests\\n\\nThere are 2 Spark running modes\\n\\n- **Running locally:** running all the tasks in the same machine which is your local machine, utilize the number of cores in that machine to perform parallelism\\n- **Running in a cluster:** Spark distribute the tasks to all the machine in the cluster. There are 2 deploy modes which are client mode and cluster mode, with 4 options of cluster resource manager, which are Spark standalone cluster manager, Apache Mesos, Hadoop Yarn, or Kubernetes.\\n\\n## Create a base image for the cluster\\n\\nBecause the images of the nodes in a cluster need to install the same software, we will build a base image for the whole cluster first, then the following images will import from this image and add the following images. other necessary dependencies.\\n\\n```bash\\nARG debian_buster_image_tag=8-jre-slim\\nFROM openjdk:${debian_buster_image_tag}\\n\\nARG shared_workspace=/opt/workspace\\n\\nRUN mkdir -p ${shared_workspace} && \\\\\\n    apt-get update -y && \\\\\\n    apt-get install -y python3 && \\\\\\n    ln -s /usr/bin/python3 /usr/bin/python && \\\\\\n    rm -rf /var/lib/apt/lists/*\\n\\nENV SHARED_WORKSPACE=${shared_workspace}\\n\\nVOLUME ${shared_workspace}\\n```\\n\\nHere, since Spark requires java version 8 or 11, we will create an image running jdk 8, we will take the variable `shared_workspace` as the Jupyterlab working environment path (later). In addition, we will install `python3` for running Jupyterlab.\\n\\n## Create a spark base image\\n\\nWe come to create a spark base image with common packages for master node and worker node.\\n\\n```bash\\nFROM spark-cluster-base\\n\\nARG spark_version=3.2.0\\nARG hadoop_version=3.2\\n\\nRUN apt-get update -y && \\\\\\n    apt-get install -y curl && \\\\\\n    curl https://archive.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz -o spark.tgz && \\\\\\n    tar -xf spark.tgz && \\\\\\n    mv spark-${spark_version}-bin-hadoop${hadoop_version} /usr/bin/ && \\\\\\n    mkdir /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}/logs && \\\\\\n    rm spark.tgz\\n\\nENV SPARK_HOME /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}\\nENV SPARK_MASTER_HOST spark-master\\nENV SPARK_MASTER_PORT 7077\\nENV PYSPARK_PYTHON python3\\n\\nWORKDIR ${SPARK_HOME}\\n```\\n\\nFirst, we will import the image from the base image above (that is `spark-cluster-base`, this name will be assigned at build time), listing the compatible Spark and Hadoop versions. You can check version compatibility on Spark\'s homepage.\\n\\n![Spark Version](./images/spark-version.PNG)\\n\\nThen it will be to download and extract Spark, along with creating the necessary environment variables to support running the command line later. Here, `SPARK_MASTER_HOST` and `SPARK_MASTER_PORT` used by worker nodes to register with the corresponding master node address.\\n\\n## Create a master node image\\n\\nHaving a spark base image, we start creating the master node by importing that base image and adding the appropriate variables to the master node as the port of the web ui interface so we can interact with spark on the interface later.\\n\\n```bash\\nFROM spark-base\\n\\nARG spark_master_web_ui=8080\\n\\nEXPOSE ${spark_master_web_ui} ${SPARK_MASTER_PORT}\\nCMD bin/spark-class org.apache.spark.deploy.master.Master >> logs/spark-master.out\\n```\\n\\nThe above command is to run master node.\\n\\n## Create a worker node image\\n\\nNext is to create worker node\\n\\n```bash\\nFROM spark-base\\n\\nARG spark_worker_web_ui=8081\\n\\nEXPOSE ${spark_worker_web_ui}\\nCMD bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} >> logs/spark-worker.out\\n```\\n\\nThe above command is to run the worker node and point to the address of the master node to register.\\n\\n## Create a Jupyterlab image for testing\\n\\nFinally, to test the spark cluster working, we will install Jupyterlab and use pyspark to run the code.\\n\\n```bash\\nFROM spark-cluster-base\\n\\nARG spark_version=3.2.0\\nARG jupyterlab_version=3.2.5\\n\\nRUN apt-get update -y && \\\\\\n    apt-get install -y python3-pip && \\\\\\n    pip3 install wget pyspark==${spark_version} jupyterlab==${jupyterlab_version}\\n\\nEXPOSE 8888\\nWORKDIR ${SHARED_WORKSPACE}\\nCMD jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=\\n```\\n\\nAlong with that is to list the command to run Jupyter on port 8888.\\n\\n## Combine images and create containers\\n\\nAfter creating all the Dockerfiles, we proceed to build the appropriate images.\\n\\n![Folder Structure](./images/folder-structure.PNG)\\n\\n**Listing versions**\\n\\n```bash\\nSPARK_VERSION=\\"3.2.0\\"\\nHADOOP_VERSION=\\"3.2\\"\\nJUPYTERLAB_VERSION=\\"3.2.5\\"\\n```\\n\\n**Build base image**\\n\\n```bash\\ndocker build \\\\\\n  --platform=linux/arm64 \\\\\\n  -f cluster_base/Dockerfile \\\\\\n  -t spark-cluster-base .\\n```\\n\\n**Build spark base image**\\n\\n```bash\\ndocker build \\\\\\n  --build-arg spark_version=\\"${SPARK_VERSION}\\" \\\\\\n  --build-arg hadoop_version=\\"${HADOOP_VERSION}\\" \\\\\\n  -f spark_base/Dockerfile \\\\\\n  -t spark-base .\\n```\\n\\n**Build master node image**\\n\\n```bash\\ndocker build \\\\\\n  -f master_node/Dockerfile \\\\\\n  -t spark-master .\\n```\\n\\n**Build worker node image**\\n\\n```bash\\ndocker build \\\\\\n  -f worker_node/Dockerfile \\\\\\n  -t spark-worker .\\n```\\n\\n**Build Jupyterlab image**\\n\\n```bash\\ndocker build \\\\\\n  --build-arg spark_version=\\"${SPARK_VERSION}\\" \\\\\\n  --build-arg jupyterlab_version=\\"${JUPYTERLAB_VERSION}\\" \\\\\\n  -f jupyter_lab/Dockerfile \\\\\\n  -t spark-jupyterlab .\\n```\\n\\nFinally, to create the necessary containers, we create a file `docker-compose.yml` with the following content\\n\\n```bash\\nversion: \\"3.6\\"\\nvolumes:\\n  shared-workspace:\\n    name: \\"hadoop-distributed-file-system\\"\\n    driver: local\\nservices:\\n  jupyterlab:\\n    image: spark-jupyterlab\\n    container_name: jupyterlab\\n    ports:\\n      - 8888:8888\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n  spark-master:\\n    image: spark-master\\n    container_name: spark-master\\n    ports:\\n      - 8080:8080\\n      - 7077:7077\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n  spark-worker-1:\\n    image: spark-worker\\n    container_name: spark-worker-1\\n    environment:\\n      - SPARK_WORKER_CORES=1\\n      - SPARK_WORKER_MEMORY=512m\\n    ports:\\n      - 8081:8081\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n    depends_on:\\n      - spark-master\\n  spark-worker-2:\\n    image: spark-worker\\n    container_name: spark-worker-2\\n    environment:\\n      - SPARK_WORKER_CORES=1\\n      - SPARK_WORKER_MEMORY=512m\\n    ports:\\n      - 8082:8081\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n    depends_on:\\n      - spark-master\\n```\\n\\nInclude the volume in which we will save data so that when deleting containers, data will not be lost, along with the necessary containers (services). To each container the appropriate environment variables are added, the ports to map to the host machine, and the order in which the containers are run. Here, the master node has to run first to get the hostname, so the worker node will depend on the master node container. After that, we run `docker-compose up`, so we have launched all the necessary containers.\\n\\n## Running Jupyterlab to check the cluster\\n\\nAfter running `docker-compose up` and seeing in the terminal the logs showing that the master node and worker node have been successfully started, along with the successful register status of the nodes, we go to `localhost:8080` to access spark ui.\\n\\n![Spark UI](./images/spark-ui.PNG)\\n\\nIn the interface, we can see that there are 2 workers working as red circled areas.\\n\\nEnter `localhost:8888` to access the Jupyterlab interface, execute the following code\\n\\n![Jupyter Lab](./images/jupyterlab.PNG)\\n\\nRun the code, then go back to spark ui, we can see our application is running\\n\\n![Application](./images/application.PNG)\\n\\nClick on the application, we see our workers processing the job\\n\\n![Application Workers](./images/application-workers.PNG)"},{"id":"ai-interview-questions/","metadata":{"permalink":"/blog/ai-interview-questions/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-07-24-ai-interview-questions/index.md","source":"@site/blog/2021-07-24-ai-interview-questions/index.md","title":"M\u1ed9t s\u1ed1 c\xe2u h\u1ecfi ph\u1ecfng v\u1ea5n AI/ML","description":"Intro","date":"2021-07-24T00:00:00.000Z","formattedDate":"July 24, 2021","tags":[{"label":"AI","permalink":"/blog/tags/ai"},{"label":"ML","permalink":"/blog/tags/ml"},{"label":"Machine Learning","permalink":"/blog/tags/machine-learning"},{"label":"Artificial Intelligence","permalink":"/blog/tags/artificial-intelligence"}],"readingTime":8.04,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"ai-interview-questions/","title":"M\u1ed9t s\u1ed1 c\xe2u h\u1ecfi ph\u1ecfng v\u1ea5n AI/ML","authors":"tranlam","tags":["AI","ML","Machine Learning","Artificial Intelligence"],"image":"./images/intro.JPEG"},"prevItem":{"title":"Create A Standalone Spark Cluster With Docker","permalink":"/blog/spark-cluster-docker/"},"nextItem":{"title":"Receptive field trong th\u1ecb gi\xe1c m\xe1y t\xednh","permalink":"/blog/receptive-field/"}},"content":"![Intro](./images/intro.JPEG)\\n\\nG\u1ea7n \u0111\xe2y, AI/ML l\xe0 m\u1ed9t c\xe1i trend, ng\u01b0\u1eddi ng\u01b0\u1eddi AI, nh\xe0 nh\xe0 AI. Sinh vi\xean \u0111\u1ed5 x\xf4 \u0111i h\u1ecdc AI h\u1ebft, c\xe1c tr\u01b0\u1eddng \u0111\u1ea1i h\u1ecdc c\u0169ng d\u1ea7n m\u1edf c\xe1c m\xf4n v\u1ec1 h\u1ecdc m\xe1y, tr\xed tu\u1ec7 nh\xe2n t\u1ea1o, r\u1ed3i th\u1ecb gi\xe1c m\xe1y t\xednh \u0111\u1ec3 \\"b\u1eaft k\u1ecbp\\". \x3c!--truncate--\x3eD\u01b0\u1edbi \u0111\xe2y, m\xecnh s\u1ebd tr\xecnh b\xe0y v\u1ec1 c\xe1c c\xe2u h\u1ecfi ph\u1ecfng v\u1ea5n c\u01a1 b\u1ea3n, d\xe0nh cho c\xe1c b\u1ea1n mu\u1ed1n t\xecm c\xe1c v\u1ecb tr\xed th\u1ef1c t\u1eadp trong l\u0129nh v\u1ef1c n\xe0y.\\n\\n### Bias, variance\\n**C\xe1c c\xe2u h\u1ecfi m\u1ee5c n\xe0y s\u1ebd xung quanh c\xe1c th\xf4ng s\u1ed1 tr\xean l\xe0 g\xec, n\xf3 cao n\xf3 th\u1ea5p th\xec \u1ea3nh h\u01b0\u1edfng nh\u01b0 n\xe0o, x\u1eed l\xfd th\u1ebf n\xe0o?**\\n- Bias l\xe0 sai s\u1ed1 gi\u1eefa k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\xe1n c\u1ee7a model v\xe0 c\xe1c nh\xe3n th\u1ef1c s\u1ef1 c\u1ee7a ch\xfang ta. Bias th\u1ec3 hi\u1ec7n n\u0103ng l\u1ef1c c\u1ee7a model trong vi\u1ec7c d\u1ef1 \u0111o\xe1n. Bias cao n\xf3i l\xean r\u1eb1ng model c\u1ee7a ch\xfang ta kh\xf4ng quan t\xe2m \u0111\u1ebfn d\u1eef li\u1ec7u, model qu\xe1 \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 m\xe0 c\xf3 th\u1ec3 h\u1ecdc \u0111\u01b0\u1ee3c c\xe1c \u0111\u1eb7c tr\u01b0ng t\u1eeb d\u1eef li\u1ec7u. Bias cao th\u01b0\u1eddng cho k\u1ebft qu\u1ea3 l\u1ed7i cao tr\xean c\u1ea3 t\u1eadp hu\u1ea5n luy\u1ec7n v\xe0 t\u1eadp ki\u1ec3m th\u1eed. Hi\u1ec7n t\u01b0\u1ee3ng n\xe0y g\u1ecdi l\xe0 underfitting.\\n- Variance l\xe0 \u0111\u1ed9 ph\xe2n t\xe1n c\u1ee7a k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\xe1n c\u1ee7a model ch\xfang ta. Variance cao n\xf3i l\xean r\u1eb1ng model c\u1ee7a ch\xfang ta qu\xe1 t\u1eadp trung v\xe0o t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n, c\xf3 th\u1ec3 ph\xe2n lo\u1ea1i c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n t\u1ed1t m\xe0 kh\xf4ng t\u1ed5ng qu\xe1t t\u1ed1t, th\xedch \u1ee9ng t\u1ed1t v\u1edbi c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u m\u1edbi, model c\u1ee7a ch\xfang ta qu\xe1 ph\u1ee9c t\u1ea1p nh\u01b0ng l\u01b0\u1ee3ng d\u1eef li\u1ec7u c\u1ee7a ch\xfang ta l\u1ea1i kh\xf4ng \u0111\u1ee7 l\u1edbn. Hi\u1ec7n t\u01b0\u1ee3ng n\xe0y g\u1ecdi l\xe0 overfitting.\\n\\nD\u01b0\u1edbi \u0111\xe2y l\xe0 h\xecnh \u1ea3nh minh h\u1ecda cho bias v\xe0 variance\\n![Bias Variance Tradeoff](./images/bias_variance_tradeoff.PNG)\\n\\nG\u1ecdi ground truth (c\xf3 ngh\u0129a l\xe0 gi\xe1 tr\u1ecb nh\xe3n th\u1ef1c s\u1ef1 c\u1ee7a d\u1eef li\u1ec7u \u0111\u1ea7u v\xe0o) l\xe0 \u0111i\u1ec3m m\xe0 n\u1eb1m \u1edf t\xe2m c\xe1c h\xecnh tr\xf2n \u1edf tr\xean, c\xf2n c\xe1c prediction (l\xe0 c\xe1c gi\xe1 tr\u1ecb nh\xe3n d\u1ef1 \u0111o\xe1n c\u1ee7a ta), l\xe0 c\xe1c ch\u1ea5m tr\xf2n m\xe0u xanh n\u01b0\u1edbc bi\u1ec3n. V\u1edbi tr\u01b0\u1eddng h\u1ee3p low bias v\xe0 low variance, c\xe1c \u0111i\u1ec3m nh\xe3n d\u1ef1 \u0111o\xe1n s\u1ebd r\u1ea5t s\xe1t nhau v\xe0 s\xe1t v\u1edbi t\xe2m c\u1ee7a bia tr\xf2n, th\u1ec3 hi\u1ec7n r\u1eb1ng \u0111\u1ed9 ph\xe2n t\xe1n c\xe1c k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\xe1n th\u1ea5p v\xe0 kho\u1ea3ng c\xe1ch gi\u1eefa nh\xe3n d\u1ef1 \u0111o\xe1n v\xe0 nh\xe3n th\u1ef1c t\u1ebf th\u1ea5p, t\u01b0\u1ee3ng t\u1ef1 c\xe1c tr\u01b0\u1eddng h\u1ee3p kh\xe1c.\\n\\nC\xe1c ph\u01b0\u01a1ng ph\xe1p gi\u1ea3m thi\u1ec3u overfitting\\n- Ki\u1ebfm nhi\u1ec1u d\u1eef li\u1ec7u h\u01a1n cho m\xf4 h\xecnh.\\n- C\xe1c k\u1ef9 thu\u1eadt regularization, drop out, early stopping, batch normalization (c\xe1i n\xe0y gi\u1ea3m thi\u1ec3u m\u1ed9t \xedt th\xf4i).\\n- Ch\u1ec9nh s\u1eeda c\xe1c si\xeau tham s\u1ed1 c\u1ee7a m\xf4 h\xecnh \u0111\u1ec3 ki\u1ec3m so\xe1t qu\xe1 tr\xecnh hu\u1ea5n luy\u1ec7n.\\n\\nC\xe1c ph\u01b0\u01a1ng ph\xe1p gi\u1ea3m thi\u1ec3u underfitting\\n- Hu\u1ea5n luy\u1ec7n nhi\u1ec1u epoch h\u01a1n.\\n- X\xe2y m\u1ea1ng to h\u01a1n, t\u0103ng s\u1ed1 \u0111\u1eb7c tr\u01b0ng \u0111\u1ea7u v\xe0o l\xean.\\n- Lo\u1ea1i b\u1ecf c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u g\xe2y nhi\u1ec5u.\\n\\n### Batch normalization\\nL\xe0 m\u1ed9t k\u1ef9 thu\u1eadt gi\xfap m\xf4 h\xecnh \u1ed5n \u0111\u1ecbnh h\u01a1n trong qu\xe1 tr\xecnh hu\u1ea5n luy\u1ec7n (\u1edf \u0111\xe2y m\xecnh s\u1ebd kh\xf4ng tr\xecnh b\xe0y v\u1ec1 to\xe1n c\u1ee7a batch normalization). M\u1ed9t s\u1ed1 t\xednh ch\u1ea5t c\u1ee7a batch normalization nh\u01b0 sau\\n- Batchnorm khi\u1ebfn qu\xe1 tr\xecnh hu\u1ea5n luy\u1ec7n \u1ed5n \u0111\u1ecbnh h\u01a1n, m\u1ea1ng h\u1ed9i t\u1ee5 nhanh h\u01a1n.\\n- Batchnorm cho ph\xe9p ta s\u1eed d\u1ee5ng learning rate l\u1edbn h\u01a1n.\\n- Batchnorm khi\u1ebfn vi\u1ec7c kh\u1edfi t\u1ea1o c\xe1c ma tr\u1eadn tr\u1ecdng s\u1ed1 d\u1ec5 d\xe0ng h\u01a1n v\xec n\xf3 gi\xfap gi\u1ea3m thi\u1ec3u \u0111\u1ed9 nh\u1ea1y c\u1ee7a m\xf4 h\xecnh v\u1edbi c\xe1c v\u1ea5n \u0111\u1ec1 gradient vanishing/exploding. T\u1eeb \u0111\xf3, vi\u1ec7c t\u1ea1o d\u1ef1ng m\u1ea1ng c\u1ee7a ch\xfang ta d\u1ec5 th\u1edf h\u01a1n.\\n- Batchnorm cho ta nhi\u1ec1u l\u1ef1a ch\u1ecdn c\xe1c h\xe0m k\xedch ho\u1ea1t, b\u1edfi v\xec m\u1ed9t s\u1ed1 h\xe0m k\xedch ho\u1ea1t c\xf3 \u0111\u1ea1o h\xe0m th\u01b0\u1eddng co v\u1ec1 ${0}$ nhanh trong qu\xe1 tr\xecnh backpropagation nh\u01b0 sigmoid,\u2026 Batchnorm \u0111i\u1ec1u ti\u1ebft d\u1eef li\u1ec7u c\u1ee7a ch\xfang ta v\u1ec1 1 ph\xe2n b\u1ed1 nh\u1ea5t \u0111\u1ecbnh \u0111\u1ec3 trong kho\u1ea3ng gi\xe1 tr\u1ecb \u0111\xf3, \u0111\u1ea1o h\xe0m c\xe1c h\xe0m k\xedch ho\u1ea1t c\xf3 \u0111\u1ed9 l\u1edbn t\u01b0\u01a1ng \u0111\u1ed1i.\\n- Batchnorm cho m\u1ed9t s\u1ed1 hi\u1ec7u \u1ee9ng ph\u1ee5 c\u1ee7a regularization, b\u1edfi v\xec Batchnorm khi\u1ebfn c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u c\xf3 k\u1ef3 v\u1ecdng l\xe0 ${0}$, \u0111\u1ed9 l\u1ec7ch chu\u1ea9n nh\u1ecf, \u1edf \u0111\xf3, c\xe1c h\xe0m k\xedch ho\u1ea1t th\u01b0\u1eddng c\xf3 d\u1ea1ng g\u1ea7n v\u1edbi tuy\u1ebfn t\xednh, n\xean m\xf4 h\xecnh c\u1ee7a ta n\xf3 \u0111\u1ee1 \u0111\u1ed9 phi tuy\u1ebfn h\u01a1n.\\n\\n**T\u1ea1i sao Batchnorm gi\xfap thu\u1eadt to\xe1n h\u1ed9i t\u1ee5 nhanh h\u01a1n?**\\n\\n- Batchnorm khi\u1ebfn cho c\xe1c \u0111\u1ea7u v\xe0o tr\u01b0\u1edbc c\xe1c h\xe0m k\xedch ho\u1ea1t c\u1ee7a ch\xfang ta v\u1ec1 kho\u1ea3ng gi\xe1 tr\u1ecb nh\u1ea5t \u0111\u1ecbnh v\xe0 c\xf3 k\u1ef3 v\u1ecdng l\xe0 ${0}$. \u0110\u1ea7u v\xe0o l\xfac \u0111\xf3 kh\xf4ng qu\xe1 to hay c\u0169ng kh\xf4ng qu\xe1 nh\u1ecf, n\xean \u0111\u1ea1o h\xe0m c\u1ee7a h\xe0m k\xedch ho\u1ea1t s\u1ebd kh\xf4ng g\u1ea7n \u0111\u1ebfn ${0}$, do \u0111\xf3 h\u1ed9i t\u1ee5 nhanh h\u01a1n.\\n- Batchnorm khi\u1ebfn cho c\xe1c \u0111\u1ea7u v\xe0o n\u1eb1m trong 1 kho\u1ea3ng gi\xe1 tr\u1ecb nh\u1ecf, khi \u0111\xf3 th\xec h\xe0m m\u1ea5t m\xe1t c\u1ee7a ta c\xf3 \u0111\u1ed3 th\u1ecb tr\xf2n h\u01a1n, do v\u1eady, kho\u1ea3ng c\xe1ch t\u1eeb 1 \u0111i\u1ec3m b\u1ea5t k\u1ef3 tr\xean \u0111\u1ed3 th\u1ecb t\u1edbi m\u1ed9t \u0111i\u1ec3m c\u1ef1c ti\u1ec3u b\u1ea5t k\u1ef3 s\u1ebd ng\u1eafn h\u01a1n, model c\u1ee7a ch\xfang ta s\u1ebd t\xecm \u0111\u01b0\u1ee3c \u0111i\u1ec3m c\u1ef1c ti\u1ec3u nhanh h\u01a1n.\\n\\n**S\u1ef1 kh\xe1c nhau c\u1ee7a train v\xe0 test khi th\u1ef1c hi\u1ec7n Batchnorm?**\\n\\nKhi train, Batchnorm c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c \xe1p d\u1ee5ng tr\xean c\xe1c minibatch, 1 mini-batch c\xf3 N \u0111i\u1ec3m d\u1eef li\u1ec7u. Trong qu\xe1 tr\xecnh train, ta Batchnorm tr\xean $N$ \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\xf3. Nh\u01b0ng \u1edf test time, ta ch\u1ec9 c\xf3 1 d\u1eef li\u1ec7u 1 l\u1ea7n v\xe0 ph\u1ea3i \u0111\u01b0a d\u1ef1 \u0111o\xe1n v\u1edbi 1 \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\xf3. C\xf3 1 c\xe1ch l\xe0 ta \u01b0\u1edbc l\u01b0\u1ee3ng $\\\\mu$ v\xe0 $\\\\sigma$ b\u1eb1ng Exponentially Weighted Average \u0111\u1ec3 s\u1eed d\u1ee5ng v\xe0o test time.\\n\\nV\xed d\u1ee5: Trong qu\xe1 tr\xecnh train, t\u1ea1i layer $L$, n\xf3 cho $\\\\mu$ v\xe0 $\\\\sigma$ nh\u1ea5t \u0111\u1ecbnh, ta l\u01b0u n\xf3,\u2026 Khi ho\xe0n t\u1ea5t qu\xe1 tr\xecnh train, m\u1ed7i layer $L$ \u0111\u1ec1u s\u1ed1 c\u1eb7p $\\\\mu$ v\xe0 $\\\\sigma$ b\u1eb1ng s\u1ed1 mini-batch. V\xe0 ta s\u1eed d\u1ee5ng k\u1ef9 thu\u1eadt Exponentially Weighted Average \u0111\u1ec3 t\xednh $\\\\mu$ v\xe0 $\\\\sigma$ cho layer $L$ cho \u0111i\u1ec3m d\u1eef li\u1ec7u \u1edf test time \u0111\xf3.\\n\\n### Mini-batch nh\u1ecf hay l\u1edbn \u1ea3nh h\u01b0\u1edfng th\u1ebf n\xe0o?\\n- Mini-batch nh\u1ecf th\xec tr\xean \u0111\u1ed3 th\u1ecb h\xe0m loss, \u0111\u01b0\u1eddng \u0111i t\u1eeb 1 \u0111i\u1ec3m b\u1ea5t k\u1ef3 \u0111\u1ebfn 1 \u0111i\u1ec3m c\u1ef1c tr\u1ecb local minimum s\u1ebd r\u1ea5t g\u1ed3 gh\u1ec1 v\xe0 c\xf3 th\u1ec3 c\u1ea7n nhi\u1ec1u epoch h\u01a1n \u0111\u1ec3 c\xf3 \u0111\u01b0\u1ee3c 1 local minimum t\u1ed1t. Nh\u01b0ng vi\u1ec7c t\xednh to\xe1n s\u1ebd nhanh h\u01a1n, b\u1ed9 nh\u1edb n\u1ea1p d\u1eef li\u1ec7u t\u1ed1n \xedt h\u01a1n.\\n- Mini-batch l\u1edbn th\xec \u0111\u01b0\u1eddng \u0111i th\u1eb3ng v\u1ec1 \u0111\xedch h\u01a1n nh\u01b0ng vi\u1ec7c t\xednh to\xe1n \u0111\u1eaft \u0111\u1ecf h\u01a1n s\u1ebd l\xe0m qu\xe1 tr\xecnh hu\u1ea5n luy\u1ec7n ch\u1ea1y l\xe2u h\u01a1n.\\n\\n### Hi\u1ec7n t\u01b0\u1ee3ng Imbalanced Data\\nL\xe0 hi\u1ec7n t\u01b0\u1ee3ng m\xe0 t\u1eadp d\u1eef li\u1ec7u c\u1ee7a ta c\xf3 c\xe1c l\u1edbp ph\xe2n lo\u1ea1i c\xf3 s\u1ef1 sai kh\xe1c l\u1edbn v\u1ec1 s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u.\\nV\xed d\u1ee5: trong chu\u1ea9n \u0111o\xe1n b\u1ec7nh ung th\u01b0, t\u1eadp d\u1eef li\u1ec7u cho th\u1ea5y, c\xf3 $95\\\\%$ \u0111\u01b0\u1ee3c \u0111\xe1nh nh\xe3n l\xe0 kh\xf4ng c\xf3 d\u1ea5u hi\u1ec7u b\u1ecb ung th\u01b0, $5\\\\%$ c\xf2n l\u1ea1i l\xe0 c\xf3 d\u1ea5u hi\u1ec7u b\u1ecb ung th\u01b0. Khi \u0111\xf3 m\xf4 h\xecnh c\u1ee7a ta s\u1ebd r\u1ea5t kh\xf3 kh\u0103n \u0111\u1ec3 \u0111\u01b0a ra d\u1ef1 \u0111o\xe1n, th\u1eadm ch\xed trong tr\u01b0\u1eddng h\u1ee3p \u0111\u01b0a ra d\u1ef1 \u0111o\xe1n \xe2m t\xednh h\u1ebft th\xec c\u0169ng \u0111\xe3 c\xf3 accuracy l\xe0 $\\\\sim95\\\\%$. Khi n\xe0y accuracy kh\xf4ng c\xf2n l\xe0 1 metric t\u1ed1t \u0111\u1ec3 m\xe0 \u0111\xe1nh gi\xe1 m\xf4 h\xecnh n\u1eefa, ta s\u1ebd d\xf9ng precision, recall, hay F2-score.\\n\\n\u0110\u1ec3 gi\u1ea3i quy\u1ebft, d\xf9ng c\xe1c k\u1ef9 thu\u1eadt sau\\n- Ta c\xf3 th\u1ec3 data augmentation l\u1edbp ph\xe2n lo\u1ea1i m\xe0 c\xf3 s\u1ed1 l\u01b0\u1ee3ng \xedt d\u1eef li\u1ec7u, nh\u01b0 crop, xoay h\xecnh, l\xe0m m\xe9o,\u2026\\n- S\u1eed d\u1ee5ng metric ph\xf9 h\u1ee3p \u0111\u1ec3 \u0111\xe1nh gi\xe1.\\n- Oversampling v\xe0 Undersampling.\\n- Thay \u0111\u1ed5i h\xe0m m\u1ea5t m\xe1t c\u1ee7a ta, b\u1eb1ng vi\u1ec7c t\u0103ng tr\u1ecdng s\u1ed1 cho ph\u1ea7n h\xe0m m\u1ea5t m\xe1t t\u1ea1i c\xe1c \u0111i\u1ec3m \u1edf l\u1edbp ph\xe2n lo\u1ea1i c\xf3 l\u01b0\u1ee3ng d\u1eef li\u1ec7u th\u1ea5p, tr\u1eebng ph\u1ea1t n\u1eb7ng n\u1ebfu m\xf4 h\xecnh d\u1ef1 \u0111o\xe1n sai c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\xf3.\\n\\n### Gradient vanishing/exploding\\nTheo l\xfd thuy\u1ebft, m\u1ea1ng c\u1ee7a ta c\xe0ng s\xe2u, th\xec \u0111\u1ed9 ch\xednh x\xe1c trong qu\xe1 tr\xecnh d\u1ef1 \u0111o\xe1n c\xe0ng cao h\u01a1n, b\u1edfi v\xec khi \u0111\xf3 m\u1ea1ng c\u1ee7a ta s\u1ebd h\u1ecdc \u0111\u01b0\u1ee3c nhi\u1ec1u \u0111\u1eb7c tr\u01b0ng ph\u1ee9c t\u1ea1p. Nh\u01b0ng th\u1ef1c t\u1ebf, \u0111\u1ed9 ch\xednh x\xe1c c\u1ee7a m\u1ea1ng s\u1ebd b\xe3o h\xf2a \u0111\u1ebfn m\u1ed9t m\u1ee9c s\xe2u n\xe0o \u0111\xf3 c\u1ee7a m\u1ea1ng, th\u1eadm ch\xed c\xf2n gi\u1ea3m khi m\u1ea1ng ta qu\xe1 s\xe2u. Nguy\xean nh\xe2n l\xe0 b\u1edfi c\xe1c hi\u1ec7n t\u01b0\u1ee3ng gradient vanishing/exploding.\\n\\n**Hi\u1ec7n t\u01b0\u1ee3ng Gradient vanishing/exploding l\xe0 g\xec?**\\n\\nQu\xe1 tr\xecnh kh\u1edfi t\u1ea1o ma tr\u1eadn tr\u1ecdng s\u1ed1, c\xe1c gi\xe1 tr\u1ecb th\u01b0\u1eddng c\xf3 k\u1ef3 v\u1ecdng l\xe0 ${0}$, \u0111\u1ed9 l\u1ec7ch chu\u1ea9n l\xe0 $1$, do v\u1eady c\xe1c gi\xe1 tr\u1ecb \u0111\xf3 th\u01b0\u1eddng n\u1eb1m trong kho\u1ea3ng $[-1, 1]$. C\xe1c h\xe0m k\xedch ho\u1ea1t th\u01b0\u1eddng c\xf3 gi\xe1 tr\u1ecb \u0111\u1ea7u ra n\u1eb1m trong kho\u1ea3ng $(0, 1)$ \u0111\u1ec3 bi\u1ec3u di\u1ec5n x\xe1c su\u1ea5t c\u1ee7a qu\xe1 tr\xecnh ph\xe2n lo\u1ea1i.\\n- Gradient vanishing: hi\u1ec7n t\u01b0\u1ee3ng m\xe0 khi trong qu\xe1 tr\xecnh th\u1ef1c hi\u1ec7n thu\u1eadt to\xe1n lan truy\u1ec1n ng\u01b0\u1ee3c backpropagation, gradient c\xe0ng v\u1ec1 c\xe1c l\u1edbp n\xf4ng h\u01a1n s\u1ebd c\xe0ng co d\u1ea7n v\u1ec1 ${0}$, l\xe0m cho c\xe1c l\u1edbp \u1edf \u0111\u1ea7u \u0111\xf3 kh\xf4ng th\u1ec3 c\u1eadp nh\u1eadt, kh\xf4ng th\u1ec3 \u201ch\u1ecdc\u201d \u0111\u01b0\u1ee3c n\u1eefa. C\xe1c gi\xe1 tr\u1ecb gradient \u1edf t\u1ea7ng $L - 1$ s\u1ebd ph\u1ee5 thu\u1ed9c v\xe0o gi\xe1 tr\u1ecb gradient \u1edf t\u1ea7ng $L$, gi\xe1 tr\u1ecb \u0111\u1ea7u ra h\xe0m k\xedch ho\u1ea1t t\u1ea1i t\u1ea7ng $L - 1$ v\xe0 gi\xe1 tr\u1ecb ma tr\u1eadn tr\u1ecdng s\u1ed1 t\u1ea1i t\u1ea7ng $L - 1$. Do v\u1eady, gi\xe1 tr\u1ecb s\u1ebd co v\u1ec1 ${0}$ r\u1ea5t nhanh v\xec n\xf3 bao g\u1ed3m nhi\u1ec1u s\u1ed1 nh\u1ecf h\u01a1n $1$ nh\xe2n v\u1edbi nhau.\\n- Gradient exploding: t\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\xean nh\u01b0ng ng\u01b0\u1ee3c l\u1ea1i.\\n\\n**C\xe1c ph\u01b0\u01a1ng ph\xe1p ch\u1ed1ng Gradient vanishing/exploding?**\\n\\n- Kh\u1edfi t\u1ea1o ma tr\u1eadn tr\u1ecdng s\u1ed1 b\u1eb1ng c\xe1ch kh\u1edfi t\u1ea1o kh\xe1c: Xavier initialization,\u2026\\n- Ch\u1ecdn h\xe0m k\xedch ho\u1ea1t ph\xf9 h\u1ee3p: nh\u1eefng bi\u1ebfn th\u1ec3 c\u1ee7a Relu (ELU, SELU)\u2026\\n- S\u1eed d\u1ee5ng Batchnorm. Tuy nhi\xean s\u1eed d\u1ee5ng Batchnorm s\u1ebd khi\u1ebfn m\u1ea1ng ph\u1ee9c t\u1ea1p h\u01a1n, t\xednh to\xe1n l\xe2u h\u01a1n, ta n\xean c\xe2n nh\u1eafc gi\u1ea3i ph\xe1p s\u1eed d\u1ee5ng c\xe1c h\xe0m k\xedch ho\u1ea1t ph\xf9 h\u1ee3p tr\u01b0\u1edbc khi \xe1p d\u1ee5ng Batchnorm.\\n- Gradient clipping (x\u1eed l\xfd gradient exploding).\\n![Gradient Clipping](./images/gradient_clipping.PNG)"},{"id":"receptive-field/","metadata":{"permalink":"/blog/receptive-field/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-07-24-receptive-field/index.md","source":"@site/blog/2021-07-24-receptive-field/index.md","title":"Receptive field trong th\u1ecb gi\xe1c m\xe1y t\xednh","description":"Convolution","date":"2021-07-24T00:00:00.000Z","formattedDate":"July 24, 2021","tags":[{"label":"Computer Vision","permalink":"/blog/tags/computer-vision"},{"label":"Receptive Field","permalink":"/blog/tags/receptive-field"},{"label":"Artificial Intelligence","permalink":"/blog/tags/artificial-intelligence"},{"label":"AI","permalink":"/blog/tags/ai"}],"readingTime":3.275,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"receptive-field/","title":"Receptive field trong th\u1ecb gi\xe1c m\xe1y t\xednh","authors":"tranlam","tags":["Computer Vision","Receptive Field","Artificial Intelligence","AI"],"image":"./images/convolution.PNG"},"prevItem":{"title":"M\u1ed9t s\u1ed1 c\xe2u h\u1ecfi ph\u1ecfng v\u1ea5n AI/ML","permalink":"/blog/ai-interview-questions/"},"nextItem":{"title":"\u0110\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh c\u01a1 b\u1ea3n - Ph\u1ea7n 1","permalink":"/blog/linear-algebra-part-1/"}},"content":"![Convolution](./images/convolution.PNG)\\n\\nTrong b\xe0i vi\u1ebft n\xe0y, m\xecnh mu\u1ed1n n\xf3i v\u1ec1 receptive field, m\u1ed9t kh\xe1i ni\u1ec7m r\u1ea5t quan tr\u1ecdng trong c\xe1c b\xe0i to\xe1n th\u1ecb gi\xe1c m\xe1y t\xednh m\xe0 b\u1ea1n n\xe0o h\u1ecdc c\u0169ng c\u1ea7n ph\u1ea3i bi\u1ebft \u0111\u1ec3 gi\u1ea3i th\xedch t\u1ea1i sao ng\u01b0\u1eddi ta l\u1ea1i mu\u1ed1n x\xe2y m\u1ea1ng s\xe2u h\u01a1n. C\xf9ng \u0111i v\xe0o b\xe0i vi\u1ebft th\xf4i.\\n\\n\x3c!--truncate--\x3e\\n\\n### Th\u1ebf n\xe0o l\xe0 receptive field?\\nReceptive field l\xe0 k\xedch th\u01b0\u1edbc c\u1ee7a m\u1ed9t v\xf9ng c\u1ee7a \u0111\u1ea7u v\xe0o m\xe0 c\xf3 \u1ea3nh h\u01b0\u1edfng l\xean m\u1ed9t neuron t\u1ea1i m\u1ed9t layer n\xe0o \u0111\xf3. N\xf3 c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c mi\xeau t\u1ea3 b\u1eb1ng \u0111i\u1ec3m trung t\xe2m v\xe0 k\xedch th\u01b0\u1edbc. Tuy v\u1eady th\xec kh\xf4ng ph\u1ea3i pixel n\xe0o trong receptive field \u0111\u1ec1u c\xf3 \u0111\u1ed9 quan tr\u1ecdng nh\u01b0 nhau \u0111\u1ed1i v\u1edbi neuron m\xe0 n\xf3 \u1ea3nh h\u01b0\u1edfng t\u1edbi. C\xe1c pixel g\u1ea7n trung t\xe2m c\u1ee7a receptive field s\u1ebd \u0111\xf3ng g\xf3p nhi\u1ec1u vai tr\xf2 trong vi\u1ec7c t\xednh to\xe1n neuron tham chi\u1ec1u t\u1edbi h\u01a1n l\xe0 c\xe1c pixel \u1edf r\xeca receptive field.\\n\\n### C\xe1ch t\xednh to\xe1n receptive field\\nGi\u1ea3 s\u1eed, ta c\xf3 \u1ea3nh \u0111\u1ea7u v\xe0o c\xf3 k\xedch th\u01b0\u1edbc chi\u1ec1u d\xe0i, chi\u1ec1u r\u1ed9ng l\xe0 ${n = W = H}$.\\n\\nTa c\xf3 hai c\xe1ch \u0111\u1ec3 m\xf4 t\u1ea3 feature map \u1edf c\xe1c layer trong m\u1ea1ng, v\u1edbi k\xedch th\u01b0\u1edbc c\u1eeda s\u1ed5 t\xedch ch\u1eadp l\xe0 ${3 \\\\times 3}$, padding l\xe0 ${1 \\\\times 1}$, b\u01b0\u1edbc nh\u1ea3y l\xe0 ${2 \\\\times 2}$ nh\u01b0 sau\\n\\n![Convolution 2 Way](./images/conv_2_ways.PNG)\\n\\nT\u1eeb \u0111\xf3, v\u1edbi ${r}$ l\xe0 k\xedch th\u01b0\u1edbc receptive field t\u1ea1i l\u1edbp hi\u1ec7n t\u1ea1i, ${j}$ l\xe0 kho\u1ea3ng c\xe1ch gi\u1eefa 2 \u0111i\u1ec3m \u0111\u1eb7c tr\u01b0ng li\u1ec1n k\u1ec1, ${start}$ l\xe0 t\u1ecda \u0111\u1ed9 \u0111i\u1ec3m ch\xednh gi\u1eefa c\u1ee7a \u0111i\u1ec3m \u0111\u1eb7c tr\u01b0ng tr\xean c\xf9ng g\xf3c tr\xe1i, ${k}$ l\xe0 k\xedch th\u01b0\u1edbc c\u1eeda s\u1ed5 b\u1ed9 l\u1ecdc, ${p}$ l\xe0 padding m\xe0 ta mu\u1ed1n ch\xeam th\xeam \u0111\u1ec3 mu\u1ed1n b\u1ea3o to\xe0n k\xedch th\u01b0\u1edbc \u0111\u1ea7u ra hay kh\xf4ng, ${s}$ l\xe0 b\u01b0\u1edbc nh\u1ea3y c\u1eeda s\u1ed5 t\xedch ch\u1eadp. Ta c\xf3 c\xe1c c\xf4ng th\u1ee9c t\xednh c\xe1c th\xf4ng s\u1ed1 \u0111\u1ea7u ra khi cho m\u1ed9t \u0111\u1ea7u v\xe0o qua c\u1eeda s\u1ed5 t\xedch ch\u1eadp nh\u01b0 sau\\n\\n![Output](./images/output.PNG)\\n\\nD\u01b0\u1edbi \u0111\xe2y l\xe0 minh h\u1ecda v\u1ec1 \xe1p d\u1ee5ng c\xe1ch t\xednh to\xe1n receptive field khi cho \u1ea3nh \u0111\u1ea7u v\xe0o qua m\u1ed9t s\u1ed1 c\u1eeda s\u1ed5 t\xedch ch\u1eadp \u0111\u01a1n gi\u1ea3n\\n\\n![Math](./images/math.PNG)\\n\\n### M\u1ed9t s\u1ed1 nh\u1eadn x\xe9t\\nV\u1ec1 c\xe1ch t\xednh to\xe1n receptive field \u1edf tr\xean, m\xecnh c\xf3 m\u1ed9t s\u1ed1 nh\u1eadn x\xe9t sau\\n- 2 l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp v\u1edbi k\xedch th\u01b0\u1edbc ${3 \\\\times 3}$ s\u1ebd c\xf3 receptive field t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp k\xedch th\u01b0\u1edbc ${5 \\\\times 5}$.\\n- 3 l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp v\u1edbi k\xedch th\u01b0\u1edbc ${3 \\\\times 3}$ s\u1ebd c\xf3 receptive field t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp k\xedch th\u01b0\u1edbc ${7 \\\\times 7}$.\\n- 5 l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp v\u1edbi k\xedch th\u01b0\u1edbc ${3 \\\\times 3}$ s\u1ebd c\xf3 receptive field t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp k\xedch th\u01b0\u1edbc ${11 \\\\times 11}$.\\n\\nV\xe0 t\u01b0\u01a1ng t\u1ef1 nh\u01b0 v\u1eady, ta c\xf3 th\u1ec3 thay th\u1ebf c\xe1c l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp k\xedch th\u01b0\u1edbc l\u1edbn b\u1eb1ng nhi\u1ec1u c\u1eeda s\u1ed5 t\xedch ch\u1eadp c\xf3 k\xedch th\u01b0\u1edbc nh\u1ecf h\u01a1n trong m\u1ed9t m\u1ea1ng n\u01a1-ron t\xedch ch\u1eadp. \u0110i\u1ec1u n\xe0y c\xf3 2 l\u1ee3i \xedch l\xe0 \\n- T\u0103ng \u0111\u1ed9 phi tuy\u1ebfn cho m\u1ea1ng, b\u1edfi v\xec khi ch\xfang ta c\xe0ng s\u1eed d\u1ee5ng nhi\u1ec1u l\u1edbp c\u1eeda s\u1ed5 t\xedch ch\u1eadp, ta c\xe0ng s\u1eed d\u1ee5ng nhi\u1ec1u h\u01a1n c\xe1c h\xe0m k\xedch ho\u1ea1t cho c\xe1c l\u1edbp \u0111\xf3.\\n- Gi\u1ea3m thi\u1ec3u l\u01b0\u1ee3ng bi\u1ebfn s\u1ed1 m\xf4 h\xecnh c\u1ea7n h\u1ecdc. V\xed d\u1ee5: ${N}$ b\u1ed9 l\u1ecdc t\xedch ch\u1eadp ${5 \\\\times 5 \\\\times D}$ s\u1ebd t\u1ea1o ra ${5 \\\\times 5 \\\\times D \\\\times N = 25 \\\\times D \\\\times N}$ bi\u1ebfn s\u1ed1 c\u1ea7n h\u1ecdc, trong khi \u0111\xf3, ${2N}$ b\u1ed9 l\u1ecdc t\xedch ch\u1eadp k\xedch th\u01b0\u1edbc ${3 \\\\times 3 \\\\times D}$ s\u1ebd t\u1ea1o ra ${3 \\\\times 3 \\\\times D \\\\times 2N = 18 \\\\times D \\\\times N}$ bi\u1ebfn s\u1ed1. \u0110i\u1ec1u n\xe0y t\u01b0\u1ee3ng t\u1ef1 v\u1edbi c\xe1c tr\u01b0\u1eddng h\u1ee3p kh\xe1c, c\xe0ng nhi\u1ec1u c\u1eeda s\u1ed5 t\xedch ch\u1eadp nh\u1ecf \u0111\u01b0\u1ee3c d\xf9ng \u0111\u1ec3 thay th\u1ebf c\xe1c c\u1eeda s\u1ed5 k\xedch th\u01b0\u1edbc l\u1edbn h\u01a1n th\xec l\u01b0\u1ee3ng bi\u1ebfn s\u1ed1 c\xe0ng gi\u1ea3m xu\u1ed1ng.\\n\\n### T\xe0i li\u1ec7u tham kh\u1ea3o\\n\\n[A guide to receptive field arithmetic for Convolutional Neural Networks](https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)"},{"id":"linear-algebra-part-1/","metadata":{"permalink":"/blog/linear-algebra-part-1/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-07-10-linear-algebra-p1/index.md","source":"@site/blog/2021-07-10-linear-algebra-p1/index.md","title":"\u0110\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh c\u01a1 b\u1ea3n - Ph\u1ea7n 1","description":"Linear Algebra","date":"2021-07-10T00:00:00.000Z","formattedDate":"July 10, 2021","tags":[{"label":"Math","permalink":"/blog/tags/math"},{"label":"Linear Algebra","permalink":"/blog/tags/linear-algebra"}],"readingTime":25.18,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"linear-algebra-part-1/","title":"\u0110\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh c\u01a1 b\u1ea3n - Ph\u1ea7n 1","authors":"tranlam","tags":["Math","Linear Algebra"],"image":"./images/linear-algebra.JPEG"},"prevItem":{"title":"Receptive field trong th\u1ecb gi\xe1c m\xe1y t\xednh","permalink":"/blog/receptive-field/"},"nextItem":{"title":"X\xe1c su\u1ea5t th\u1ed1ng k\xea c\u01a1 b\u1ea3n","permalink":"/blog/probability/"}},"content":"![Linear Algebra](./images/linear-algebra.JPEG)\\n\\nTi\u1ebfp \u0111\xe2y s\u1ebd l\xe0 lo\u1ea1t b\xe0i vi\u1ebft v\u1ec1 \u0111\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh m\xecnh \u0111\xe3 h\u1ecdc l\u1ea1i khi \u0111\u1ecdc quy\u1ec3n **[Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf)** trong th\u1eddi gian h\u1ecdc v\u1ec1 Machine Learning v\xe0 AI. \u0110\xe2y l\xe0 ph\u1ea7n th\u1ee9 nh\u1ea5t trong lo\u1ea1t b\xe0i n\xe0y.\\n\\n\x3c!--truncate--\x3e\\n\\n### Gi\u1ea3i h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh\\nH\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh l\xe0 t\u1eadp c\xe1c ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh v\u1edbi c\xf9ng nh\u1eefng bi\u1ebfn s\u1ed1, c\u1ea5p 2 ch\xfang ta \u0111\xe3 gi\u1ea3i ch\xe1n ch\xea ph\u01b0\u01a1ng tr\xecnh n\xe0y, th\u1eadm ch\xed c\xf2n ph\u1ea3i l\xe0m c\xe1c h\u1ec7 ph\u01b0\u01a1ng tr\xecnh kh\xf3 nh\u1eb1n h\u01a1n ch\u1ee9a c\u1ea3 bi\u1ebfn b\u1eadc 2, b\u1eadc 3,... Gi\u1edd \u0111\xe2y, tr\u1edf l\u1ea1i v\u1edbi \u0111\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh, ta c\xf3 th\u1ec3 m\xf4 h\xecnh h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh b\u1eadc m\u1ed9t v\u1edbi t\xedch c\u1ee7a ma tr\u1eadn v\xe0 v\xe9c-t\u01a1 t\u01b0\u01a1ng \u1ee9ng.\\nV\xed d\u1ee5 ta c\xf3 m\u1ed9t h\u1ec7 ph\u01b0\u01a1ng tr\xecnh \u0111\u01a1n gi\u1ea3n sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\begin{cases} x_{1} + 8x_{3} - 4x_{4} = 42 \\\\\\\\\\\\\\\\ x_{2} + 2x_{3} + 12x_{4} = 8 \\\\end{cases}    \\n}$ \\n\\n</p>\\n\\nS\u1ebd \u0111\u01b0\u1ee3c vi\u1ebft d\u01b0\u1edbi d\u1ea1ng t\xedch ma tr\u1eadn v\xe0 v\xe9c-t\u01a1 bi\u1ebfn s\u1ed1 nh\u01b0 sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\begin{bmatrix} 1 & 0 & 8 & -4 \\\\\\\\\\\\\\\\ 0 & 1 & 2 & 12 \\\\end{bmatrix} * \\\\begin{bmatrix} x_{1} \\\\\\\\\\\\\\\\ x_{2} \\\\\\\\\\\\\\\\ x_{3} \\\\\\\\\\\\\\\\ x_{4} \\\\end{bmatrix} = \\\\begin{bmatrix} 42 \\\\\\\\\\\\\\\\ 8 \\\\end{bmatrix}\\n}$\\n\\n${\\n\\\\Leftrightarrow \\\\begin{bmatrix} 1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} x_{1} + \\\\begin{bmatrix} 0 \\\\\\\\\\\\\\\\ 1 \\\\end{bmatrix} x_{2} + \\\\begin{bmatrix} 8 \\\\\\\\\\\\\\\\ 2 \\\\end{bmatrix} x_{3} + \\\\begin{bmatrix} -4 \\\\\\\\\\\\\\\\ 12 \\\\end{bmatrix} x_{4} = \\\\begin{bmatrix} 42 \\\\\\\\\\\\\\\\ 8 \\\\end{bmatrix} \\\\\\\\\\\\;\\\\\\\\\\\\;\\\\\\\\\\\\;\\\\\\\\\\\\;(1)\\n}$\\n\\n</p>\\n\\n#### Nghi\u1ec7m chung v\xe0 nghi\u1ec7m ri\xeang c\u1ee7a ph\u01b0\u01a1ng tr\xecnh\\nNh\xecn v\xe0o bi\u1ec3u th\u1ee9c b\xean tr\xean ${(1)}$, ta c\xf3\\n- M\u1ed9t nghi\u1ec7m ri\xeang c\u1ee7a h\u1ec7 ph\u01b0\u01a1ng tr\xecnh l\xe0 ${\\\\begin{bmatrix} 42 & 8 & 0 & 0 \\\\end{bmatrix}^T}$.\\n- \u0110\u1ec3 c\xf3 th\u1ec3 thu \u0111\u01b0\u1ee3c t\u1ea5t c\u1ea3 nghi\u1ec7m th\u1ecfa m\xe3n, ta c\u1ea7n ph\u1ea3i \u0111i gi\u1ea3i ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ n\u1eefa. Ta ti\u1ebfn h\xe0nh ph\xe2n t\xedch ph\u01b0\u01a1ng tr\xecnh ${(1)}$ ra nh\u01b0 sau\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\begin{bmatrix} 1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} x_{1} + \\\\begin{bmatrix} 0 \\\\\\\\\\\\\\\\ 1 \\\\end{bmatrix} x_{2} + 8\\\\begin{bmatrix} 1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} x_{3} + 2\\\\begin{bmatrix} 0 \\\\\\\\\\\\\\\\ 1 \\\\end{bmatrix} x_{3} + \\\\begin{bmatrix} -4 \\\\\\\\\\\\\\\\ 12 \\\\end{bmatrix} x_{4} = \\\\boldsymbol{0} \\n}$\\n\\n</p>\\n\\nNh\xecn v\xe0o ph\u01b0\u01a1ng tr\xecnh tr\xean, ta c\xf3 \u0111\u01b0\u1ee3c m\u1ed9t nghi\u1ec7m cho ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ l\xe0 ${\\\\lambda_{1}\\\\begin{bmatrix} 8 & 2 & -1 & 0 \\\\end{bmatrix}^T}$.\\nT\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\xean, ta t\xecm th\xeam \u0111\u01b0\u1ee3c m\u1ed9t nghi\u1ec7m cho ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ n\u1eefa l\xe0 ${\\\\lambda_{2}\\\\begin{bmatrix} -4 & 12 & 0 & -1 \\\\end{bmatrix}^T}$.\\nPh\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ ch\u1ec9 c\xf3 2 nghi\u1ec7m, t\u1ea1i sao l\u1ea1i nh\u01b0 v\u1eady th\xec b\xean d\u01b0\u1edbi m\xecnh s\u1ebd n\xf3i sau.\\nNh\u01b0 v\u1eady, h\u1ec7 ph\u01b0\u01a1ng tr\xecnh c\xf3 nghi\u1ec7m chung l\xe0 \\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n    \\\\\\\\{ x \\\\in \\\\Bbb R^4: x = \\\\begin{bmatrix} 42 \\\\\\\\\\\\\\\\ 8 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} + \\\\lambda_{1}\\\\begin{bmatrix} 8 \\\\\\\\\\\\\\\\ 2 \\\\\\\\\\\\\\\\ -1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} + \\\\lambda_{2}\\\\begin{bmatrix} -4 \\\\\\\\\\\\\\\\ 12 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ -1 \\\\end{bmatrix}, \\\\lambda_{1}, \\\\lambda_{2} \\\\in \\\\Bbb R \\\\\\\\}\\n}$\\n\\n</p>\\n\\n#### Bi\u1ebfn \u0111\u1ed5i ma tr\u1eadn\\n##### V\xed d\u1ee5\\nM\xecnh s\u1ebd \u0111em m\u1ed9t v\xed d\u1ee5 cho ph\u01b0\u01a1ng ph\xe1p n\xe0y\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\begin{cases} \\n-2x_{1} + 4x_{2} - 2x_{3} - x_{4} + 4x_{5} = -3 \\\\\\\\\\\\\\\\\\n4x_{1} - 8x_{2} + 3x_{3} - 3x_{4} + x_{5} = 2 \\\\\\\\\\\\\\\\\\nx_{1} - 2x_{2} + x_{3} - x_{4} + x_{5} = 0 \\\\\\\\\\\\\\\\\\nx_{1} - 2x_{2} - 3x_{4} + x_{5} = a \\\\\\\\\\\\\\\\\\n\\\\end{cases}    \\n}$ \\n\\n</p>\\n\\nVi\u1ebft th\xe0nh d\u1ea1ng\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\left[\\n  \\\\begin{matrix}\\n    -2 & 4 & -2 & -1 & 4 \\\\\\\\\\\\\\\\\\n    4 & -8 & 3 & -3 & 1 \\\\\\\\\\\\\\\\\\n    1 & -2 & 1 & -1 & 1 \\\\\\\\\\\\\\\\\\n    1 & -2 & 0 & -3 & 1 \\\\\\\\\\\\\\\\\\n  \\\\end{matrix}\\n\\\\left|\\n\\\\begin{matrix}\\n    -3  \\\\\\\\\\\\\\\\\\n    2  \\\\\\\\\\\\\\\\\\n    0  \\\\\\\\\\\\\\\\\\n    a  \\\\\\\\\\\\\\\\\\n\\\\end{matrix}\\n\\\\right.\\n\\\\right]\\n}$\\n\\n${\\n\\\\rightsquigarrow ... \\\\rightsquigarrow\\n\\\\left[\\n  \\\\begin{matrix}\\n    1 & -2 & 1 & -1 & 1 \\\\\\\\\\\\\\\\\\n    0 & 0 & 1 & -1 & 3 \\\\\\\\\\\\\\\\\\n    0 & 0 & 0 & 1 & -2 \\\\\\\\\\\\\\\\\\n    0 & 0 & 0 & 0 & 0 \\\\\\\\\\\\\\\\\\n  \\\\end{matrix}\\n\\\\left|\\n\\\\begin{matrix}\\n    0  \\\\\\\\\\\\\\\\\\n    -2  \\\\\\\\\\\\\\\\\\n    1  \\\\\\\\\\\\\\\\\\n    a+1  \\\\\\\\\\\\\\\\\\n\\\\end{matrix}\\n\\\\right.\\n\\\\right]\\n}$\\n\\n</p>\\n\\nDo v\u1eady, h\u1ec7 ph\u01b0\u01a1ng tr\xecnh \u0111\u01b0\u1ee3c gi\u1ea3i khi ${a = -1}$.\\n- Nghi\u1ec7m ri\xeang c\u1ee7a ph\u01b0\u01a1ng tr\xecnh l\xe0 ${\\\\begin{bmatrix} 2 & 0 & -1 & 1 & 0 \\\\end{bmatrix}^T}$.\\n- Nghi\u1ec7m c\u1ee7a ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ l\xe0 ${\\\\lambda_{1}\\\\begin{bmatrix} 2 & 1 & 0 & 0 & 0 \\\\end{bmatrix}^T}$ v\xe0 ${\\\\lambda_{2}\\\\begin{bmatrix} -2 & 0 & 1 & -2 & -1 \\\\end{bmatrix}^T}$.\\n\\nNh\u01b0 v\u1eady, nghi\u1ec7m chung c\u1ee7a ph\u01b0\u01a1ng tr\xecnh l\xe0 \\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n    \\\\\\\\{ x \\\\in \\\\Bbb R^5: x = \\\\begin{bmatrix} 2 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ -1 \\\\\\\\\\\\\\\\ 1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} + \\\\lambda_{1}\\\\begin{bmatrix} 2 \\\\\\\\\\\\\\\\ 1 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} + \\\\lambda_{2}\\\\begin{bmatrix} -2 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 1 \\\\\\\\\\\\\\\\ -2 \\\\\\\\\\\\\\\\ -1 \\\\end{bmatrix}, \\\\lambda_{1}, \\\\lambda_{2} \\\\in \\\\Bbb R \\\\\\\\}\\n}$\\n\\n</p>\\n\\n\\n##### Ma tr\u1eadn b\u1eadc thang\\nM\u1ed9t ma tr\u1eadn \u1edf d\u1ea1ng b\u1eadc thang n\u1ebfu\\n- Nh\u1eefng h\xe0ng ch\u1ec9 ch\u1ee9a 0 s\u1ebd \u1edf \u0111\xe1y ma tr\u1eadn. Nh\u1eefng h\xe0ng ch\u1ee9a \xedt nh\u1ea5t 1 s\u1ed1 kh\xe1c kh\xf4ng s\u1ebd n\u1eb1m tr\xean c\xe1c h\xe0ng ch\u1ee9a to\xe0n 0.\\n- C\xe1c ph\u1ea7n t\u1eed pivot c\u1ee7a m\u1ed7i h\xe0ng lu\xf4n \u1edf ph\xeda b\xean ph\u1ea3i c\u1ee7a c\xe1c ph\u1ea7n t\u1eed pivot \u1edf c\xe1c h\xe0ng b\xean tr\xean.\\n\\nC\xe1c bi\u1ebfn \u1ee9ng v\u1edbi c\xe1c ph\u1ea7n t\u1eed pivot l\xe0 c\xe1c basic variables, c\xe1c bi\u1ebfn c\xf2n l\u1ea1i l\xe0 free variables. Do v\u1eady m\xe0 \u1edf **M\u1ee5c 1.1** ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ ch\u1ec9 c\xf3 hai nghi\u1ec7m, t\u1ea1i ch\u1ec9 c\xf3 2 bi\u1ebfn l\xe0 free variables (t\u1ee9c l\xe0 c\xe1c bi\u1ebfn \u1ee9ng v\u1edbi c\xe1c ph\u1ea7n t\u1eed kh\xf4ng l\xe0 ph\u1ea7n t\u1eed pivot).\\n\\nV\u1edbi h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{b}}$, \u0111\u1ec3 t\xednh to\xe1n m\u1ed9t nghi\u1ec7m ri\xeang, ta bi\u1ec3u di\u1ec5n c\xe1c ${\\\\boldsymbol{b} = \\\\sum_{i = 1}^p \\\\lambda_{i}\\\\boldsymbol{p_{i}}}$ v\u1edbi ${\\\\boldsymbol{p_{i}}}$ l\xe0 c\xe1c pivot columns, ch\xfang ta th\u01b0\u1eddng b\u1eaft \u0111\u1ea7u \u01b0\u1edbc l\u01b0\u1ee3ng c\xe1c gi\xe1 tr\u1ecb ${\\\\lambda_{i}}$ v\u1edbi c\xe1c pivot columns t\u1eeb ph\u1ea3i sang tr\xe1i.\\n\\nM\u1ed9t ma tr\u1eadn \u1edf d\u1ea1ng b\u1eadc thang t\u1ed1i gi\u1ea3n n\u1ebfu\\n- N\xf3 l\xe0 m\u1ed9t ma tr\u1eadn b\u1eadc thang.\\n- C\xe1c ph\u1ea7n t\u1eed pivot \u0111\u1ec1u b\u1eb1ng 1.\\n- Ph\u1ea7n t\u1eed pivot l\xe0 ph\u1ea7n t\u1eed duy nh\u1ea5t kh\xe1c 0 t\u1ea1i pivot column \u0111\xf3.\\n\\nVi\u1ec7c t\xednh to\xe1n nghi\u1ec7m ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ s\u1ebd d\u1ec5 d\xe0ng h\u01a1n r\u1ea5t nhi\u1ec1u n\u1ebfu ma tr\u1eadn bi\u1ec3u di\u1ec5n h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh \u1edf d\u1ea1ng b\u1eadc thang t\u1ed1i gi\u1ea3n \\n\\n##### Ph\xe9p kh\u1eed Gaussian\\nL\xe0 m\u1ed9t thu\u1eadt to\xe1n bi\u1ec3u di\u1ec5n c\xe1c ph\xe9p bi\u1ebfn \u0111\u1ed5i tri\u1ec7t ti\xeau gi\u1eefa c\xe1c h\xe0ng \u0111\u1ec3 \u0111\u01b0a ma tr\u1eadn bi\u1ec3u di\u1ec5n h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh v\u1ec1 d\u1ea1ng ma tr\u1eadn b\u1eadc thang t\u1ed1i gi\u1ea3n.\\n\\n\u0110\u1ec3 t\xednh to\xe1n nghi\u1ec7m c\u1ee7a ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ trong ma tr\u1eadn b\u1eadc thang t\u1ed1i gi\u1ea3n, ta bi\u1ec3u di\u1ec5n c\xe1c pivot column b\u1eb1ng t\u1ed5ng c\xe1c c\u1ea5p s\u1ed1 c\u1ee7a c\xe1c pivot columns \u1edf b\xean tr\xe1i c\u1ee7a ch\xfang.\\n\\nV\xed d\u1ee5 v\u1edbi ma tr\u1eadn b\u1eadc thang sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\boldsymbol{A} = \\n\\\\begin{bmatrix} \\n1 & 3 & 0 & -4 & 0 \\\\\\\\\\\\\\\\ \\n0 & 0 & 1 & -3 & 0 \\\\\\\\\\\\\\\\\\n0 & 0 & 0 & 0 & 1 \\\\\\\\\\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix}\\n}$\\n\\n</p>\\n\\nNghi\u1ec7m c\u1ee7a ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ l\xe0\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n    \\\\\\\\{ x \\\\in \\\\Bbb R^5: x = \\\\lambda_{1}\\\\begin{bmatrix} 3 \\\\\\\\\\\\\\\\ -1 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix} + \\\\lambda_{2}\\\\begin{bmatrix} -4 \\\\\\\\\\\\\\\\ 0 \\\\\\\\\\\\\\\\ -3 \\\\\\\\\\\\\\\\ -1 \\\\\\\\\\\\\\\\ 0 \\\\end{bmatrix}, \\\\lambda_{1}, \\\\lambda_{2} \\\\in \\\\Bbb R \\\\\\\\}\\n}$\\n\\n</p>\\n\\n#### The -1 trick\\nV\u1edbi ma tr\u1eadn ${\\\\boldsymbol{A}}$ \u1edf tr\xean, ta s\u1ebd ch\xeam c\xe1c h\xe0ng g\u1ed3m to\xe0n ${0}$ v\xe0 ch\u1ec9 m\u1ed9t s\u1ed1 ${-1}$ v\xe0o gi\u1eefa c\xe1c h\xe0ng c\u1ee7a ma tr\u1eadn b\u1eadc thang t\u1ed1i gi\u1ea3n, \u0111\u1ec3 t\u1ea1o n\xean ma tr\u1eadn m\u1edbi c\xf3 \u0111\u01b0\u1eddng ch\xe9o ch\xednh g\u1ed3m to\xe0n ${-1}$ v\xe0 ${1}$ nh\u01b0 d\u01b0\u1edbi \u0111\xe2y.\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${ \\n\\\\boldsymbol{A} \\\\rightsquigarrow\\n\\\\begin{bmatrix} \\n1 & 3 & 0 & -4 & 0 \\\\\\\\\\\\\\\\ \\n0 & -1 & 0 & 0 & 0 \\\\\\\\\\\\\\\\\\n0 & 0 & 1 & -3 & 0 \\\\\\\\\\\\\\\\\\n0 & 0 & 0 & -1 & 0 \\\\\\\\\\\\\\\\\\n0 & 0 & 0 & 0 & 1 \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix}\\n}$\\n\\n</p>\\n\\nT\u1eeb \u0111\xf3, c\xe1c c\u1ed9t ch\u1ee9a gi\xe1 tr\u1ecb ${-1}$ t\u1ea1i c\xe1c v\u1ecb tr\xed tr\xean \u0111\u01b0\u1eddng ch\xe9o ch\xednh c\u1ee7a ma tr\u1eadn s\u1ebd l\xe0 nghi\u1ec7m c\u1ee7a ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$.\\n\\n\\n#### M\u1ed9t s\u1ed1 thu\u1eadt to\xe1n \u0111\u1ec3 gi\u1ea3i h\u1ec7 ph\u01b0\u01a1ng tr\xecnh n\xe0y\\nC\xf3 m\u1ed9t s\u1ed1 thu\u1eadt to\xe1n th\xf4ng d\u1ee5ng \u0111\u1ec3 gi\u1ea3i h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh ph\u1ee9c t\u1ea1p\\n- S\u1eed d\u1ee5ng thu\u1eadt to\xe1n h\u1ed3i quy tuy\u1ebfn t\xednh (linear regression), thu\u1eadt to\xe1n n\xe0y th\u01b0\u1eddng \xe1p d\u1ee5ng v\xe0o c\xe1c b\xe0i to\xe1n m\xe0 ta ch\u1ec9 c\xf3 th\u1ec3 t\xednh x\u1ea5p x\u1ec9 nghi\u1ec7m c\u1ee7a ph\u01b0\u01a1ng tr\xecnh.\\n- Normal equation, thu\u1eadt to\xe1n n\xe0y t\xednh ch\xednh x\xe1c nghi\u1ec7m c\u1ee7a h\u1ec7 ph\u01b0\u01a1ng tr\xecnh, nh\u01b0ng s\u1ed1 l\u01b0\u1ee3ng t\xednh to\xe1n r\u1ea5t nhi\u1ec1u n\xean kh\xf4ng th\u1ec3 s\u1eed d\u1ee5ng v\u1edbi c\xe1c tr\u01b0\u1eddng h\u1ee3p s\u1ed1 l\u01b0\u1ee3ng bi\u1ebfn nhi\u1ec1u. Ph\u01b0\u01a1ng ph\xe1p n\xe0y \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi \u0111\xe2y\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n    \\\\boldsymbol{Ax} = \\\\boldsymbol{b} \\\\Leftrightarrow \\\\boldsymbol{A^TAx} = \\\\boldsymbol{A^Tb} \\\\Leftrightarrow \\\\boldsymbol{x} = \\\\boldsymbol{(A^TA)^{-1}A^Tb}\\n}$\\n\\n</p>\\n\\nVi\u1ec7c ph\u1ea3i nh\xe2n ma tr\u1eadn v\xe0 t\xednh to\xe1n ngh\u1ecbch \u0111\u1ea3o khi\u1ebfn ph\u01b0\u01a1ng ph\xe1p n\xe0y c\xf3 \u0111\u1ed9 ph\u1ee9c t\u1eadp l\xe0 ${\\\\Theta(n^3)}$.\\n\\n### Kh\xf4ng gian v\xe9c-t\u01a1\\n\\n#### Nh\xf3m\\nCho m\u1ed9t t\u1eadp ${\\\\mathcal{G}}$ v\u1edbi ph\xe9p to\xe1n ${\\\\otimes: \\\\mathcal{G} \\\\times \\\\mathcal{G} \\\\to \\\\mathcal{G}}$ \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a tr\xean ${\\\\mathcal{G}}$ th\xec ${\\\\mathcal{G} := (\\\\mathcal{G}, \\\\otimes)}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 m\u1ed9t nh\xf3m n\u1ebfu th\u1ecfa m\xe3n c\xe1c t\xednh ch\u1ea5t\\n- T\xednh \u0111\xf3ng g\xf3i c\u1ee7a ${\\\\mathcal{G}}$ trong ph\xe9p ${\\\\otimes}$: ${\\\\forall x, y \\\\in \\\\mathcal{G}}$ th\xec ${x \\\\otimes y \\\\in \\\\mathcal{G}}$.\\n- T\xednh k\u1ebft h\u1ee3p: ${\\\\forall x, y, z \\\\in \\\\mathcal{G}}$ th\xec ${(x \\\\otimes y) \\\\otimes z = x \\\\otimes (y \\\\otimes z)}$.\\n- T\u1ed3n t\u1ea1i ph\u1ea7n t\u1eed \u0111\u01a1n v\u1ecb: ${\\\\exists e \\\\in \\\\mathcal{G}, \\\\forall x \\\\in \\\\mathcal{G}}$ th\u1ecfa m\xe3n ${x \\\\otimes e = x}$ v\xe0 ${e \\\\otimes x = x}$.\\n- T\u1ed3n t\u1ea1i ph\u1ea7n t\u1eed ngh\u1ecbch \u0111\u1ea3o: ${\\\\forall x \\\\in \\\\mathcal{G}, \\\\exists y \\\\in \\\\mathcal{G}}$ th\u1ecfa m\xe3n ${x \\\\otimes y = e}$ v\xe0 ${y \\\\otimes x = e}$.\\n\\nN\u1ebfu nh\xf3m c\xf3 th\xeam t\xednh ch\u1ea5t giao ho\xe1n ${\\\\forall x, y \\\\in \\\\mathcal{G}: x \\\\otimes y = y \\\\otimes x}$ th\xec nh\xf3m \u0111\xf3 \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 nh\xf3m Abelian.\\n\\nM\u1ed9t nh\xf3m c\xe1c ma tr\u1eadn kh\u1ea3 ngh\u1ecbch ${A \\\\in \\\\Bbb R^{n \\\\times n}}$ v\u1edbi ph\xe9p to\xe1n nh\xe2n ma tr\u1eadn \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 m\u1ed9t General Linear Group. Tuy nhi\xean, ph\xe9p nh\xe2n ma tr\u1eadn kh\xf4ng c\xf3 t\xednh ch\u1ea5t giao ho\xe1n n\xean nh\xf3m n\xe0y kh\xf4ng ph\u1ea3i l\xe0 m\u1ed9t nh\xf3m Abelian. \\n\\n#### Kh\xf4ng gian v\xe9c-t\u01a1\\nM\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 th\u1ef1c ${V = (\\\\mathcal{V}, + , \\\\cdot)}$ l\xe0 m\u1ed9t t\u1eadp ${V}$ v\u1edbi 2 ph\xe9p to\xe1n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n  +: \\\\mathcal{V} \\\\times \\\\mathcal{V} \\\\to \\\\mathcal{V} }$\\n\\n${\\n  \\\\cdot: \\\\Bbb R \\\\times \\\\mathcal{V} \\\\to \\\\mathcal{V}\\n}$\\n\\n</p>\\n\\nTh\u1ecfa m\xe3n\\n- ${\\\\mathcal{V}, +}$ l\xe0 m\u1ed9t nh\xf3m Abelian.\\n- T\xednh ch\u1ea5t ph\xe2n ph\u1ed1i ${\\\\forall \\\\lambda \\\\in \\\\Bbb R, x, y \\\\in \\\\mathcal{V}: \\\\lambda \\\\cdot(x + y) = \\\\lambda \\\\cdot x  + \\\\lambda \\\\cdot y}$ v\xe0 ${\\\\forall \\\\lambda, \\\\psi \\\\in \\\\Bbb R, x \\\\in \\\\mathcal{V}: (\\\\lambda + \\\\psi)\\\\cdot x = \\\\lambda \\\\cdot x + \\\\psi \\\\cdot x}$\\n- T\xednh ch\u1ea5t k\u1ebft h\u01a1p ${\\\\forall \\\\lambda, \\\\psi \\\\in \\\\Bbb R, x \\\\in \\\\mathcal{V}: \\\\lambda \\\\cdot(\\\\psi \\\\cdot x) = (\\\\lambda \\\\cdot \\\\psi)\\\\cdot x}$\\n- T\u1ed3n t\u1ea1i ph\u1ea7n t\u1eed \u0111\u01a1n v\u1ecb ${\\\\forall x \\\\in \\\\mathcal{V}: 1 \\\\cdot x = x}$\\n\\n#### Kh\xf4ng gian v\xe9c-t\u01a1 con\\nV\u1edbi ${V = (\\\\mathcal{V}, + , \\\\cdot)}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 v\xe0 ${\\\\mathcal{U} \\\\subseteq \\\\mathcal{V}, \\\\mathcal{U} \\\\ne \\\\emptyset}$ th\xec ${U = (\\\\mathcal{U}, + , \\\\cdot)}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$ n\u1ebfu ${U}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 c\xf9ng v\u1edbi c\xe1c ph\xe9p to\xe1n ${+}$ v\xe0 ${\\\\cdot}$ \u1ee9ng v\u1edbi ${\\\\mathcal{U} \\\\times \\\\mathcal{U}}$ v\xe0 ${\\\\Bbb R \\\\times \\\\mathcal{U}}$. K\xed hi\u1ec7u ${U \\\\subseteq V}$ th\u1ec3 hi\u1ec7n ${U}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$.\\n\\nN\u1ebfu ${U}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$, ${U}$ s\u1ebd th\u1eeba h\u01b0\u1edfng t\u1ea5t c\u1ea3 c\xe1c t\xednh ch\u1ea5t c\u1ee7a ${V}$. \u0110\u1ec3 ch\u1ee9ng minh ${U}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$, ch\xfang ta v\u1eabn ph\u1ea3i ch\u1ec9 ra \u0111\u01b0\u1ee3c\\n- ${\\\\mathcal{U} \\\\ne \\\\emptyset}$ hay ${0 \\\\in \\\\mathcal{U}}$.\\n- T\xednh \u0111\xf3ng g\xf3i c\u1ee7a ${U}$: ${\\\\forall \\\\lambda \\\\in \\\\Bbb R, \\\\forall x \\\\in \\\\mathcal{U}: \\\\lambda x \\\\in \\\\mathcal{U}}$ v\xe0 ${\\\\forall x, y \\\\in \\\\mathcal{U}: x + y \\\\in \\\\mathcal{U}}$.\\n\\n### Ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh\\n\\n#### T\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh\\nCho m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ v\xe0 m\u1ed9t s\u1ed1 h\u1eefu h\u1ea1n c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}} \\\\in V}$, m\u1ed7i ${\\\\boldsymbol{v} \\\\in V}$ \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n  \\\\boldsymbol{v} = \\\\lambda_{1}\\\\boldsymbol{x_{1}} + \\\\lambda_{2}\\\\boldsymbol{x_{2}} + ... + \\\\lambda_{k}\\\\boldsymbol{x_{k}} = \\\\sum_{i=1}^{k} \\\\lambda_{i}\\\\boldsymbol{x_{i}} \\\\in V\\n}$\\n\\n</p>\\n\\nv\u1edbi ${\\\\lambda_{1}, \\\\lambda_{2},..., \\\\lambda_{k} \\\\in \\\\Bbb R}$, l\xe0 m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$.\\n\\nV\xe9c-t\u01a1 ${\\\\boldsymbol{0}}$ c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c vi\u1ebft d\u01b0\u1edbi d\u1ea1ng ${\\\\boldsymbol{0} = \\\\sum_{i=1}^{k}0\\\\boldsymbol{x_{i}}}$ nh\u01b0ng ch\xfang ta quan t\xe2m nhi\u1ec1u h\u01a1n \u0111\u1ebfn c\xe1c t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh kh\xf4ng t\u1ea7m th\u01b0\u1eddng h\u01a1n.\\n\\n#### Ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh\\nN\u1ebfu c\xf3 m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh kh\xf4ng t\u1ea7m th\u01b0\u1eddng th\u1ecfa m\xe3n ${0 = \\\\sum_{i=1}^{k} = \\\\lambda_{i}\\\\boldsymbol{x_{i}}}$ v\u1edbi \xedt nh\u1ea5t m\u1ed9t gi\xe1 tr\u1ecb ${\\\\lambda_{i} \\\\ne 0}$ th\xec c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 ***ph\u1ee5 thu\u1ed9c*** tuy\u1ebfn t\xednh.\\n\\nN\u1ebfu m\xe0 bi\u1ec3u th\u1ee9c tr\xean ch\u1ec9 t\u1ed3n t\u1ea1i nghi\u1ec7m t\u1ea7m th\u01b0\u1eddng ${\\\\lambda_{1} = \\\\lambda_{2} = ... = \\\\lambda_{k} = 0}$ th\xec c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$ l\xe0 ***\u0111\u1ed9c l\u1eadp*** tuy\u1ebfn t\xednh.\\n\\nM\u1ed9t s\u1ed1 t\xednh ch\u1ea5t cho c\xe1c v\xe9c-t\u01a1 ki\u1ec3u n\xe0y l\xe0\\n- ${k}$ v\xe9c-t\u01a1 tr\xean ho\u1eb7c l\xe0 \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh, ho\u1eb7c l\xe0 ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh, ch\u1ee9 kh\xf4ng c\xf3 lo\u1ea1i kh\xe1c.\\n- N\u1ebfu \xedt nh\u1ea5t m\u1ed9t trong s\u1ed1 c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$ l\xe0 v\xe9c-t\u01a1 ${\\\\boldsymbol{0}}$ th\xec ch\xfang s\u1ebd ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh. T\xednh ch\u1ea5t n\xe0y c\u0169ng t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi vi\u1ec7c c\xf3 2 v\xe9c-t\u01a1 gi\u1ed1ng nhau trong t\u1eadp ${k}$ v\xe9c-t\u01a1 tr\xean.\\n- T\u1eadp c\xe1c v\xe9c-t\u01a1 tr\xean l\xe0 ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh n\u1ebfu nh\u01b0 m\u1ed9t trong s\u1ed1 c\xe1c v\xe9c-t\u01a1 \u0111\xf3 c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a c\xe1c v\xe9c-t\u01a1 c\xf2n l\u1ea1i.\\n- Ta vi\u1ebft t\u1ea5t c\u1ea3 v\xe9c-t\u01a1 th\xe0nh c\xe1c c\u1ed9t c\u1ee7a m\u1ed9t ma tr\u1eadn ${\\\\boldsymbol{A}}$, sau \u0111\xf3 bi\u1ec3u di\u1ec5n ph\xe9p kh\u1eed Gaussian, ta \u0111\u01b0\u1ee3c c\xe1c pivot columns s\u1ebd \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh v\u1edbi c\xe1c v\xe9c-t\u01a1 \u1edf b\xean tr\xe1i c\u1ee7a v\xe9c-t\u01a1 \u0111\xf3, c\xf2n c\xe1c c\u1ed9t kh\xf4ng ph\u1ea3i pivot columns s\u1ebd c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a c\xe1c pivot columns \u1edf b\xean tr\xe1i c\u1ee7a n\xf3. N\u1ebfu t\u1ea5t c\u1ea3 c\xe1c c\u1ed9t \u0111\u1ec1u l\xe0 pivot columns th\xec t\u1ea5t c\u1ea3 c\xe1c v\xe9c-t\u01a1 \u0111\xf3 l\xe0 \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh, c\xf2n kh\xf4ng th\xec ch\xfang s\u1ebd l\xe0 ph\u1ee5 thu\u1ed9c tuy\u1ebfn t\xednh.\\n\\n### C\u01a1 s\u1edf v\xe0 rank\\n\\n#### H\u1ec7 sinh c\u1ee7a m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1\\nCho m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${V = (\\\\mathcal{V}, + , \\\\cdot)}$ v\xe0 m\u1ed9t t\u1eadp c\xe1c v\xe9c-t\u01a1 ${\\\\mathcal{A} = \\\\\\\\{\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}\\\\\\\\} \\\\subseteq \\\\mathcal{V}}$ . N\u1ebfu m\u1ed7i v\xe9c-t\u01a1 ${\\\\boldsymbol{v} \\\\in \\\\mathcal{V}}$ \u0111\u1ec1u c\xf3 th\u1ec3 bi\u1ec3u di\u1ec5n l\xe0 m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$, th\xec ${\\\\mathcal{A}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 h\u1ec7 sinh c\u1ee7a ${V}$. T\u1eadp t\u1ea5t c\u1ea3 t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a ${\\\\mathcal{A}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 m\u1ed9t span c\u1ee7a ${\\\\mathcal{A}}$. N\u1ebfu ${\\\\mathcal{A}}$ span kh\xf4ng gian v\xe9c-t\u01a1 ${V}$, ch\xfang ta vi\u1ebft ${V = span[\\\\mathcal{A}]}$ ho\u1eb7c ${V = span[\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}]}$.\\n\\n#### C\u01a1 s\u1edf c\u1ee7a m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1\\nCho m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${V = (\\\\mathcal{V}, + , \\\\cdot)}$ v\xe0 h\u1ec7 sinh ${\\\\mathcal{A} \\\\subseteq \\\\mathcal{V}}$, ${\\\\mathcal{A}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 h\u1ec7 sinh nh\u1ecf nh\u1ea5t n\u1ebfu kh\xf4ng t\u1ed3n t\u1ea1i m\u1ed9t h\u1ec7 sinh nh\u1ecf h\u01a1n n\xe0o kh\xe1c m\xe0 ${\\\\tilde{\\\\mathcal{A}} \\\\subsetneq \\\\mathcal{A} \\\\subseteq \\\\mathcal{V}}$ span kh\xf4ng gian v\xe9c-t\u01a1 ${V}$. T\u1ea5t c\u1ea3 h\u1ec7 sinh \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ l\xe0 nh\u1ecf nh\u1ea5t v\xe0 \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 c\u01a1 s\u1edf c\u1ee7a ${V}$.\\n\\nV\u1edbi ${V = (\\\\mathcal{V}, + , \\\\cdot)}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 v\xe0 ${\\\\mathcal{B} \\\\subseteq \\\\mathcal{V}}$ v\xe0 ${\\\\mathcal{B} \\\\ne \\\\emptyset}$. T\u1ea5t c\u1ea3 m\u1ec7nh \u0111\u1ec1 sau l\xe0 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng\\n- ${\\\\mathcal{B}}$ l\xe0 m\u1ed9t c\u01a1 s\u1edf c\u1ee7a ${V}$.\\n- ${\\\\mathcal{B}}$ l\xe0 h\u1ec7 sinh nh\u1ecf nh\u1ea5t.\\n- ${\\\\mathcal{B}}$ l\xe0 m\u1ed9t t\u1eadp g\u1ed3m s\u1ed1 v\xe9c-t\u01a1 \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh l\u1edbn nh\u1ea5t c\u1ee7a ${V}$.\\n- T\u1ea5t c\u1ea3 ${\\\\boldsymbol{v} \\\\in \\\\mathcal{V}}$ l\xe0 m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a c\xe1c v\xe9c-t\u01a1 trong ${\\\\mathcal{B}}$ v\xe0 t\u1ea5t c\u1ea3 t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh l\xe0 duy nh\u1ea5t. N\u1ebfu ${\\\\boldsymbol{x} = \\\\sum_{i=1}^{k}\\\\lambda_{i}b_{i} = \\\\sum_{i=1}^{k}\\\\psi_{i}b_{i}}$ th\xec ${\\\\lambda_{i} = \\\\psi_{i}}$.\\n\\nTa c\xf3 v\xed d\u1ee5 v\u1ec1 h\u1ec7 sinh chu\u1ea9n t\u1eafc c\u1ee7a ${\\\\Bbb R^{3}}$ l\xe0\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n    \\\\mathcal{B} = \\\\\\\\{\\n\\\\begin{bmatrix} \\n1 \\\\\\\\\\\\\\\\ \\n0 \\\\\\\\\\\\\\\\\\n0 \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix}, \\n\\\\begin{bmatrix} \\n0 \\\\\\\\\\\\\\\\ \\n1 \\\\\\\\\\\\\\\\\\n0 \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix}, \\n\\\\begin{bmatrix} \\n0 \\\\\\\\\\\\\\\\ \\n0 \\\\\\\\\\\\\\\\\\n1 \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix}\\n\\\\\\\\}\\n}$\\n\\n</p>\\n\\nM\u1ed9t s\u1ed1 k\u1ebft lu\u1eadn \u0111\u01b0\u1ee3c r\xfat ra nh\u01b0 sau\\n- T\u1ea5t c\u1ea3 c\u01a1 s\u1edf \u0111\u1ec1u c\xf3 s\u1ed1 c\xe1c ph\u1ea7n t\u1eed b\u1eb1ng nhau. S\u1ed1 chi\u1ec1u c\u1ee7a m\u1ed9t kh\xf4ng gian = s\u1ed1 c\xe1c h\u01b0\u1edbng \u0111\u1ed9c l\u1eadp nhau trong kh\xf4ng gian \u0111\xf3. S\u1ed1 chi\u1ec1u c\u1ee7a m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 s\u1ebd l\xe0 s\u1ed1 v\xe9c-t\u01a1 trong h\u1ec7 c\u01a1 s\u1edf c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 \u0111\xf3 dim(${V}$) = s\u1ed1 v\xe9c-t\u01a1 trong c\u01a1 s\u1edf c\u1ee7a n\xf3.\\n- N\u1ebfu ${U \\\\subseteq V}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$, th\xec dim(${U}$) ${\\\\le}$ dim(${V}$), dim(${U}$) ${=}$ dim(${V}$) khi v\xe0 ch\u1ec9 khi ${U = V}$.\\n- M\u1ed9t c\u01a1 s\u1edf c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 con ${U = span[\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}] \\\\subseteq \\\\Bbb R^{m}}$ c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c t\xecm b\u1eb1ng c\xe1ch sau\\n  - Vi\u1ebft c\xe1c v\xe9c-t\u01a1 ${\\\\boldsymbol{x_{1}}, \\\\boldsymbol{x_{2}},..., \\\\boldsymbol{x_{k}}}$ d\u01b0\u1edbi d\u1ea1ng m\u1ed9t c\u1ed9t c\u1ee7a m\u1ed9t ma tr\u1eadn ${\\\\boldsymbol{A}}$.\\n  - Bi\u1ebfn \u0111\u1ed5i ${\\\\boldsymbol{A}}$ v\u1ec1 d\u1ea1ng ma tr\u1eadn b\u1eadc thang t\u1ed1i gi\u1ea3n.\\n  - C\xe1c v\xe9c-t\u01a1 li\xean k\u1ebft v\u1edbi c\xe1c pivot columns l\xe0 c\u01a1 s\u1edf c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 con ${U}$.\\n\\n#### Rank c\u1ee7a m\u1ed9t ma tr\u1eadn\\nH\u1ea1ng c\u1ee7a m\u1ed9t ma tr\u1eadn ${\\\\boldsymbol{A}}$ ${=}$ s\u1ed1 v\xe9c-t\u01a1 c\u1ed9t \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh c\u1ee7a ma tr\u1eadn ${\\\\boldsymbol{A}}$ ${=}$ s\u1ed1 v\xe9c-t\u01a1 h\xe0ng \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh c\u1ee7a ma tr\u1eadn ${\\\\boldsymbol{A}}$ ${=}$ rank(${\\\\boldsymbol{A}}$) ${=}$ rk(${\\\\boldsymbol{A}}$).\\n\\nDo v\u1eady, ta c\xf3 m\u1ed9t s\u1ed1 nh\u1eadn x\xe9t sau\\n- rk(${\\\\boldsymbol{A}}$) ${=}$ rk(${\\\\boldsymbol{A^{T}}}$).\\n- C\xe1c v\xe9c-t\u01a1 c\u1ed9t c\u1ee7a ${\\\\boldsymbol{A \\\\in \\\\Bbb R^{m \\\\times n}}}$ span m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${U \\\\subseteq \\\\Bbb R^{m}}$ v\u1edbi dim(${U}$) ${=}$ rk(${\\\\boldsymbol{A}}$).\\n- C\xe1c v\xe9c-t\u01a1 h\xe0ng c\u1ee7a ${\\\\boldsymbol{A \\\\in \\\\Bbb R^{m \\\\times n}}}$ span m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${W \\\\subseteq \\\\Bbb R^{n}}$ v\u1edbi dim(${W}$) ${=}$ rk(${\\\\boldsymbol{A}}$).\\n- V\u1edbi m\u1ecdi ${\\\\boldsymbol{A \\\\in \\\\Bbb R^{n \\\\times n}}}$, ${\\\\boldsymbol{A}}$ l\xe0 ma tr\u1eadn kh\u1ea3 ngh\u1ecbch khi v\xe0 ch\u1ec9 khi rk(${\\\\boldsymbol{A}}$) ${= n}$.\\n- H\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{b}}$ c\xf3 nghi\u1ec7m n\u1ebfu rk(${\\\\boldsymbol{A}}$) ${=}$ rk(${\\\\boldsymbol{A|b}}$).\\n- V\u1edbi ${\\\\boldsymbol{A \\\\in \\\\Bbb R^{m \\\\times n}}}$, kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a b\u1ed9 nghi\u1ec7m ph\u01b0\u01a1ng tr\xecnh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ c\xf3 dim ${= n - }$ rk(${\\\\boldsymbol{A}}$).\\n- M\u1ed9t ma tr\u1eadn ${\\\\boldsymbol{A \\\\in \\\\Bbb R^{m \\\\times n}}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 ma tr\u1eadn h\u1ea1ng \u0111\u1ea7y \u0111\u1ee7 n\u1ebfu rk(${\\\\boldsymbol{A}}$) ${= min(m, n)}$.\\n\\n### \xc1nh x\u1ea1 tuy\u1ebfn t\xednh\\nHai kh\xf4ng gian v\xe9c-t\u01a1 th\u1ef1c ${V, W}$, m\u1ed9t \xe1nh x\u1ea1 ${\\\\Phi: V \\\\to W}$ b\u1ea3o to\xe0n c\u1ea5u tr\xfac c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 n\u1ebfu\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\Phi(x+y) = \\\\Phi(x) + \\\\Phi(y) \\\\\\\\\\\\\\\\\\n\\\\Phi(\\\\lambda x) = \\\\lambda \\\\Phi(x)  \\n}$\\n\\n</p>\\n\\nV\u1edbi t\u1ea5t c\u1ea3 ${x, y \\\\in V}$ v\xe0 ${\\\\lambda \\\\in \\\\Bbb R}$.\\n\\n\xc1nh x\u1ea1 tuy\u1ebfn t\xednh: V\u1edbi c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 ${V, W}$, m\u1ed9t \xe1nh x\u1ea1 ${\\\\Phi: V \\\\to W}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh n\u1ebfu ${\\\\forall x, y \\\\in V, \\\\forall \\\\lambda, \\\\psi \\\\in \\\\Bbb R: \\\\Phi(\\\\lambda x+ \\\\psi y) = \\\\lambda \\\\Phi(x) + \\\\psi \\\\Phi(y)}$. \xc1nh x\u1ea1 tuy\u1ebfn t\xednh c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u1ea1ng ma tr\u1eadn, v\u1ec1 ph\u1ea7n n\xe0y m\xecnh s\u1ebd n\xf3i \u1edf d\u01b0\u1edbi.\\n\\nHai kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ v\xe0 ${W}$ c\xf3 chi\u1ec1u h\u1eefu h\u1ea1n \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 \u0111\u1eb3ng c\u1ea5u khi v\xe0 ch\u1ec9 khi dim(${V}$) ${=}$ dim(${W}$). Do v\u1eady, t\u1ed3n t\u1ea1i m\u1ed9t song \xe1nh gi\u1eefa 2 kh\xf4ng gian v\xe9c-t\u01a1 c\xf9ng s\u1ed1 chi\u1ec1u. T\u1ee9c l\xe0, hai kh\xf4ng gian v\xe9c-t\u01a1 c\xf3 s\u1ed1 chi\u1ec1u b\u1eb1ng nhau l\xe0 hai th\u1ee9 gi\u1ed1ng nhau, khi m\xe0 n\xf3 c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ebfn \u0111\u1ed5i sang nhau m\xe0 kh\xf4ng m\u1ea5t m\xe1t th\xf4ng tin g\xec.\\n\\nTa c\xf3 m\u1ed9t s\u1ed1 t\xednh ch\u1ea5t c\u1ee7a \xe1nh x\u1ea1 tuy\u1ebfn t\xednh\\n- N\u1ebfu \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ v\xe0 ${\\\\psi: W \\\\to X}$, ph\xe9p \xe1nh x\u1ea1 ${\\\\psi \\\\circ \\\\Phi: V \\\\to X}$ c\u0169ng l\xe0 m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh.\\n- N\u1ebfu ${\\\\Phi: V \\\\to W}$ l\xe0 m\u1ed9t \u0111\u1eb3ng c\u1ea5u th\xec ${\\\\Phi^{-1}: W \\\\to V}$ c\u0169ng l\xe0 m\u1ed9t \u0111\u1eb3ng c\u1ea5u.\\n- N\u1ebfu ${\\\\Phi: V \\\\to W}$, ${\\\\psi: V \\\\to W}$ l\xe0 \xe1nh x\u1ea1 tuy\u1ebfn t\xednh th\xec ${\\\\Phi + \\\\psi, \\\\lambda \\\\Phi}$ v\u1edbi ${\\\\lambda \\\\in \\\\Bbb R}$ c\u0169ng l\xe0 c\xe1c \xe1nh x\u1ea1 tuy\u1ebfn t\xednh.\\n\\n#### Bi\u1ec3u di\u1ec5n \xe1nh x\u1ea1 tuy\u1ebfn t\xednh d\u01b0\u1edbi d\u1ea1ng ma tr\u1eadn\\nX\xe9t m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ v\u1edbi c\u01a1 s\u1edf l\xe0  ${B = (\\\\boldsymbol{b_{1}}, \\\\boldsymbol{b_{2}},..., \\\\boldsymbol{b_{n}})}$. V\u1edbi m\u1ecdi v\xe9c-t\u01a1 ${\\\\boldsymbol{x} \\\\in V}$, ta c\xf3 m\u1ed9t t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh duy nh\u1ea5t l\xe0 \\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\boldsymbol{x} = \\\\lambda_{1} \\\\boldsymbol{b_{1}} + \\\\lambda_{2} \\\\boldsymbol{b_{2}} +...+ \\\\lambda_{n} \\\\boldsymbol{b_{n}}\\n}$\\n\\n</p>\\n\\nTh\xec ${\\\\lambda_{1}, \\\\lambda_{2},..., \\\\lambda_{n}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 t\u1ecda \u0111\u1ed9 c\u1ee7a ${\\\\boldsymbol{x}}$ \u0111\u1ed1i v\u1edbi c\u01a1 s\u1edf ${B}$.\\n\\n${\\\\boldsymbol{\\\\lambda} = \\\\begin{bmatrix} \\n\\\\lambda_{1} \\\\\\\\\\\\\\\\ \\n\\\\lambda_{2} \\\\\\\\\\\\\\\\\\n\\\\vdots \\\\\\\\\\\\\\\\\\n\\\\lambda_{n} \\\\\\\\\\\\\\\\\\n\\\\end{bmatrix} }$ \u0111\u01b0\u1ee3c g\xf3i l\xe0 v\xe9c-t\u01a1 t\u1ecda \u0111\u1ed9 c\u1ee7a ${\\\\boldsymbol{x}}$ \u0111\u1ed1i v\u1edbi c\u01a1 s\u1edf ${B}$. Do v\u1eady, v\u1edbi m\u1ed9t v\xe9c-t\u01a1 v\u1edbi c\xe1c c\u01a1 s\u1edf kh\xe1c nhau th\xec s\u1ebd c\xf3 t\u1ecda \u0111\u1ed9 l\xe0 kh\xe1c nhau.\\n\\nX\xe9t c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ v\xe0 ${W}$ v\u1edbi c\xe1c c\u01a1 s\u1edf t\u01b0\u01a1ng \u1ee9ng l\xe0 ${B = (\\\\boldsymbol{b_{1}}, \\\\boldsymbol{b_{2}},..., \\\\boldsymbol{b_{n}})}$ v\xe0 ${C = (\\\\boldsymbol{c_{1}}, \\\\boldsymbol{c_{2}},..., \\\\boldsymbol{c_{m}})}$v\xe0 m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$. V\u1edbi ${j \\\\in \\\\\\\\{1, 2,..., n\\\\\\\\}}$\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\Phi(\\\\boldsymbol{b_{j}}) = \\\\lambda_{1j}\\\\boldsymbol{c_{1}} + \\\\lambda_{1j}\\\\boldsymbol{c_{2}} +...+ \\\\lambda_{mj}\\\\boldsymbol{c_{m}} = \\\\sum_{i=1}^{m}\\\\lambda_{ij}\\\\boldsymbol{c_{i}}\\n}$\\n\\n</p>\\n\\nl\xe0 m\u1ed9t bi\u1ec3u di\u1ec5n duy nh\u1ea5t c\u1ee7a ${\\\\Phi(\\\\boldsymbol{b_{j}})}$ v\u1edbi c\u01a1 s\u1edf ${C}$. Ch\xfang ta g\u1ecdi ma tr\u1eadn ${\\\\boldsymbol{A_{\\\\Phi}}}$ k\xedch th\u01b0\u1edbc ${m \\\\times n}$ v\u1edbi ${\\\\boldsymbol{A_{\\\\Phi}(i, j)} = \\\\lambda_{ij}}$ l\xe0 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i c\u1ee7a ${\\\\Phi}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${B}$ c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 ${V}$ v\xe0 c\u01a1 s\u1edf ${C}$ c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 ${W}$.\\n\\nT\u1ecda \u0111\u1ed9 c\u1ee7a ${\\\\Phi(\\\\boldsymbol{b_{j}})}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${C}$ c\u1ee7a ${W}$ l\xe0 c\u1ed9t th\u1ee9 ${j_{th}}$ c\u1ee7a ma tr\u1eadn ${\\\\boldsymbol{A_{\\\\Phi}}}$.\\n\\nDo \u0111\xf3, n\u1ebfu ${\\\\boldsymbol{\\\\hat{x}}}$ l\xe0 v\xe9c-t\u01a1 t\u1ecda \u0111\u1ed9 c\u1ee7a ${\\\\boldsymbol{x} \\\\in V}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${B}$ v\xe0 ${\\\\boldsymbol{\\\\hat{y}}}$ l\xe0 v\xe9c-t\u01a1 t\u1ecda \u0111\u1ed9 c\u1ee7a ${\\\\boldsymbol{y} = \\\\Phi(\\\\boldsymbol{x}) \\\\in W}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${C}$. Ta c\xf3\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\boldsymbol{\\\\hat{y}} = \\\\boldsymbol{A_{\\\\Phi}} \\\\boldsymbol{\\\\hat{x}}\\n}$\\n\\n</p>\\n\\n#### Chuy\u1ec3n \u0111\u1ed5i c\u01a1 s\u1edf\\nM\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$, x\xe9t 2 c\u01a1 s\u1edf sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${B = (\\\\boldsymbol{b_{1}}, \\\\boldsymbol{b_{2}},..., \\\\boldsymbol{b_{n}})}$ v\xe0 ${\\\\tilde{B} = (\\\\boldsymbol{\\\\tilde{b_{1}}}, \\\\boldsymbol{\\\\tilde{b_{2}}},..., \\\\boldsymbol{\\\\tilde{b_{n}}})}$ c\u1ee7a ${V}$.\\n\\n${C = (\\\\boldsymbol{c_{1}}, \\\\boldsymbol{c_{2}},..., \\\\boldsymbol{c_{n}})}$ v\xe0 ${\\\\tilde{C} = (\\\\boldsymbol{\\\\tilde{c_{1}}}, \\\\boldsymbol{\\\\tilde{c_{2}}},..., \\\\boldsymbol{\\\\tilde{c_{n}}})}$ c\u1ee7a ${W}$.\\n\\n</p>\\n\\n${\\\\boldsymbol{A_{\\\\Phi}} \\\\in \\\\Bbb R^{m \\\\times n}}$ l\xe0 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i c\u1ee7a \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${B}$ v\xe0 ${C}$.\\n\\n${\\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}} \\\\in \\\\Bbb R^{m \\\\times n}}$ l\xe0 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i c\u1ee7a \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${\\\\tilde{B}}$ v\xe0 ${\\\\tilde{C}}$.\\n\\nB\u1eb1ng vi\u1ec7c chuy\u1ec3n \u0111\u1ed5i c\u01a1 s\u1edf v\xe0 c\xe1c bi\u1ec3u di\u1ec5n c\u1ee7a c\xe1c v\xe9c-t\u01a1, c\xe1c ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i \u1ee9ng v\u1edbi c\xe1c c\u01a1 s\u1edf m\u1edbi c\xf3 th\u1ec3 \u1edf d\u1ea1ng r\u1ea5t \u0111\u01a1n gi\u1ea3n v\xe0 d\u1ec5 d\xe0ng th\u1ef1c hi\u1ec7n c\xe1c b\u01b0\u1edbc t\xednh to\xe1n trung gian.\\n\\nV\u1edbi 2 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i nh\u01b0 \u1edf ph\u1ea7n tr\xean, t\u01b0\u01a1ng quan gi\u1eefa ch\xfang \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n nh\u01b0 sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}} = \\\\boldsymbol{T^{-1}}\\\\boldsymbol{A_{\\\\phi}}\\\\boldsymbol{S}}$\\n\\n</p>\\n\\nV\u1edbi ${\\\\boldsymbol{S} \\\\in \\\\Bbb R^{n \\\\times n}}$ l\xe0 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i c\u1ee7a ph\xe9p \xe1nh x\u1ea1 \u0111\u01a1n v\u1ecb li\xean k\u1ebft t\u1ecda \u0111\u1ed9 c\u1ee7a v\xe9c-t\u01a1 \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${\\\\tilde{B}}$ l\xean t\u1ecda \u0111\u1ed9 \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${B}$.\\nV\u1edbi ${\\\\boldsymbol{T} \\\\in \\\\Bbb R^{m \\\\times m}}$ l\xe0 ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i c\u1ee7a ph\xe9p \xe1nh x\u1ea1 \u0111\u01a1n v\u1ecb li\xean k\u1ebft t\u1ecda \u0111\u1ed9 c\u1ee7a v\xe9c-t\u01a1 \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${\\\\tilde{C}}$ l\xean t\u1ecda \u0111\u1ed9 \u1ee9ng v\u1edbi c\u01a1 s\u1edf ${C}$.\\n(Ph\u1ea7n ch\u1ee9ng minh chi ti\u1ebft b\u1ea1n c\xf3 th\u1ec3 truy c\u1eadp link s\xe1ch m\xe0 m\xecnh \u0111\u1ec3 \u1edf tr\xean)\\n\\nNh\u01b0 v\u1eady, m\u1ed9t ph\xe9p bi\u1ebfn \u0111\u1ed5i c\u01a1 s\u1edf trong ${V}$ (c\u01a1 s\u1edf ${B}$ \u0111\u01b0\u1ee3c thay th\u1ebf b\u1eb1ng c\u01a1 s\u1edf ${\\\\tilde{B}}$) v\xe0 ${W}$ (c\u01a1 s\u1edf ${C}$ \u0111\u01b0\u1ee3c thay th\u1ebf b\u1eb1ng c\u01a1 s\u1edf ${\\\\tilde{C}}$), ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i ${\\\\boldsymbol{A_{\\\\Phi}}}$ c\u1ee7a ph\xe9p \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ \u0111\u01b0\u1ee3c thay th\u1ebf b\u1edfi ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i t\u01b0\u01a1ng \u0111\u01b0\u01a1ng ${\\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}}}$.\\n\\n![Basis Change](./images/basischange.PNG)\\n\\nNh\xecn v\xe0o h\xecnh tr\xean, ta th\u1ea5y \u0111\u01b0\u1ee3c\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\\\Phi_{\\\\tilde{B}\\\\tilde{C}} = \\\\Xi_{\\\\tilde{C}C} \\\\circ \\\\Phi_{CB} \\\\circ \\\\psi_{B\\\\tilde{B}} = \\\\Xi_{C\\\\tilde{C}}^{-1} \\\\circ \\\\Phi_{CB} \\\\circ \\\\psi_{B\\\\tilde{B}}}$\\n\\n</p>\\n\\nV\u1edbi ${\\\\psi_{B\\\\tilde{B}} = id_{V}}$ v\xe0 ${\\\\Xi_{\\\\tilde{C}C} = id_{W}}$ l\xe0 c\xe1c \xe1nh x\u1ea1 \u0111\u01a1n v\u1ecb \xe1nh x\u1ea1 c\xe1c v\xe9c-t\u01a1 l\xean ch\xednh ch\xfang, nh\u01b0ng m\xe0 \u1ee9ng v\u1edbi c\xe1c c\u01a1 s\u1edf kh\xe1c nhau.\\n\\nSau \u0111\xe2y, m\xecnh \u0111\u01b0a ra 2 kh\xe1i ni\u1ec7m sau v\u1ec1 ma tr\u1eadn\\n- T\u01b0\u01a1ng \u0111\u01b0\u01a1ng: 2 ma tr\u1eadn ${\\\\boldsymbol{A}, \\\\boldsymbol{\\\\tilde{A}} \\\\in \\\\Bbb R^{m \\\\times n}}$ l\xe0 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng n\u1ebfu t\u1ed3n t\u1ea1i hai ma tr\u1eadn kh\u1ea3 ng\u1ecbch ${\\\\boldsymbol{S} \\\\in \\\\Bbb R^{n \\\\times n}}$ v\xe0  ${\\\\boldsymbol{T} \\\\in \\\\Bbb R^{m \\\\times m}}$ th\u1ecfa m\xe3n ${\\\\boldsymbol{\\\\tilde{A}} = \\\\boldsymbol{T}^{-1}\\\\boldsymbol{A}\\\\boldsymbol{S}}$.\\n- \u0110\u1ed3ng d\u1ea1ng: 2 ma tr\u1eadn ${\\\\boldsymbol{A}, \\\\boldsymbol{\\\\tilde{A}} \\\\in \\\\Bbb R^{n \\\\times n}}$ l\xe0 \u0111\u1ed3ng d\u1ea1ng n\u1ebfu t\u1ed3n t\u1ea1i ma tr\u1eadn kh\u1ea3 ng\u1ecbch ${\\\\boldsymbol{S} \\\\in \\\\Bbb R^{n \\\\times n}}$ th\u1ecfa m\xe3n ${\\\\boldsymbol{\\\\tilde{A}} = \\\\boldsymbol{S}^{-1}\\\\boldsymbol{A}\\\\boldsymbol{S}}$.\\n\\nCu\u1ed1i c\xf9ng, x\xe9t c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 ${V, W, X}$. C\xe1c \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ v\u1edbi ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i l\xe0 ${\\\\boldsymbol{A_{\\\\Phi}}}$ v\xe0 ${\\\\psi: W \\\\to X}$ v\u1edbi ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i l\xe0 ${\\\\boldsymbol{A_{\\\\psi}}}$, \xe1nh x\u1ea1 ${\\\\psi \\\\circ \\\\Phi: V \\\\to X}$ c\u0169ng l\xe0 \xe1nh x\u1ea1 tuy\u1ebfn t\xednh v\u1edbi ma tr\u1eadn bi\u1ebfn \u0111\u1ed5i l\xe0 ${\\\\boldsymbol{A_{\\\\psi \\\\circ \\\\Phi}} = \\\\boldsymbol{A_{\\\\psi}}\\\\boldsymbol{A_{\\\\Phi}}}$. Ch\u1ee9ng minh nh\u01b0 sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\boldsymbol{A_{\\\\Phi}}: B \\\\to C; \\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}}: \\\\tilde{B} \\\\to \\\\tilde{C}; \\\\boldsymbol{S}: \\\\tilde{B} \\\\to B; \\\\boldsymbol{T}: \\\\tilde{C} \\\\to C\\n}$\\n\\n${\\n\\\\Rightarrow \\\\tilde{B} \\\\to \\\\tilde{C} = \\\\tilde{B} \\\\to B \\\\to C \\\\to \\\\tilde{C}\\n}$\\n\\n${\\n\\\\Rightarrow \\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}} = \\\\boldsymbol{T}^{-1}\\\\boldsymbol{A_{\\\\Phi}}\\\\boldsymbol{S}\\n}$\\n\\n</p>\\n\\nB\u1edfi v\xec\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\n\\\\boldsymbol{x} \\\\longmapsto \\\\boldsymbol{Sx} \\\\longmapsto \\\\boldsymbol{A_{\\\\Phi}}(\\\\boldsymbol{Sx}) \\\\longmapsto \\\\boldsymbol{T}^{-1}(\\\\boldsymbol{A_{\\\\Phi}}(\\\\boldsymbol{Sx})) = \\\\boldsymbol{\\\\tilde{A_{\\\\Phi}}}(\\\\boldsymbol{x})\\n}$\\n\\n</p>\\n\\n#### Image v\xe0 kernel\\nCho ${\\\\Phi: V \\\\to W}$, ch\xfang ta \u0111\u1ecbnh ngh\u0129a kernel/null space l\xe0\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${Ker(\\\\Phi) := \\\\Phi^{-1}(\\\\boldsymbol{0_{W}}) = \\\\\\\\{\\\\boldsymbol{v} \\\\in V: \\\\Phi(\\\\boldsymbol{v}) = \\\\boldsymbol{0_{W}}\\\\\\\\}}$.\\n\\n</p>\\n\\nV\xe0 image/range l\xe0\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${Im(\\\\Phi) := \\\\Phi(V) = \\\\\\\\{\\\\boldsymbol{w} \\\\in W | \\\\exists \\\\boldsymbol{v} \\\\in V: \\\\Phi(\\\\boldsymbol{v}) = \\\\boldsymbol{w}\\\\\\\\}}$.\\n\\n</p>\\n\\nCh\xfang ta g\u1ecdi V v\xe0 W l\u1ea7n l\u01b0\u1ee3t l\xe0 c\xe1c domain v\xe0 codomain c\u1ee7a ${\\\\Phi}$.\\nM\u1ed9t v\xe0i t\xednh ch\u1ea5t \u0111\u01b0\u1ee3c r\xfat ra nh\u01b0 sau\\n- Kernel kh\xf4ng bao gi\u1edd l\xe0 t\u1eadp r\u1ed7ng khi m\xe0 ta lu\xf4n c\xf3 ${\\\\Phi({\\\\boldsymbol{0_{V}}}) = \\\\boldsymbol{0_{W}}}$.\\n- ${Im(\\\\Phi) \\\\subseteq W }$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${W}$ v\xe0 ${Ker(\\\\Phi) \\\\subseteq V}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$.\\n- ${\\\\Phi}$ l\xe0 m\u1ed9t \u0111\u01a1n \xe1nh khi v\xe0 ch\u1ec9 khi ${Ker(\\\\Phi) = \\\\\\\\{\\\\boldsymbol{0}\\\\\\\\}}$.\\n\\nCho m\u1ed9t ma tr\u1eadn ${\\\\boldsymbol{A} \\\\in \\\\Bbb R^{m \\\\times n}}$ l\xe0 m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: \\\\Bbb R^{n} \\\\to \\\\Bbb R^{m}, \\\\boldsymbol{x} \\\\longmapsto \\\\boldsymbol{Ax}}$. V\u1edbi ${\\\\boldsymbol{A} = [\\\\boldsymbol{a_{1}}, \\\\boldsymbol{a_{2}},..., \\\\boldsymbol{a_{n}}]}$\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${\\nIm(\\\\Phi) = \\\\\\\\{\\\\boldsymbol{Ax}: x \\\\in \\\\Bbb R^{n}\\\\\\\\} = \\\\\\\\{\\\\sum_{i=1}^{n}x_{i}\\\\boldsymbol{a_{i}}: x_{1},..., x_{n} \\\\in \\\\Bbb R\\\\\\\\} = span[\\\\boldsymbol{a_{1}}, \\\\boldsymbol{a_{2}},..., \\\\boldsymbol{a_{n}}] \\\\subseteq \\\\Bbb R^{m}\\n}$.\\n\\n</p>\\n\\nImage \u1edf \u0111\xe2y s\u1ebd l\xe0 span c\u1ee7a c\xe1c c\u1ed9t trong ma tr\u1eadn ${A}$, v\xe0 c\u0169ng \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 column space. B\u1edfi v\u1eady, column space l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${\\\\Bbb R^{m}}$.\\n- rk(${A}$) ${=}$ dim(${Im(\\\\Phi)}$).\\n- Kernel ${Ker(\\\\Phi)}$ l\xe0 nghi\u1ec7m chung c\u1ee7a h\u1ec7 ph\u01b0\u01a1ng tr\xecnh tuy\u1ebfn t\xednh ${\\\\boldsymbol{Ax} = \\\\boldsymbol{0}}$ v\xe0 bao g\u1ed3m t\u1ea5t c\u1ea3 t\u1ed5 h\u1ee3p tuy\u1ebfn t\xednh c\u1ee7a c\xe1c ph\u1ea7n t\u1eed thu\u1ed9c ${\\\\Bbb R^{n}}$ m\xe0 cho ra k\u1ebft qu\u1ea3 ${\\\\boldsymbol{0} \\\\in \\\\Bbb R^{m}}$.\\n- Kernel l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${\\\\Bbb R^{n}}$ t\u1ea1i \u0111\xf3 ${n}$ l\xe0 chi\u1ec1u r\u1ed9ng c\u1ee7a ma tr\u1eadn.\\n- Kernel t\u1eadp trung v\xe0o c\xe1c m\u1ed1i quan h\u1ec7 c\u1ee7a c\xe1c v\xe9c-t\u01a1 c\u1ed9t.\\n\\nV\u1edbi c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 ${V, W}$ v\xe0 m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$\\n<p style={{textAlign: \\"center\\"}}> \\n\\ndim(${Ker(\\\\Phi)}$) + dim(${Im(\\\\Phi)}$) = dim(${V}$)\\n\\n</p>\\n\\n- N\u1ebfu dim(${Im(\\\\Phi)}$) < dim(${V}$) th\xec ${Ker(\\\\Phi)}$ l\xe0 m\u1ed9t t\u1eadp kh\xf4ng t\u1ea7m th\u01b0\u1eddng, n\xf3 s\u1ebd g\u1ed3m v\xe9c-t\u01a1 ${0_{V}}$ v\xe0 c\xe1c v\xe9c-t\u01a1 kh\xe1c n\u1eefa.\\n- N\u1ebfu ${\\\\boldsymbol{A_{\\\\Phi}}}$ v\xe0 dim(${Im(\\\\Phi)}$) < dim(${V}$), th\xec ${\\\\boldsymbol{A_{\\\\Phi}x} = 0}$ c\xf3 v\xf4 s\u1ed1 nghi\u1ec7m.\\n- N\u1ebfu dim(${V}$) = dim(${W}$) th\xec ${\\\\Phi}$ l\xe0 \u0111\u01a1n \xe1nh, to\xe0n \xe1nh, song \xe1nh, ${Im(\\\\Phi) \\\\subseteq W}$.\\n\\n### Kh\xf4ng gian v\xe9c-t\u01a1 Affine\\n\\n#### Kh\xf4ng gian v\xe9c-t\u01a1 Affine con\\nV\u1edbi V l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1, ${\\\\boldsymbol{x_{0}} \\\\in V}$ v\xe0 ${U \\\\subseteq V}$ l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1. N\u1ebfu t\u1eadp con\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${\\nL = \\\\boldsymbol{x_{0}} + U := \\\\\\\\{\\\\boldsymbol{x_{0}} + \\\\boldsymbol{u}: \\\\boldsymbol{u} \\\\in U\\\\\\\\} = \\\\\\\\{\\\\boldsymbol{v} \\\\in V | \\\\exists \\\\boldsymbol{u} \\\\in U: \\\\boldsymbol{v} = \\\\boldsymbol{x_{0}} + \\\\boldsymbol{u}\\\\\\\\} \\\\subseteq V\\n}$\\n\\n</p>\\n\\n\u0111\u01b0\u1ee3c g\u1ecdi l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 Affine con. ${U}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 kh\xf4ng gian h\u01b0\u1edbng v\xe0 ${\\\\boldsymbol{x_{0}}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 support point.\\n- \u0110\u1ecbnh ngh\u0129a c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 Affine s\u1ebd kh\xf4ng ch\u1ee9a ${0}$ n\u1ebfu ${\\\\boldsymbol{x_{0}} \\\\notin U}$. V\xec th\u1ebf m\xe0 kh\xf4ng gian v\xe9c-t\u01a1 Affine kh\xf4ng ph\u1ea3i l\xe0 m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 con c\u1ee7a ${V}$ v\u1edbi ${\\\\boldsymbol{x_{0}} \\\\notin U}$.\\n\\nCho 2 kh\xf4ng gian v\xe9c-t\u01a1 Affine ${L = \\\\boldsymbol{x_{0}} + U}$ v\xe0 ${\\\\tilde{L} = \\\\boldsymbol{\\\\tilde{x_{0}}} + \\\\tilde{U}}$ c\u1ee7a kh\xf4ng gian v\xe9c-t\u01a1 ${V}$. ${L \\\\in \\\\tilde{L}}$ khi v\xe0 ch\u1ec9 khi ${U \\\\in \\\\tilde{U}}$ v\xe0 ${\\\\boldsymbol{x_{0}} - \\\\boldsymbol{\\\\tilde{x_{0}}} \\\\in \\\\tilde{U}}$.\\n\\nCho m\u1ed9t kh\xf4ng gian v\xe9c-t\u01a1 Affine ${k}$ chi\u1ec1u ${L = \\\\boldsymbol{x_{0}} + U}$ c\u1ee7a ${V}$. N\u1ebfu ${(\\\\boldsymbol{b_{1}}, \\\\boldsymbol{b_{2}},..., \\\\boldsymbol{b_{k}})}$ l\xe0 m\u1ed9t c\u01a1 s\u1edf c\u1ee7a ${U}$, th\xec m\u1ed7i ${\\\\boldsymbol{x} \\\\in L}$ c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n duy nh\u1ea5t d\u01b0\u1edbi d\u1ea1ng\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${\\n\\\\boldsymbol{x} = \\\\boldsymbol{x_{0}} + \\\\lambda_{1}\\\\boldsymbol{b_{1}} + \\\\lambda_{2}\\\\boldsymbol{b_{2}} +...+ \\\\lambda_{k}\\\\boldsymbol{b_{k}}\\n}$\\n\\n</p>\\n\\nBi\u1ec3u di\u1ec5n n\xe0y \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 ph\u01b0\u01a1ng tr\xecnh tham s\u1ed1 c\u1ee7a ${L}$ v\u1edbi c\xe1c v\xe9c-t\u01a1 h\u01b0\u1edbng ${\\\\boldsymbol{b_{1}}, \\\\boldsymbol{b_{2}},..., \\\\boldsymbol{b_{k}}}$ v\xe0 c\xe1c tham s\u1ed1 ${\\\\lambda_{1}, \\\\lambda_{2},..., \\\\lambda_{k}}$.\\n\\n#### \xc1nh x\u1ea1 Affine\\nV\u1edbi c\xe1c kh\xf4ng gian v\xe9c-t\u01a1 ${V, W}$, m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ v\xe0 ${\\\\lambda \\\\in W}$, c\xe1c \xe1nh x\u1ea1\\n<p style={{textAlign: \\"center\\"}}> \\n\\n${\\\\phi: V \\\\to W \\\\\\\\\\\\\\\\\\n\\\\boldsymbol{x} \\\\longmapsto \\\\boldsymbol{\\\\lambda} + \\\\Phi(\\\\boldsymbol{x})}$\\n\\n</p>\\n\\nl\xe0 m\u1ed9t \xe1nh x\u1ea1 Affine t\u1eeb ${V}$ \u0111\u1ebfn ${W}$. V\xe9c-t\u01a1 ${\\\\boldsymbol{\\\\lambda}}$ \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 v\xe9c-t\u01a1 d\u1ecbch c\u1ee7a ${\\\\phi}$.\\n- M\u1ed7i \xe1nh x\u1ea1 Affine ${\\\\phi: V \\\\to W}$ l\xe0 m\u1ed9t s\u1ef1 k\u1ebft h\u1ee3p c\u1ee7a m\u1ed9t \xe1nh x\u1ea1 tuy\u1ebfn t\xednh ${\\\\Phi: V \\\\to W}$ v\xe0 m\u1ed9t ph\xe9p d\u1ecbch ${\\\\tau: W \\\\to W}$ trong ${W}$, v\u1edbi ${\\\\phi = \\\\tau \\\\circ \\\\Phi}$. \xc1nh x\u1ea1 ${\\\\Phi}$ v\xe0 ${\\\\tau}$ l\xe0 duy nh\u1ea5t.\\n- S\u1ef1 k\u1ebft h\u1ee3p ${\\\\phi^{\'} \\\\circ \\\\phi}$ c\u1ee7a \xe1nh x\u1ea1 Affine ${\\\\phi: V \\\\to W, \\\\phi^{\'}: W \\\\to X}$ c\u0169ng l\xe0 m\u1ed9t \xe1nh x\u1ea1 Affine.\\n- C\xe1c \xe1nh x\u1ea1 Affine b\u1ea3o to\xe0n c\u1ea5u tr\xfac h\xecnh h\u1ecdc. N\xf3 c\u0169ng b\u1ea3o to\xe0n s\u1ed1 chi\u1ec1u v\xe0 t\xednh ch\u1ea5t song song."},{"id":"probability/","metadata":{"permalink":"/blog/probability/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-07-04-probability/index.md","source":"@site/blog/2021-07-04-probability/index.md","title":"X\xe1c su\u1ea5t th\u1ed1ng k\xea c\u01a1 b\u1ea3n","description":"Probability","date":"2021-07-04T00:00:00.000Z","formattedDate":"July 4, 2021","tags":[{"label":"Probability","permalink":"/blog/tags/probability"},{"label":"Math","permalink":"/blog/tags/math"}],"readingTime":9.58,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"probability/","title":"X\xe1c su\u1ea5t th\u1ed1ng k\xea c\u01a1 b\u1ea3n","authors":"tranlam","tags":["Probability","Math"],"image":"./images/probability.JPEG"},"prevItem":{"title":"\u0110\u1ea1i s\u1ed1 tuy\u1ebfn t\xednh c\u01a1 b\u1ea3n - Ph\u1ea7n 1","permalink":"/blog/linear-algebra-part-1/"},"nextItem":{"title":"AVL Tree, AVL Sorting Algorithm","permalink":"/blog/avl-tree/"}},"content":"![Probability](./images/probability.JPEG)\\n\\nB\xe0i vi\u1ebft n\xe0y nh\u1eb1m \xf4n l\u1ea1i m\u1ed9t s\u1ed1 kh\xe1i ni\u1ec7m trong to\xe1n x\xe1c su\u1ea5t c\u01a1 b\u1ea3n, s\u1ebd kh\xf4ng c\xf3 nh\u1eefng ph\u1ea7n to\xe1n r\u1ea5t ph\u1ee9c t\u1ea1p v\xe0 d\u1ed3n d\u1eadp nh\u01b0 trong l\xfac h\u1ecdc tr\xean tr\u01b0\u1eddng l\u1edbp. Thay v\xe0o \u0111\xf3, n\u1ed9i dung s\u1ebd t\u1eadp trung v\xe0o c\xe1c ki\u1ebfn th\u1ee9c x\xe1c su\u1ea5t ph\u1ee5 tr\u1ee3 cho tr\xed tu\u1ec7 nh\u1eadn t\u1ea1o hay l\xe0 th\u1ed1ng k\xea d\u1eef li\u1ec7u. \x3c!--truncate--\x3eB\xe0i vi\u1ebft n\xe0y tham chi\u1ebfu r\u1ea5t nhi\u1ec1u \u0111\u1ebfn m\u1ed9t b\xe0i trong blog c\u1ee7a anh Ph\u1ea1m \u0110\xecnh Kh\xe1nh, c\u1ea3m \u01a1n anh Kh\xe1nh v\u1edbi m\u1ed9t series x\xe1c su\u1ea5t r\u1ea5t chi ti\u1ebft.\\n\\n### \u0110\xf4i ch\xfat v\u1ec1 \u0111\xe1nh gi\xe1 d\u1eef li\u1ec7u\\nKhi \u0111\xe1nh gi\xe1 d\u1eef li\u1ec7u, ta th\u01b0\u1eddng quan t\xe2m \u0111\u1ebfn c\xe1c ti\xeau ch\xed \u0111\u1ec3 \u0111o \u0111\u1ea1c \u0111\u1ed9 t\u1eadp trung (measure of central tendency) c\u0169ng nh\u01b0 \u0111\u1ed9 ph\xe2n t\xe1n (dispersion) c\u1ee7a d\u1eef li\u1ec7u.\\n\\nV\u1ec1 \u0111\u1ed9 t\u1eadp trung c\u1ee7a d\u1eef li\u1ec7u, c\xf3 3 th\xf4ng s\u1ed1 c\u1ea7n quan t\xe2m\\n\\n- K\u1ef3 v\u1ecdng (mean): \u0111\xe1nh gi\xe1 gi\xe1 tr\u1ecb trung b\xecnh c\u1ee7a m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean.\\n    - N\u1ebfu ${x}$ l\xe0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean r\u1eddi r\u1ea1c, gi\xe1 tr\u1ecb k\u1ef3 v\u1ecdng \u0111\u01b0\u1ee3c t\xednh nh\u01b0 sau\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${E(x) = \\\\sum_{i=1}^{n}x_ip(x_i) }$\\n\\n    </p>        \\n\\n    Trong \u0111\xf3, ${p(x_i)}$ l\xe0 x\xe1c su\u1ea5t khi m\xe0 bi\u1ebfn ng\u1eabu nhi\xean ${x}$ nh\u1eadn gi\xe1 tr\u1ecb ${x_i}$.\\n\\n    - N\u1ebfu ${x}$ l\xe0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean li\xean t\u1ee5c\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${E(x) = \\\\int xp(x)dx }$\\n\\n    </p>\\n\\n    T\u1eeb \u0111\xf3, ta c\xf3 m\u1ed9t s\u1ed1 t\xednh ch\u1ea5t c\u1ee7a k\u1ef3 v\u1ecdng nh\u01b0 sau\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${E(ax + by) = aE(x) + bE(y) }$\\n    \\n    </p>\\n    <p style={{textAlign: \\"center\\"}}>\\n    \\n    ${E(xy) = E(x)E(y)}$, n\u1ebfu hai bi\u1ebfn ${x}$ v\xe0 ${y}$ \u0111\u1ed9c l\u1eadp\\n    \\n    </p>\\n\\n- Trung v\u1ecb (median): l\xe0 gi\xe1 tr\u1ecb c\u1ee7a ph\u1ea7n t\u1eed n\u1eb1m \u1edf ch\xednh gi\u1eefa m\u1ed9t d\xe3y gi\xe1 tr\u1ecb \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp theo th\u1ee9 t\u1ef1. Tr\u01b0\u1eddng h\u1ee3p s\u1ed1 ph\u1ea7n t\u1eed l\xe0 ch\u1eb5n, gi\xe1 tr\u1ecb trung v\u1ecb b\u1eb1ng trung b\xecnh c\u1ed9ng 2 ph\u1ea7n t\u1eed \u1edf gi\u1eefa.\\n\\n- Mode: gi\xe1 tr\u1ecb ph\u1ea7n t\u1eed c\xf3 t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong d\xe3y gi\xe1 tr\u1ecb.\\n\\nV\xed d\u1ee5, ta c\xf3 d\xe3y gi\xe1 tr\u1ecb c\u1ee7a m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean qua c\xe1c s\u1ef1 ki\u1ec7n l\xe0 ${\\\\{1, 1, 2, 4, 6, 6, 6, 14\\\\}}$ v\u1edbi c\xe1c s\u1ef1 ki\u1ec7n c\xf3 x\xe1c su\u1ea5t x\u1ea3y ra nh\u01b0 nhau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${Mean = \\\\frac{1+1+2+4+6+6+6+14}{8} = 5}$, ${Median = \\\\frac{4+6}{2} = 5}$, ${Mode = 6}$       \\n\\n</p>\\n\\nN\u1ebfu th\xeam m\u1ed9t gi\xe1 tr\u1ecb n\u1eefa v\xe0o d\xe3y ${\\\\{1, 1, 2, 4, 6, 6, 6, 14, 500\\\\}}$\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${Mean = \\\\frac{1+1+2+4+6+6+6+14+500}{9} = 60}$, ${Median = 6}$, ${Mode = 6}$\\n\\n</p>\\nTa nh\u1eadn th\u1ea5y r\u1eb1ng, gi\xe1 tr\u1ecb mean ch\u1ecbu \u1ea3nh h\u01b0\u1edfng n\u1eb7ng b\u1edfi c\xe1c outliers h\u01a1n l\xe0 median (c\xe1c outliers s\u1ebd \u0111\u01b0\u1ee3c gi\u1ea3i th\xedch \u1edf d\u01b0\u1edbi).\\n\\nV\u1ec1 \u0111\u1ed9 ph\xe2n t\xe1n c\u1ee7a d\u1eef li\u1ec7u, ta quan t\xe2m \u0111\u1ebfn c\xe1c \u0111\u1ea1i l\u01b0\u1ee3ng sau\\n\\n- Ph\u01b0\u01a1ng sai: th\u1ec3 hi\u1ec7n m\u1ee9c \u0111\u1ed9 bi\u1ebfn \u0111\u1ed9ng c\u1ee7a \u0111\u1ea1i l\u01b0\u1ee3ng xung quanh gi\xe1 tr\u1ecb k\u1ef3 v\u1ecdng.\\n\\n    - N\u1ebfu ${x}$ l\xe0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean r\u1eddi r\u1ea1c\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${Var(x) = \\\\sum_{i=1}^{n}(x_i - E(x))^2p(x_i) }$\\n\\n    </p>\\n\\n    - N\u1ebfu ${x}$ l\xe0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean li\xean t\u1ee5c\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${Var(x) = \\\\int(x - E(x))^2p(x)dx }$\\n\\n    </p>\\n\\n- \u0110\u1ed9 l\u1ec7ch chu\u1ea9n: b\u1eb1ng c\u0103n b\u1eadc hai c\u1ee7a ph\u01b0\u01a1ng sai\\n    <p style={{textAlign: \\"center\\"}}>\\n\\n    ${\\\\sigma_x = \\\\sqrt{Var(x)} }$\\n\\n    </p>\\n\\n    C\xe1c outliers l\xe0 nh\u1eefng gi\xe1 tr\u1ecb n\u1eb1m ngo\xe0i kho\u1ea3ng ${[\\\\mu - 3\\\\sigma_x, \\\\mu + 3\\\\sigma_x]}$ v\u1edbi ${\\\\mu = E(x)}$. Trong ch\u1ecdn l\u1ecdc c\xe1c \u0111\u1eb7c tr\u01b0ng t\u1ed1t cho h\u1ecdc m\xe1y, c\xe1c outliers th\u01b0\u1eddng b\u1ecb l\u1ecdc b\u1ecf b\u1edfi n\xf3 c\xf3 nguy c\u01a1 l\xe0 nh\u1eefng \u0111i\u1ec3m g\xe2y nhi\u1ec5u cao.\\n\\nT\xecm ra c\xe1c \u0111\u1eb7c tr\u01b0ng c\u1ee7a t\u1eadp d\u1eef li\u1ec7u, ng\u01b0\u1eddi ta c\xf2n hay quan t\xe2m \u0111\u1ebfn c\xe1c th\xf4ng s\u1ed1 tr\xean \u0111\u1ed3 th\u1ecb box plot\\n![Box Plot](./images/boxplot.PNG)\\n\\n\u1ede h\xecnh tr\xean, median l\xe0 gi\xe1 tr\u1ecb trung v\u1ecb, ${Q1}$ v\xe0 ${Q3}$ l\u1ea7n l\u01b0\u1ee3t l\xe0 c\xe1c gi\xe1 tr\u1ecb trung v\u1ecb ${25\\\\%}$ v\xe0 ${75\\\\%}$, \u0111\u1ed9 tr\u1ea3i gi\u1eefa (${IQR)}$ l\xe0 kho\u1ea3ng c\xe1c gi\u1eefa ch\xfang. C\xe1c \u0111i\u1ec3m n\u1eb1m ngo\xe0i kho\u1ea3ng ${[Q1 - 1.5IQR, Q3 + 1.5IQR]}$ \u0111\u01b0\u1ee3c coi l\xe0 c\xe1c mild outliers, n\u1eb1m ngo\xe0i kho\u1ea3ng ${[Q1 - 3IQR, Q3 + 3IQR]}$ \u0111\u01b0\u1ee3c coi l\xe0 c\xe1c extreme outliers. C\xe1c outliers th\u01b0\u1eddng \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf kh\u1ecfi d\u1eef li\u1ec7u khi m\xe0 ta mu\u1ed1n ph\xe2n t\xedch th\xf4ng tin g\xec \u0111\xf3 t\u1eeb d\u1eef li\u1ec7u.\\n\\n\u0110\u1ec3 \u0111\xe1nh gi\xe1 t\u01b0\u01a1ng quan tuy\u1ebfn t\xednh gi\u1eefa 2 bi\u1ebfn ng\u1eabu nhi\xean, ta quan t\xe2m \u0111\u1ebfn \u0111\u1ea1i l\u01b0\u1ee3ng h\u1ec7 s\u1ed1 t\u01b0\u01a1ng quan. Gi\xe1 tr\u1ecb c\u1ee7a h\u1ec7 s\u1ed1 t\u01b0\u01a1ng quan th\u01b0\u1eddng tr\u1ea3i t\u1eeb kho\u1ea3ng ${[-1, 1]}$. C\xf4ng th\u1ee9c t\xednh h\u1ec7 s\u1ed1 t\u01b0\u01a1ng quan nh\u01b0 sau\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${\\\\rho_{xy} = \\\\frac{cov(x, y)}{\\\\sigma_x\\\\sigma_y}}$, v\u1edbi ${cov(x, y) = E[(x - \\\\mu_x)(y - \\\\mu_y)] = \\\\frac{\\\\sum_{i = 1}^{n}(x_i - \\\\mu_x)(y_i - \\\\mu_y)}{n}}$, ${\\\\sigma}$ v\xe0 ${\\\\mu}$ l\u1ea7n l\u01b0\u1ee3t l\xe0 \u0111\u1ed9 l\u1ec7ch chu\u1ea9n v\xe0 k\u1ef3 v\u1ecdng c\u1ee7a bi\u1ebfn ng\u1eabu nhi\xean.\\n</p>\\n\\nTa c\xf3 m\u1ed9t v\xe0i nh\u1eadn x\xe9t nh\u01b0 sau\\n- Khi ${\\\\rho_{xy} = 0}$, hai bi\u1ebfn ng\u1eabu nhi\xean ho\xe0n to\xe0n \u0111\u1ed9c l\u1eadp tuy\u1ebfn t\xednh v\u1edbi nhau.\\n- Khi ${\\\\rho_{xy} > 0}$, ch\xfang \u0111\u1ed3ng bi\u1ebfn, bi\u1ebfn \u0111\u1ea7u ti\xean t\u0103ng th\xec bi\u1ebfn th\u1ee9 hai c\u0169ng t\u0103ng.\\n- Khi ${\\\\rho_{xy} < 0}$, ch\xfang ngh\u1ecbch bi\u1ebfn, bi\u1ebfn \u0111\u1ea7u ti\xean t\u0103ng th\xec bi\u1ebfn th\u1ee9 hai gi\u1ea3m.\\n- Khi ${\\\\rho_{xy}}$ b\u1eb1ng ${-1}$ ho\u1eb7c ${1}$, hai bi\u1ebfn ho\xe0n to\xe0n t\u01b0\u01a1ng quan tuy\u1ebfn t\xednh. \\n\\n### C\xe1c ki\u1ec3u d\u1eef li\u1ec7u\\nC\xf3 nhi\u1ec1u lo\u1ea1i ki\u1ec3u d\u1eef li\u1ec7u m\xe0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\xean c\xf3 th\u1ec3 nh\u1eadn, t\xf9y v\xe0o b\xe0i to\xe1n nh\u1ea5t \u0111\u1ecbnh, ta c\xf3 th\u1ec3 gi\u1eef nguy\xean gi\xe1 tr\u1ecb c\xe1c bi\u1ebfn trong t\xednh to\xe1n, ho\u1eb7c m\xe3 h\xf3a n\xf3.\\n- Ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh t\xednh (quanlitative data): l\xe0 c\xe1c ki\u1ec3u d\u1eef li\u1ec7u mang th\xf4ng tin m\xf4 t\u1ea3 \u0111\u1eb7c tr\u01b0ng c\u1ee7a s\u1ef1 v\u1eadt, hi\u1ec7n t\u01b0\u1ee3ng.\\n    - C\xe1c ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh t\xednh kh\xf4ng c\xf3 th\u1ee9 t\u1ef1 (nominal data): m\xe0u s\u1eafc, gi\u1edbi t\xednh,... V\u1edbi lo\u1ea1i d\u1eef li\u1ec7u n\xe0y, ta th\u01b0\u1eddng d\xf9ng mode \u0111\u1ec3 \u0111\xe1nh gi\xe1 ch\xfang. V\xed d\u1ee5 nh\u01b0: \u0111\xe1nh gi\xe1 m\xe0u xe con n\xe0o xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t,...\\n    - C\xe1c ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh t\xednh c\xf3 th\u1ee9 t\u1ef1 (ordinal data): c\xe1c m\u1ee9c \u0111\u1ed9, c\xe1c c\u1ea5p h\u1ecdc nh\u01b0 ti\u1ec3u h\u1ecdc, trung h\u1ecdc c\u01a1 s\u1edf, trung h\u1ecdc ph\u1ed5 th\xf4ng,... V\u1edbi lo\u1ea1i d\u1eef li\u1ec7u n\xe0y, ta quan t\xe2m nhi\u1ec1u h\u01a1n \u0111\u1ebfn th\xf4ng s\u1ed1 median. V\xed d\u1ee5 nh\u01b0: \u0111\xe1nh g\xeda m\u1ee9c \u0111\u1ed9 trung b\xecnh trong t\u1eadp d\u1eef li\u1ec7u,...\\n- Ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh l\u01b0\u1ee3ng (quantitative data): l\xe0 c\xe1c th\xf4ng tin m\xf4 t\u1ea3 \u0111\u1ed1i t\u01b0\u1ee3ng d\u01b0\u1edbi d\u1ea1ng con s\u1ed1. V\u1edbi lo\u1ea1i d\u1eef li\u1ec7u n\xe0y, ta d\xf9ng c\xe1c gi\xe1 tr\u1ecb mean v\xe0 median \u0111\u1ec3 \u0111\xe1nh gi\xe1 ch\xfang.\\n    - D\u1eef li\u1ec7u li\xean t\u1ee5c (continous data): chi\u1ec1u cao, c\xe2n n\u1eb7ng,...\\n    - D\u1eef li\u1ec7u r\u1eddi r\u1ea1c (discrete data): s\u1ed1 chi\u1ebfc \xe1o, s\u1ed1 b\xe1nh xe,...\\n- Ki\u1ec3u d\u1eef li\u1ec7u k\xe8m theo y\u1ebfu t\u1ed1 th\u1eddi gian (temporal data): v\xed d\u1ee5 nh\u01b0 gi\xe1 c\u1ed5 phi\u1ebfu, khi b\xe1o c\xe1o c\u1ea7n ph\u1ea3i \u0111i k\xe8m v\u1edbi th\xf4ng tin l\xe0 ng\xe0y n\xe0o, th\u1eddi \u0111i\u1ec3m n\xe0o,...\\n- Ki\u1ec3u d\u1eef li\u1ec7u k\xe8m theo y\u1ebfu t\u1ed1 kh\xf4ng gian (spartial data): v\xed d\u1ee5 nh\u01b0 v\u1ecb tr\xed \u0111\u1ecba l\xfd tr\xean b\u1ea3n \u0111\u1ed3, gi\xe1 tr\u1ecb t\u1ecda \u0111\u1ed9 c\u1ee7a m\u1ed9t \u0111i\u1ec3m tr\xean tr\u1ee5c t\u1ecda \u0111\u1ed9,...\\n\\n### C\xe1c h\xe0m x\xe1c su\u1ea5t\\nC\xe1c h\xe0m x\xe1c su\u1ea5t c\xe1c b\u1ea1n c\xf3 th\u1ec3 xem **[t\u1ea1i \u0111\xe2y](https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#ham-mat-do-pdf-va-ham-khoi-xac-suat-pmf)**, c\u1ea3m \u01a1n anh Ph\u1ea1m \u0110\xecnh Kh\xe1nh \u0111\xe3 vi\u1ebft r\u1ea5t chi ti\u1ebft v\u1ec1 c\xe1c h\xe0m n\xe0y.\\n\\n### C\xe1c lo\u1ea1i bi\u1ec3u \u0111\u1ed3 th\u1ed1ng k\xea\\nT\xf9y theo m\u1ee5c \u0111\xedch th\xf4ng tin ta c\u1ea7n tr\xedch xu\u1ea5t t\u1eeb d\u1eef li\u1ec7u m\xe0 ta s\u1ebd s\u1eed d\u1ee5ng c\xe1c lo\u1ea1i bi\u1ec3u \u0111\u1ed3 ph\xf9 h\u1ee3p cho b\xe0i to\xe1n\\n<table id=\\"t01\\">\\n  <tr>\\n    <th>M\u1ee5c \u0111\xedch</th>\\n    <th>Bi\u1ec3u \u0111\u1ed3</th>\\n  </tr>\\n  <tr>\\n    <th colspan=\\"2\\">C\u1ea7n nh\xecn b\u1ee9c tranh t\u1ed5ng th\u1ec3 c\u1ee7a d\u1eef li\u1ec7u</th>\\n  </tr>\\n  <tr>\\n    <td>Bi\u1ec3u di\u1ec5n ph\xe2n b\u1ed1 d\u1eef li\u1ec7u, v\u1edbi tr\u1ee5c ho\xe0nh l\xe0 c\xe1c kho\u1ea3ng gi\xe1 tr\u1ecb bi\u1ebfn ng\u1eabu nhi\xean v\xe0 tr\u1ee5c tung l\xe0 gi\xe1 tr\u1ecb t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n c\u1ee7a ch\xfang</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Histogram plot\\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/histogramplot.PNG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>\\n    Bi\u1ec3u di\u1ec5n c\xe1c gi\xe1 tr\u1ecb quan tr\u1ecdng nh\u01b0 min, max, trung v\u1ecb 25%, 50%, 75%, \u0111\u1ed9 tr\u1ea3i gi\u1eefa IQR,... T\u1eeb \u0111\xf3, ta \u0111\xe1nh gi\xe1 \u0111\u01b0\u1ee3c c\xe1c \u0111i\u1ec3m chia c\u1eaft d\u1eef li\u1ec7u hay c\xe1c \u0111i\u1ec3m d\u1eef li\u1ec7u g\xe2y nhi\u1ec5u,...</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Box plot \\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/boxplot.PNG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>Bi\u1ec3u di\u1ec5n d\u1eef li\u1ec7u theo th\u1eddi gian</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Time series plot\\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/timeseriesplot.PNG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <th colspan=\\"2\\">C\u1ea7n bi\u1ec3u di\u1ec5n s\u1ef1 so s\xe1nh gi\u1eefa c\xe1c bi\u1ebfn</th>\\n  </tr>\\n  <tr>\\n    <td>So s\xe1nh gi\xe1 tr\u1ecb c\u1ee7a nhi\u1ec1u bi\u1ebfn</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Bar chart\\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/barchart.PNG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>So s\xe1nh gi\xe1 tr\u1ecb m\u1ed9t bi\u1ebfn thay \u0111\u1ed5i theo th\u1eddi gian</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Line chart, gi\u1ed1ng v\u1edbi timeseriesplot\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>So s\xe1nh nhi\u1ec1u nh\xf3m</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Radar chart\\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/radarchart.PNG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n  <tr>\\n    <th colspan=\\"2\\">C\u1ea7n nh\u1eadn bi\u1ebft s\u1ef1 t\u01b0\u01a1ng quan gi\u1eefa hai ho\u1eb7c nhi\u1ec1u bi\u1ebfn</th>\\n  </tr>\\n  <tr>\\n    <td>T\u01b0\u01a1ng quan gi\u1eefa hai ho\u1eb7c nhi\u1ec1u bi\u1ebfn</td>\\n    <td>\\n        <p style={{textAlign: \\"center\\"}}>\\n            Scatter plot\\n        <img style={{width: \\"90%\\"}} src={require(\\"./images/scatterplot.JPG\\").default} />\\n        </p>\\n    </td>\\n  </tr>\\n</table> \\n\\n### \u0110\u1ed9 l\u1ec7ch d\u01b0\u01a1ng, \u0111\u1ed9 l\u1ec7ch \xe2m\\n- \u0110\u1ed9 l\u1ec7ch d\u01b0\u01a1ng c\xf3 gi\xe1 tr\u1ecb trung b\xecnh (mean) l\u1edbn h\u01a1n gi\xe1 tr\u1ecb trung v\u1ecb (median) v\xe0 ph\xe2n b\u1ed1 c\xf3 ph\xeda kh\xf4ng \u0111\u1ed1i x\u1ee9ng m\u1edf r\u1ed9ng \u0111\u1ebfn nhi\u1ec1u c\xe1c gi\xe1 tr\u1ecb d\u01b0\u01a1ng h\u01a1n.\\n\\n![Positive Skewness](./images/positiveskewness.PNG)\\n\\n- \u0110\u1ed9 l\u1ec7ch \xe2m c\xf3 gi\xe1 tr\u1ecb trung b\xecnh (mean) nh\u1ecf h\u01a1n gi\xe1 tr\u1ecb trung v\u1ecb (median) v\xe0 ph\xe2n b\u1ed1 c\xf3 ph\xeda kh\xf4ng \u0111\u1ed1i x\u1ee9ng m\u1edf r\u1ed9ng \u0111\u1ebfn nhi\u1ec1u c\xe1c gi\xe1 tr\u1ecb \xe2m h\u01a1n.\\n\\n![Negative Skewness](./images/negativeskewness.JPG)\\n\\n\u0110\u1ed9 l\u1ec7ch k\u1ebft h\u1ee3p v\u1edbi \u0111\u1ed9 nh\u1ecdn \u0111\xe1nh gi\xe1 kh\u1ea3 n\u0103ng c\xe1c bi\u1ebfn c\u1ed1 r\u01a1i v\xe0o \u0111u\xf4i c\u1ee7a ph\xe2n ph\u1ed1i x\xe1c su\u1ea5t t\u1ed1t h\u01a1n.\\n\\n### Quy lu\u1eadt s\u1ed1 l\u1edbn\\nPh\u1ea7n n\xe0y c\xe1c b\u1ea1n h\xe3y tham kh\u1ea3o **[t\u1ea1i \u0111\xe2y](https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#qui-luat-so-lon)** c\u1ee7a anh Ph\u1ea1m \u0110\xecnh Kh\xe1nh.\\n\\n- Kho\u1ea3ng tin c\u1eady (confidence interval)\\n\\nKhi m\xecnh l\u1ea5y m\u1eabu t\u1eeb m\u1ed9t qu\u1ea7n th\u1ec3, c\xe1c tham s\u1ed1 c\u1ee7a m\u1ed9t qu\u1ea7n th\u1ec3 c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c \u0111\xe1nh gi\xe1 b\u1eb1ng m\u1eabu \u0111\xf3. Th\xf4ng th\u01b0\u1eddng, gi\xe1 tr\u1ecb k\u1ef3 v\u1ecdng c\u1ee7a m\u1eabu s\u1ebd \u0111\u01b0\u1ee3c \u01b0\u1edbc l\u01b0\u1ee3ng b\u1eb1ng gi\xe1 tr\u1ecb k\u1ef3 v\u1ecdng c\u1ee7a qu\u1ea7n th\u1ec3, tuy nhi\xean, nh\u1eefng l\u1ea7n l\u1ea5y m\u1eabu kh\xe1c nhau th\xec c\xf3 k\u1ef3 v\u1ecdng kh\xe1c nhau. Do v\u1eady, ta \u0111\u01b0a ra \u0111\u1ecbnh ngh\u0129a kho\u1ea3ng tin c\u1eady d\xf9ng \u0111\u1ec3 bi\u1ebfn c\xe1i tham s\u1ed1 k\u1ef3 v\u1ecdng \u0111\u01b0\u1ee3c \u01b0\u1edbc l\u01b0\u1ee3ng tr\u1edf th\xe0nh m\u1ed9t kho\u1ea3ng gi\xe1 tr\u1ecb \u01b0\u1edbc l\u01b0\u1ee3ng, \u0111\u1ea3m b\u1ea3o r\u1eb1ng k\u1ef3 v\u1ecdng c\u1ee7a qu\u1ea7n th\u1ec3 s\u1ebd n\u1eb1m trong kho\u1ea3ng gi\xe1 tr\u1ecb \u0111\xf3.\\n\\n${k\\\\%}$ m\u1ee9c tin c\u1eady s\u1ebd \u0111\u1ea3m b\u1ea3o ph\u1ee7 \u0111\u01b0\u1ee3c tham s\u1ed1 k\u1ef3 v\u1ecdng c\u1ea7n \u01b0\u1edbc l\u01b0\u1ee3ng c\u1ee7a qu\u1ea7n th\u1ec3 v\u1edbi x\xe1c su\u1ea5t l\xe0 ${k\\\\%}$.\\n\\nKho\u1ea3ng tin c\u1eady c\xe0ng nh\u1ecf th\xec c\xe0ng \u0111\u1ea3m b\u1ea3o \u0111\u01b0\u1ee3c \u0111\u1ed9 ch\xednh x\xe1c c\u1ee7a b\u1ed9 \u01b0\u1edbc l\u01b0\u1ee3ng, \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\xe0y, ta c\xf3 th\u1ec3 gi\u1ea3m m\u1ee9c tin c\u1eady ho\u1eb7c t\u0103ng k\xedch th\u01b0\u1edbc c\u1ee7a m\u1eabu l\u1ea5y t\u1eeb qu\u1ea7n th\u1ec3. Kho\u1ea3ng tin c\u1eady \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1edfi ${(lower bound, upper bound)}$ v\u1edbi\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${lower bound = \\\\bar{x} - z\\\\frac{\\\\sigma}{\\\\sqrt{n}}}$\\n\\n${upper bound = \\\\bar{x} + z\\\\frac{\\\\sigma}{\\\\sqrt{n}}}$\\n\\n</p>\\n\\nV\u1edbi ${\\\\bar{x}}$ l\xe0 gi\xe1 tr\u1ecb k\u1ef3 v\u1ecdng c\u1ee7a m\u1eabu \u0111\u01b0\u1ee3c l\u1ea5y, ${\\\\sigma}$ l\xe0 gi\xe1 tr\u1ecb \u0111\u1ed9 l\u1ec7ch chu\u1ea9n gi\u1ea3 \u0111\u1ecbnh c\u1ee7a qu\u1ea7n th\u1ec3. ${n}$ l\xe0 k\xedch th\u01b0\u1edbc c\u1ee7a m\u1eabu v\xe0 ${z}$ l\xe0 h\u1ec7 s\u1ed1 tin c\u1eady.\\n\\nV\u1edbi ph\xe2n ph\u1ed1i chu\u1ea9n\\n- ${z = 1.645}$ \u1ee9ng v\u1edbi m\u1ee9c tin c\u1eady ${90\\\\%}$.\\n- ${z = 1.96}$ \u1ee9ng v\u1edbi m\u1ee9c tin c\u1eady ${95\\\\%}$.\\n- ${z = 2.576}$ \u1ee9ng v\u1edbi m\u1ee9c tin c\u1eady ${99\\\\%}$.\\n\\n### C\xe1c ph\xe2n ph\u1ed1i x\xe1c su\u1ea5t\\nC\xe1c ph\xe2n ph\u1ed1i x\xe1c su\u1ea5t c\xe1c b\u1ea1n c\xf3 th\u1ec3 xem **[t\u1ea1i \u0111\xe2y](https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#phan-phoi-xac-suat)**.\\n\\n### X\xe1c su\u1ea5t \u0111\u1ed3ng th\u1eddi, x\xe1c su\u1ea5t bi\xean, x\xe1c su\u1ea5t c\xf3 \u0111i\u1ec1u ki\u1ec7n v\xe0 \u0111\u1ecbnh l\u1ef9 Bayes\\nC\xe1c b\u1ea1n c\xf3 th\u1ec3 t\xecm th\u1ea5y th\xf4ng tin m\u1ee5c n\xe0y **[t\u1ea1i \u0111\xe2y](https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#xac-suat-dong-thoi-join-distribution)**, m\u1ed9t l\u1ea7n n\u1eefa c\u1ea3m \u01a1n anh Ph\u1ea1m \u0110\xecnh Kh\xe1nh \u0111\xe3 tr\xecnh b\xe0y chi ti\u1ebft v\u1ec1 ph\u1ea7n n\xe0y.\\n\\n### T\xe0i li\u1ec7u tham kh\u1ea3o\\n\\n[https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#phan-phoi-xac-suat](https://phamdinhkhanh.github.io/deepai-book/ch_probability/appendix_probability.html#)"},{"id":"avl-tree/","metadata":{"permalink":"/blog/avl-tree/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-02-24-avl-tree/index.md","source":"@site/blog/2021-02-24-avl-tree/index.md","title":"AVL Tree, AVL Sorting Algorithm","description":"AVL Tree, AVL Sorting Algorithm","date":"2021-02-24T00:00:00.000Z","formattedDate":"February 24, 2021","tags":[{"label":"tree","permalink":"/blog/tags/tree"},{"label":"avl","permalink":"/blog/tags/avl"},{"label":"binary tree","permalink":"/blog/tags/binary-tree"},{"label":"algorithms","permalink":"/blog/tags/algorithms"}],"readingTime":7.165,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"avl-tree/","title":"AVL Tree, AVL Sorting Algorithm","description":"AVL Tree, AVL Sorting Algorithm","authors":"tranlam","tags":["tree","avl","binary tree","algorithms"],"image":"./images/intuition.PNG"},"prevItem":{"title":"X\xe1c su\u1ea5t th\u1ed1ng k\xea c\u01a1 b\u1ea3n","permalink":"/blog/probability/"},"nextItem":{"title":"Binary Search Tree","permalink":"/blog/binarysearch-tree/"}},"content":"![Intuition](./images/intuition.PNG)\\n\\nIn the previous post, I talked about the **[binary search tree](/blog/2021-02-22-binarysearch-tree/index.md)**. With efficiency in search, insert, delete,... binary search tree can be done in logrithmic time (${\\\\Theta(logn)}$) in the average case. In this article, I will talk about AVL tree, which is a type of binary search tree, ensuring that in all cases, the time complexity of the above operations is the same ${\\\\Theta(logn)}$.\\n\\n\x3c!--truncate--\x3e\\n\\n## AVL tree\\n\\nAn AVL tree is a balanced binary search tree in which the heights of the left and right subtrees differ by at most 1 ${(1)}$. In the process of performing operations on the tree that make the tree unbalanced, we need to rebalance the tree to ensure the nature of the tree ${(1)}$.\\n\\n## Tree height assessment\\n\\nThe height of the tree\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${h}$ = max{ left subtree height, right subtree height } + 1\\n\\n</p>\\n\\nThe heights of the nodes are numbered as shown above.\\n\\nWith the property ${(1)}$, the worst case of an AVL tree occurs when the right subtree is 1 unit taller than the left subtree for all nodes (or vice versa).\\n\\nFor ${N{_h}}$ is the smallest number of nodes in a tree of height ${h}$.\\n\\n![Height](./images/height.PNG)\\n\\nWith the diagram above, we have\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${N{_ {O(1)}} = O(1)}$ v\xe0 ${N{_ h} = 1 + N{_ {h-1}} + N{_{h-2}}}$\\n\\n</p>\\n\\n### First approach\\n\\nThe above expression reminds us of the fibonacci sequence, we have ${N{_h} > F{_h}}$ v\u1edbi ${F{_h}}$ is the ${h^{th}}$ fibonacci. We have ${F{_h} = \\\\frac{\\\\gamma^h}{\\\\sqrt{5}}}$, with ${\\\\gamma = 1.61803398875...}$, (golden ratio).\\n\\nWith ${N{_h} = n}$ (the number of nodes in the tree).\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${ n > \\\\frac{\\\\gamma^h}{\\\\sqrt{5}} => h < log{_\\\\gamma}n => h < 1.440log{_2}n }$.\\n\\n</p>\\n\\n### Second approach\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${=> N{_ h} = 1 + N{_ {h-1}} + N{_{h-2}}}$\\n\\n${=> N{_ h} > 1 + 2N{_{h-2}}}$\\n\\n${=> N{_ h} > 2N{_{h-2}}}$\\n\\n${=> h < 2log{_2}n}$\\n\\n</p>\\n\\nTherefore, ${h = O(logn)}$.\\n\\n## AVL tree operations\\n\\n### Node insertion, node deletion, search node\\n\\nThe above operations in an AVL tree are the same as in a binary search tree, with ${h = O(logn)}$. The difference in node insertion and deletion is that after performing those operations, we need to do an extra step of tree balancing to ensure the integrity of the tree ${(1)}$.\\n\\n### Tree balancing activities\\n\\nThe following example demonstrates the need for tree rebalancing\\n\\nSuppose we have the following tree\\n\\n![Insert](./images/insert.JPG)\\n\\nWe want to insert ${23}$ into the tree, perform the insertion as in a binary search tree. After inserting, we see the property ${(1)}$ violated. We need to rebalance the tree to get\\n\\n![Insert2](./images/insert_2.JPG)\\n\\n#### Balance factor\\n\\nIn a binary tree, the balance factor is defined as follows:\\n\\n${BF(x)}$ = height of left subtree if ${x}$ ${-}$ height of right subtree of ${x}$.\\n\\nThus, in the AVL tree, we have ${BF(x) \\\\in \\\\\\\\{-1, 0, 1\\\\\\\\} }$.\\n\\n#### Tree balancing operations\\n\\nWhen the BF of a certain node has a value that is not in the above set of values, then we need to re-balancing the tree. We have two basic types of balancing operations of AVL trees: **right rotation** and **left rotation**\\n\\n![Rotation](./images/rotation.PNG)\\n\\n### Tree balancing in specific cases\\n\\n#### Left left case\\n\\nThis case occurs when a node has BF = 2 and its left subtree has BF = 1. Then, we only need 1 **right rotation** at the node to be considered, then the tree is balanced.\\n\\n![Left Left](./images/left_left.PNG)\\n\\n#### Left right case\\n\\nThis case occurs when a node has BF = 2 and its left subtree has BF = -1. Then, we need to do the following 2 steps in turn\\n\\n- **Left rotation** left subtree.\\n- **Right rotation** the node to be considered.\\n\\n![Left Right](./images/left_right.PNG)\\n\\n#### Right right case\\n\\nThis case occurs when a node has BF = -2 and its right subtree has BF = -1. Then, we only need 1 **left rotation** at the node to be considered, then the tree is balanced.\\n\\n![Right Right](./images/right_right.PNG)\\n\\n#### Right left case\\n\\nThis case occurs when a node has BF = -2 and its left subtree has BF = 1. Then, we need to do the following 2 steps in turn\\n\\n- **Right rotation** right subtree.\\n- **Left rotation** the node to be considered.\\n\\n![Right Left](./images/right_left.PNG)\\n\\n## Python code for tree balancing activities\\n\\n```python\\n# ---------------------------METHOD TO HELP BALANCE THE TREE---------------------------\\n#      y                               x\\n#     / \\\\     Right Rotation          /  \\\\\\n#    x   T3   - - - - - - - >        T1   y\\n#   / \\\\       < - - - - - - -            / \\\\\\n#  T1  T2     Left Rotation            T2  T3\\n\\n# ---------------------------BALANCE THE TREE IN PARTICULAR CASES---------------------------\\n# -----Left Left Case\\n#          z                                      y\\n#         / \\\\                                   /   \\\\\\n#        y   T4      Right Rotate (z)          x      z\\n#       / \\\\          - - - - - - - - ->      /  \\\\    /  \\\\\\n#      x   T3                               T1  T2  T3  T4\\n#     / \\\\\\n#   T1   T2\\n# -----Left Right Case\\n#      z                               z                           x\\n#     / \\\\                            /   \\\\                        /  \\\\\\n#    y   T4  Left Rotate (y)        x    T4  Right Rotate(z)    y      z\\n#   / \\\\      - - - - - - - - ->    /  \\\\      - - - - - - - ->  / \\\\    / \\\\\\n# T1   x                          y    T3                    T1  T2 T3  T4\\n#     / \\\\                        / \\\\\\n#   T2   T3                    T1   T2\\n# -----Right Right Case\\n#   z                                y\\n#  /  \\\\                            /   \\\\\\n# T1   y     Left Rotate(z)       z      x\\n#     /  \\\\   - - - - - - - ->    / \\\\    / \\\\\\n#    T2   x                     T1  T2 T3  T4\\n#        / \\\\\\n#      T3  T4\\n# -----Right Left Case\\n#    z                            z                            x\\n#   / \\\\                          / \\\\                          /  \\\\\\n# T1   y   Right Rotate (y)    T1   x      Left Rotate(z)   z      y\\n#     / \\\\  - - - - - - - - ->     /  \\\\   - - - - - - - ->  / \\\\    / \\\\\\n#    x   T4                      T2   y                  T1  T2  T3  T4\\n#   / \\\\                              /  \\\\\\n# T2   T3                           T3   T4\\n\\nclass AVLNode:\\n    def __init__(self, val):\\n        self.left = None\\n        self.right = None\\n        self.val = val\\n        self.height = 1\\n\\n\\nclass AVLTree:\\n    def insert(self, root, key):\\n        # perform bst\\n        if not root:\\n            return AVLNode(key)\\n        if root.val < key:\\n            root.right = self.insert(root.right, key)\\n        if root.val > key:\\n            root.left = self.insert(root.left, key)\\n        # update the height of the ancestor node\\n        root.height = 1 + max(self.get_height(root.left),\\n                              self.get_height(root.right))\\n\\n        # get balance factor\\n        balance = self.get_balance(root)\\n\\n        # perform balance\\n        # left left\\n        if balance > 1 and key < root.left.val:\\n            return self.right_rotate(root)\\n        # right right\\n        if balance < -1 and key > root.right.val:\\n            return self.left_rotate(root)\\n        # left right\\n        if balance > 1 and key > root.left.val:\\n            root.left = self.left_rotate(root.left)\\n            return self.right_rotate(root)\\n        # right left\\n        if balance < -1 and key < root.left.val:\\n            root.right = self.right_rotate(root.right)\\n            return self.left_rotate(root)\\n\\n        return root\\n\\n    def left_rotate(self, x):\\n        y = x.right\\n        T2 = y.left\\n\\n        y.left = x\\n        x.right = T2\\n\\n        x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))\\n        y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))\\n\\n        return y\\n\\n    def right_rotate(self, y):\\n        x = y.left\\n        T2 = x.right\\n\\n        x.right = y\\n        y.left = T2\\n\\n        x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))\\n        y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))\\n\\n        return x\\n\\n    def get_height(self, root):\\n        if not root:\\n            return 0\\n\\n        return root.height\\n\\n    def get_balance(self, root):\\n        if not root:\\n            return 0\\n\\n        return self.get_height(root.left) - self.get_height(root.right)\\n\\n    def inorder(self, root):\\n        if root is not None:\\n            self.inorder(root.left)\\n            print(root.val)\\n            self.inorder(root.right)\\n\\n\\navl_tree = AVLTree()\\nroot = None\\n\\nroot = avl_tree.insert(root, 20)\\nroot = avl_tree.insert(root, 10)\\nroot = avl_tree.insert(root, 30)\\nroot = avl_tree.insert(root, 40)\\nroot = avl_tree.insert(root, 50)\\nroot = avl_tree.insert(root, 5)\\nroot = avl_tree.insert(root, 15)\\nroot = avl_tree.insert(root, 25)\\nroot = avl_tree.insert(root, 55)\\n\\navl_tree.inorder(root)\\n```\\n\\n## AVL sorting algorithm\\n\\nGiven an array of ${n}$ elements, the AVL sorting algorithm is done through the following steps\\n\\n- Perform inserts ${n}$ elements into AVL tree. Each inserted operation costs ${O(logn)}$ time (as in a binary search tree). We need to insert ${n}$ elements, so the time complexity of the process will be ${O(nlogn)}$.\\n- We perform inorder traversal (as in a binary search tree). This causes us to go through all the elements, so the time complexity of the process is ${O(n)}$.\\n\\nTherefore, the total time complexity is ${O(n + nlogn) = O(nlogn)}$. However, because of the extra ${O(n)}$ makes this AVL sorting algorithm inefficient and less practical than the other **[sorting algorithms](/blog/2021-02-20-sorting-algorithms/index.md)** that I have listed.\\n\\n## Additional notes\\n\\nReaders can find visualizations for AVL tree operations at **[USFCA website](https://www.cs.usfca.edu/~galles/visualization/AVLtree.html)**.\\n\\n## References\\n\\n[AVL tree](https://en.wikipedia.org/wiki/AVL_tree)\\n\\n[AVL Trees: Rotations, Insertion, Deletion with C++ Example](https://www.guru99.com/avl-tree.html)"},{"id":"binarysearch-tree/","metadata":{"permalink":"/blog/binarysearch-tree/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-02-22-binarysearch-tree/index.md","source":"@site/blog/2021-02-22-binarysearch-tree/index.md","title":"Binary Search Tree","description":"Binary Search Tree","date":"2021-02-22T00:00:00.000Z","formattedDate":"February 22, 2021","tags":[{"label":"tree","permalink":"/blog/tags/tree"},{"label":"binary tree","permalink":"/blog/tags/binary-tree"},{"label":"algorithms","permalink":"/blog/tags/algorithms"},{"label":"search","permalink":"/blog/tags/search"}],"readingTime":10.065,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"binarysearch-tree/","title":"Binary Search Tree","description":"Binary Search Tree","authors":"tranlam","tags":["tree","binary tree","algorithms","search"],"image":"./images/intro.PNG"},"prevItem":{"title":"AVL Tree, AVL Sorting Algorithm","permalink":"/blog/avl-tree/"},"nextItem":{"title":"Fundamental Sorting Algorithms","permalink":"/blog/sorting-algorithms/"}},"content":"![Intro](./images/intro.PNG)\\n\\nIn the process of learning programming, you will encounter many types of data structures such as arrays, linked lists, maps, etc. Each type of data structure has its own advantages and disadvantages. Today, I will talk about an interesting type of data structure called a binary search tree, a very convenient data structure for the search problem.\\n\\n\x3c!--truncate--\x3e\\n\\n## Highlight the main problem\\n\\nThe real problems that we or businesses solve are often divided into small problems and then applying algorithms, as well as appropriate data structures to come up with a way to do it, effectively, efficiently and least costly. For the following problem, I would like to take an example from MIT\'s course 6.006, this is **[the link of the course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/lecture-5-binary-search-trees-bst-sort/)**.\\n\\nSuppose an airline has a route management program. Each flight when it arrives at the airport must request a schedule to land at a certain time. In order not to have any conflicts, the landing times must be at least separated ${k}$ minutes ${(1)}$. The list of landing times is ${R}$ including ${n}$ elements. How to add a landing time ${t}$ to satisfy the constraint ${(1)}$ above.\\n\\nI have an image to make the problem more intuitive\\n\\n![Example](./images/example.PNG)\\n\\nWith the number of elements ${n}$, we want to perform a suitable position finding and insert new landing times in effective time complexity, like ${O(logn)}$. And the following will be the evaluations for the problem with some specific data structures.\\n\\n### Unsorted array\\n\\nProcedure for inserting elements into an unsorted array, regardless of the constraint condition ${(1)}$, will cost ${O(1)}$.\\nProcedure for inserting elements into an unsorted array, taking care of the constraint condition ${(1)}$, will cost ${O(n)}$.\\n\\nTime complexity: ${O(n)}$.\\n\\n### Sorted array\\n\\n- Find the right index will costs ${O(logn)}$ (using binary search).\\n- Compare with the element on either side of the element cost ${O(1)}$.\\n- Inserting the element in the appropriate position takes ${O(n)}$ (when you will probably have to shift most of the elements up 1 position in the case of inserting the element at the beginning of the array).\\n\\nTime complexity: ${O(n)}$.\\n\\n### Sorted linked list\\n\\nInserting an element into a linked list will take ${O(1)}$. But finding the inserted position will takes ${O(n)}$ when we have to traverse from head to that position.\\n\\n![Linked List](./images/linked_list.PNG)\\n\\n### Heap tree\\n\\nInsertion into min-heap or max-heap\\n\\n- Finding the insertion location will takes ${O(n)}$ when we may have to traverse all elements.\\n- Insertion into the min/max heap tree is unstable, because maybe after inserting in a certain position, we break the properties of the min/max heap tree and have to rerun **min/max-heapify** (refer to this article **[Fundamental Sorting Algorithms](/blog/2021-02-20-sorting-algorithms/index.md)** for **max-heapify** implementation) to get the right tree. Rerunning **min/max-heapify** will not able to guarantee the binding condition ${(1)}$.\\n\\n_We need a better data structure to be able to do locating and inserting element in ${O(logn)}$._\\n\\n## Binary Search Tree\\n\\nBinary Search Tree is a data structure that satisfy below conditions ${(2)}$\\n\\n- Each node has a maximum of 2 child nodes.\\n- The value of the left child node is less than the parent node.\\n- The value of the right child node must be greater than the parent node.\\n- The left and right subtree is also a binary search tree.\\n\\nEach node of the tree consists of\\n\\n- The value of node.\\n- The pointer points to the left child node.\\n- The pointer points to the right child node.\\n\\n```python\\nclass Node:\\n    def __init__(self, val, left=None, right=None):\\n        self.val = val\\n        self.left = left\\n        self.right = right\\n```\\n\\nTypes of binary tree:\\n\\n- **Full binary tree:** each node of the tree has 0 or 2 child nodes.\\n- **Complete binary tree:** all the tree layers are filled with nodes except for the last layer, and the bottom layer nodes must be filled from left to right\\n- **Degenerate binary tree:** a tree where all parent nodes have only one child node.\\n- **Perfect binary tree:** Every internal node has 2 children and the leaves are at the same level.\\n- **Balanced binary tree:** the height of the left and right subtrees differ by at most 1.\\n\\nHere is an illustration\\n\\n![Types](./images/types.PNG)\\n\\n## Operations on binary search treee\\n\\nConsider a tree with ${n}$ nodes, height is ${h}$.\\n\\n### Search - Finding a value in the tree\\n\\nFor searching for a key on the tree, we do it by recursive method. Starting at the root, we compare the value of the root node with the key. If the root node value is less than the key, we have to find it on the left subtree, if the root node value is greater than the key, we find that key on the right subtree. We do this with every node we go to. If the node value is equal to the key, we return that node. If the node value is null, we conclude that the key was not found in the tree.\\n\\n![Search](./images/search.JPG)\\n\\nExample of finding the key ${40}$. We first compare ${40 < 50}$, the go down the left subtree to continue to search. We compare ${40 > 30}$, then go down the right subtree to search. Finally, we find a node whose value is ${40}$.\\n\\n**Python Code**\\n\\n```python\\ndef search(root, key):\\n    if root is None:\\n        print(\\"Cannot find the key \\" + key +  \\" in bst\\")\\n        return None\\n    # continue to traverse\\n    if root.val < key:\\n        return search(root.right, key)\\n    elif root.val > key:\\n        return search(root.left, key)\\n    else:\\n        return root\\n```\\n\\n**Algorithm analysis**: finding a key in a tree will cost ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n### Insert - Add a node to the tree\\n\\nThe process of inserting a node with a certain value in the tree is quite similar to search. We search until we encounter an empty node, then we insert the node we want to insert at that position. During searching, we find a node with a value equal to the key, and we return that node.\\n\\n![Insert](./images/insert.PNG)\\n\\nExample of inserting the key with value ${4}$. Compare ${4 < 6}$, go down to the left subtree, compare ${4 > 3}$, go down to the right subtree. We encounter an empty position then we insert the key ${4}$ at that location.\\n\\n**Python Code**\\n\\n```python\\ndef insert(root, key):\\n    # reached leaves\\n    if root is None:\\n        return Node(key)\\n    # continue to search, if encounter an equal value, return that node\\n    else:\\n        if root.val == key:\\n            return root\\n        elif root.val < key:\\n            root.right = insert(root.right, key)\\n        else:\\n            root.left = insert(root.left, key)\\n\\n    return root\\n```\\n\\n**Algorithm analysis**: finding the position to insert in the tree costs ${O(h)}$, insertion costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n**Solving the problem in the first section:** because at the step of inserting the node into the tree, we can see that we can add conditional statements to approve the insertion or not without affecting the properties in the tree at ${(2)}$ of a binary search tree.\\n\\n![Insert2](./images/insert_2.PNG)\\n\\nAs with the tree above, with constraint value ${k = 3}$. We want to insert ${45}$. At the node ${40}$, we have ${45 - 40 > 3}$, so the next step is to insert ${45}$ into the tree, without losing the properties of the tree. If we want to insert ${42}$, we check ${42 - 40 < 3}$, so we do not perform node insertion.\\n\\n### Delete - Remove a node from the tree\\n\\nThe process of deleting a node in a binary search tree occurs in 3 cases\\n\\n- The node to be deleted has no child nodes.\\n- The node to be deleted has 1 child node.\\n- The node to be deleted has both child nodes.\\n\\n**Case 1:** The node to be deleted has no child nodes\\n\\n![Delete](./images/delete.JPG)\\n\\nExample of deleting a node with a key ${40}$ in the tree above, we just need to release it from the tree.\\n\\n**Case 2:** The node to be deleted has 1 child node\\n\\n![Delete2](./images/delete_2.JPG)\\n\\nExample of deleting a node with a key ${90}$ in the tree above, we just need to replace that node with its only child node.\\n\\n**Case 3:** The node to be deleted has 2 children, we replace it with the node with the largest key in its left subtree (the rightmost node of the left subtree), or the node with the smallest key in its right subtree (the leftmost node of the right subtree).\\n\\n![Delete3](./images/delete_3.JPG)\\n\\nExample of deleting a node with a key ${30}$ in the tree above, we find the node with the smallest key in its right subtree, which is ${35}$, and replace the node with the key ${35}$ into the node with the key ${30}$. The we realize that the node has a key ${35}$ in the old position is a node with 1 child node. So we also apply the node deletion procedure like **Case 2** for that location.\\n\\n**Python Code**\\n\\n```python\\n# find leftmost node in the right subtree\\ndef find_min(root):\\n    current = root\\n    while current.left is not None:\\n        current = current.left\\n    return current\\n\\ndef delete(root, key):\\n    if root is None:\\n        return root\\n    # continue to search until we find the node with the right key\\n    if root.val < key:\\n        root.right = delete(root.right, key)\\n    elif root.val > key:\\n        root.left = delete(root.left, key)\\n    else:\\n        # case 1\\n        if root.left is None and root.right is None:\\n            root = None\\n            return root\\n        # case 2\\n        elif root.left is None:\\n            root.val = root.right.val\\n            root.right = None\\n            return root\\n        elif root.right is None:\\n            root.val = root.left.val\\n            root.left = None\\n            return root\\n        # case 3\\n        else:\\n            temp = find_min(root.right)\\n            root.val = temp.val\\n            root.right = delete(root.right, temp.val)\\n            return root\\n\\n    return root\\n```\\n\\n**Algorithmic analysis:**\\n\\n- **Case 1:** Finding the node costs ${O(h)}$, delete the node costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n- **Case 2:** Finding the node costs ${O(h)}$, delete the node costs ${O(1)}$, node transfer costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n- **Case 3:** Finding the node costs ${O(h)}$, finding the leftmost node in the right subtree takes ${O(h)}$, deleting the leftmost node in the right subtree costs ${O(h)}$. Therefore, time complexity will be ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n### 3.4. Traversal\\n\\nThere are three ways to traverse and print the values \u200b\u200bof the nodes in the tree: pre-order, in-order, and post-order.\\n\\nConsider the following tree ${(3)}$\\n\\n![Order](./images/order.PNG)\\n\\n#### Pre-order traversal\\n\\nWe traverse the parent node first, to the left child node, and then to the right child node.\\n\\nExample with ${(3)}$, in the pre-order traversal, the result will be ${6, 3, 1, 10, 9, 12}$.\\n\\n**Python Code**\\n\\n```python\\ndef preorder(root):\\n    if root:\\n        print(root.val)\\n        preorder(root.left)\\n        preorder(root.right)\\n```\\n\\n#### In-order traversal\\n\\nWe traverse the left child node first, to the parent node, and then to the right child node.\\n\\nExample with ${(3)}$, in the pre-order traversal, the result will be ${1, 3, 6, 9, 10, 12}$.\\n\\n**Python Code**\\n\\n```python\\ndef inorder(root):\\n    if root:\\n        inorder(root.left)\\n        print(root.val)\\n        inorder(root.right)\\n```\\n\\n#### Post-order traversal\\n\\nWe traverse the left child node first, the right child node, and then the parent node.\\n\\nExample with ${(3)}$, in the post-order traversal, the result will be ${1, 3, 9, 12, 10, 6}$.\\n\\n**Python Code**\\n\\n```python\\ndef postorder(root):\\n    if root:\\n        postorder(root.left)\\n        postorder(root.right)\\n        print(root.val)\\n```\\n\\n**Algorithm analysis:**\\nWe traverse all the nodes in the tree, so the time complexity is ${O(n)}$.\\n\\n## Additional notes\\n\\nBinary search trees are interesting and efficient data structures. Readers can find **[visualizations of binary search tree operations](https://www.cs.usfca.edu/~galles/visualization/BST.html)**.\\n\\n## References\\n\\n[Binary Search Tree | Set 1 (Search and Insertion)](https://www.geeksforgeeks.org/binary-search-tree-set-1-search-and-insertion/)\\n\\n[Binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree)"},{"id":"sorting-algorithms/","metadata":{"permalink":"/blog/sorting-algorithms/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-02-20-sorting-algorithms/index.md","source":"@site/blog/2021-02-20-sorting-algorithms/index.md","title":"Fundamental Sorting Algorithms","description":"Fundamental Sorting Algorithms","date":"2021-02-20T00:00:00.000Z","formattedDate":"February 20, 2021","tags":[{"label":"algorithms","permalink":"/blog/tags/algorithms"},{"label":"sorting","permalink":"/blog/tags/sorting"}],"readingTime":18.295,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"sorting-algorithms/","title":"Fundamental Sorting Algorithms","description":"Fundamental Sorting Algorithms","authors":"tranlam","tags":["algorithms","sorting"],"image":"./images/intro.JPEG"},"prevItem":{"title":"Binary Search Tree","permalink":"/blog/binarysearch-tree/"},"nextItem":{"title":"Peak Finding Algorithm","permalink":"/blog/peak-finding/"}},"content":"![Intro](./images/intro.JPEG)\\n\\nIn these early blogs, I will only write about the most basic algorithms when I\'m just starting to learn programming. First, let me relearn the basics (because I\'m extremely forgetful). Second, for those who are new to programming, you can refer to it. This article will talk about the basic sorting algorithms I learned in school, and also taught myself.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why do we need sorting algorithms?\\n\\n**Firstly**, simply to pass exams at university, learn some Programming Languages, Data Structures and Algorithms, etc., it\'s easy to be asked these sorts of questions when taking the exam.\\n\\n**Second**, element arrangement is usually an intermediate stage, pre-processing data in many problems, processing systems,... to perform larger jobs after it. Since the amount of data in real systems is always very large, we need efficient sorting algorithms to save the cost (time and memory).\\n\\n**Basic examples of applying sorting algorithms**\\n\\n- Sort the list of customers by name in the customer management system.\\n- Find the median element in ${\\\\Theta(1)}$, or search for an element with ${\\\\Theta(logn)}$ if there is a sorted array.\\n- The database uses merge sort algorithms to sort data sets when they are too large to load locally into memory.\\n- Files search, data compression, routes finding.\\n- Graphic application also use sorting algorithms to arrange layers to render efficiently.\\n- After finishing the meal, your mother made you wash the dishes. You struggled with dozens of bowls for an hour and now you don\'t want to waste any more time on those bowls. As it turns out, the remaining job is to arrange the dishes so that they are neat, beautiful, and most of all, quickly so that you can play with your phone. Instinctively for all Asians of average intelligence, you\'ve sorted them out very quickly and stacked them up from small to big chunks, and then you realize you\'ve accidentally applied Counting Sort algorithm.\\n\\n**Basic operations using in the intermediate stages**\\n\\n- Compare 2 elements ${(a, b)}$, return ${True}$ if ${(a > b)}$, otherwise return ${False}$.\\n- Swap 2 elements ${(a, b)}$, in Python, it can be performed easily like\\n\\n```python\\na, b = b, a\\n```\\n\\nDuring the analysis of algorithms, we assume that the above operations take only constant time ${\\\\Theta(1)}$.\\n\\n## Bubble sort\\n\\nBubble sort is a simple and inefficient sort that is taught in almost all algorithm courses because it is quite intuitive. Bubble sort compares each pair of numbers in an array and swaps places if they\'re out of order. The largest elements will be pushed to the bottom of the array, while the smaller elements will gradually \\"float\\" to the top of the array.\\n\\n**Algorithm:**\\n\\n- Compare ${arr[0]}$ to ${arr[1]}$, if ${arr[0] > arr[1]}$, swap their positions. Continue doing this with (${arr[1], arr[2]}$), (${arr[2], arr[3]}$),...\\n- Repeat this step ${n}$ times.\\n\\nTo make it more intuitive, I give the following descriptive image\\n\\n![Bubble Sort](./images/bubble.GIF)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when we apply the algorithm on the sorted array. Then, there will be no swap steps in the first pass, only comparison steps, from which the algorithm will end after this pass. So the time complexity will be ${\\\\Theta(n)}$. For this reason, bubble sort is also used to check if an array is sorted.\\n- **Worst case:** occurs when the array is sorted in reverse, therefore, ${n-1}$ comparisons and swaps will be performed on the first pass, ${n-2}$ comparisons and swaps will be performed on the second pass,... Therefore, the total number of comparisons and swaps will be ${Sum = (n-1) + (n-2) +...+ 2 + 1 = \\\\frac{n \\\\times (n-1)}{2}}$. Time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef bubble_sort(arr):\\n    for i in range(len(arr)):\\n        swapped = False\\n        for j in range(len(arr) - i - 1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n                swapped = True\\n        if not swapped:\\n            return\\n\\nbubble_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Insertion sort\\n\\nImagine you play card game, when you have the deck in your hand, you have many ways to arrange it depending on your personality. For me, I usually arrange the cards in order from smallest to largest. When I want to arrange a new card into the deck of cards in my hand that is in order, I just insert the card into the appropriate position, and that is also the idea of \u200b\u200binsertion sort.\\n\\n**Algorithm:**\\nWith ${i = 1, 2,..., n - 1}$, we will insert ${arr[i]}$ into the sorted array ${arr[0:i-1]}$ by moving the elements greater than ${arr[i]}$ of the array ${arr[0:i-1]}$ to the top and put ${arr[i]}$ in the desired position.\\n\\nTo make it more intuitive, I give the following descriptive image\\n\\n![Insertion Sort](./images/insertion.GIF)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when we apply the algorithm to the sorted array. Then, we only need to iterate over the array, only compare and do not need to perform a swap step at all. So the time complexity will be ${\\\\Theta(n)}$.\\n- **Worst case:** occurs when the array is sorted in reverse, there will be 1 comparison and assignment in the first pass, 2 comparisons and assignment in the second,... Therefore, the total number of operations compare and assign would be ${Sum = 1 + 2 +...+ (n-1) = \\\\frac{n \\\\times (n-1)}{2}}$. Therefore, time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef insertion_sort(arr):\\n    for key in range(1, len(arr)):\\n        value = arr[key]\\n        j = key - 1\\n        while j >= 0 and value < arr[j]:\\n            arr[j+1] = arr[j]\\n            j -= 1\\n        if key != j+1:\\n            arr[j+1] = value\\n\\ninsertion_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Selection sort\\n\\nThe idea is that we will **assume** to split our array into 2 parts: sorted subarray ${arr{_1}}$ and unsorted subarray ${arr{_2}}$. At this moment, ${arr = (arr{_1})(arr{_2})}$.\\nWe will in turn find the smallest element of ${arr{_2}}$, detach and push the element to ${arr{_1}}$. The **assumption** here, that we are not actually creating 2 new sub-arrays, but the operations are performed on the original array.\\n\\n**Algorithm:**\\n\\n- Find the smallest element of ${arr{_2}}$.\\n- Swap the position of that smallest element with the first element of ${arr{_2}}$. At this point, we assume in ${arr{_2}}$, \u200bthat smallest element is gone, and now it has been merged into ${arr{_1}}$.\\n\\nI have an image to make the algorithm more intuitive\\n\\n![Selection Sort](./images/selection.PNG)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when applying the algorithm on the sorted array, we only have to compare, not swap positions. So the time complexity will be ${\\\\Theta(n)}$.\\n- **Worst case:** occurs when the above array is sorted in reverse, each time we have to find the smallest element of the subarray ${arr{_2}}$. Therefore, the total number of traversals to find the smallest elements will be ${Sum = (n-1) + (n-2) +...+ 1 = \\\\frac{n \\\\times (n-1)}{2}}$. Time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef selection_sort(arr):\\n    for i in range(len(arr) - 1):\\n        min_index = i\\n        for j in range(i+1, len(arr)):\\n            if arr[j] < arr[min_index]:\\n                min_index = j\\n        if i != min_index:\\n            arr[min_index], arr[i] = arr[i], arr[min_index]\\n\\nselection_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Merge sort\\n\\nMerge sort is one of the most efficient algorithms. The algorithm works on the principle of divide and conquer, separating the arrays into 2 sub-arrays, respectively, until the sub-arrays have only 1 element left. Then the algorithm \\"merge\\" those sub-arrays into a fully sorted array.\\n\\n**Algorithm:**\\n\\n- Divide the original array into 2 sub-arrays, 2 sub-arrays into 4 more sub-arrays,... until we get ${n}$ subarrays, each subarray contains 1 element.\\n\\n![Merge Sort 1](./images/merge_1.PNG)\\n\\n- Merge sub-arrays to create larger arrays sorted in order until we get a single array. That is the sorted array from the original array.\\n\\n![Merge Sort 2](./images/merge_2.PNG)\\n\\nSummarize the algorithm in 1 image\\n\\n![Merge Sort](./images/merge.PNG)\\n\\n**Algorithm analysis:**\\n\\n- **Split array:** the algorithm will calculate the midpoint of the array by taking the array length and then dividing it by 2, so it takes constant time ${\\\\Theta(1)}$ to split the array into 2 sub-arrays.\\n- **Sorting subarrays:** assuming array sorting costs ${T(n)}$ time. So to sort 2 sub-arrays, we spend ${2T(\\\\frac{n}{2})}$ time.\\n- **Merge 2 subarrays:** using the \\"2-finger\\" algorithm, each index finger points to the beginning of each subarray. We in turn compare 2 numbers at 2 positions that 2 fingers point to and choose the smaller number to push into the resulting array. Every element in a subarray is pushed in, we move the index finger to the next element of that array. This will make us have to traverse ${2 \\\\times \\\\frac{n}{2} = n}$ elements, therefore, that costs ${\\\\Theta(n)}$. Thus, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = \\\\Theta(1) + 2T(\\\\frac{n}{2}) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith base case here is ${T(1) = \\\\Theta(1)}$.\\n\\n![Merge Sort 3](./images/merge_3.PNG)\\n\\nFor each tree level, the algorithm executes ${\\\\Theta(n)}$ units of work, there are total ${1+logn}$ levels. Therefore, ${T(n) = \\\\Theta(n + nlogn) = \\\\Theta(nlogn)}$. Time complexity will be ${\\\\Theta(nlogn)}$.\\n\\n- **Space complexity:** Because in the \\"merge\\" step, we have to manually create 2 sub-arrays, each with a number of elements ${\\\\frac{n}{2}}$, so the auxiliary memory space will be ${\\\\Theta(n)}$.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef merge(arr, l, m, r):\\n    n1 = m - l + 1\\n    n2 = r - m\\n    L = []\\n    R = []\\n    for i in range(n1):\\n        L.append(arr[l+i])\\n    for j in range(n2):\\n        R.append(arr[m+j+1])\\n    i = 0\\n    j = 0\\n    k = l\\n    while i < n1 and j < n2:\\n        if L[i] <= R[j]:\\n            arr[k] = L[i]\\n            i += 1\\n        else:\\n            arr[k] = R[j]\\n            j += 1\\n        k += 1\\n    while i < n1:\\n        arr[k] = L[i]\\n        i += 1\\n        k += 1\\n    while j < n2:\\n        arr[k] = R[j]\\n        j += 1\\n        k += 1\\n\\ndef merge_sort(arr, l, r):\\n    if l < r:\\n        m = math.floor(l + (r-l)/2)\\n        merge_sort(arr, l, m)\\n        merge_sort(arr, m+1, r)\\n        merge(arr, l, m, r)\\n\\nmerge_sort(ini_arr, 0 ,len(ini_arr) - 1)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Heap sort\\n\\nThe heap sort is based on the binary heap data structure.\\n\\n**Binary heap data structure:**\\nAn array of data can be represented as a binary tree as follows\\n\\n![Heap Sort](./images/heap.PNG)\\n\\nFor any node with the corresponding index ${i}$ in the binary tree above\\n\\n- Parent node of ${i}$, which is ${parent(i)}$ will have the index of ${\\\\Bigl\\\\lfloor\\\\dfrac{i}{2}\\\\Bigr\\\\rfloor\\\\qquad}$.\\n- Left child node, which is ${leftchild(i)}$ will have the index of ${2i}$.\\n- Right child node, which is ${rightchild(i)}$ will have the index of ${2i + 1}$.\\n\\nThere are two types of this data structure: max-heap and min-heap.\\n\\n- In max-heap, we always have ${A[parent(i)] \\\\ge A[i]}$. Therefore, the largest element is in the root, and the smallest element is in the leaf.\\n- In min-heap, we always have ${A[parent(i)] \\\\le A[i]}$. Therefore, the smallest element is in the root, and the largest element is in the leaf.\\n\\nFrom there, this sorting algorithm applies max-heap or min-heap (in this article, I will use max-heap) to create an ordered array.\\n\\n**Create a max-heap:** called **max_heapify**\\nI will give a simple example with a 3-element array for added visualization, but with an n-element array it will need to be done in a more general way\\n\\n![Heap Sort 2](./images/heap_2.PNG)\\n\\nPython code for **max_heapify** at a node with index ${i = index}$, ${length}$ is the length of the array, added as a constraint for the index of child nodes. The algorithm below says that, if the node is at ${i = index}$ ot in accordance with the max-heap rule, we will **max_heapify** the tree with the root as that node, and **max_heapify** the trees with the root being the left and right child nodes of that node.\\n\\n```python\\ndef max_heapify(arr, length, index):\\n    l = (index + 1) * 2 - 1\\n    r = (index + 1) * 2\\n    largest = index\\n    if l < length and arr[index] < arr[l]:\\n        largest = l\\n    if r < length and arr[largest] < arr[r]:\\n        largest = r\\n    if largest != index:\\n        arr[index], arr[largest] = arr[largest], arr[index]\\n        max_heapify(arr, length, largest)\\n```\\n\\n**max_heapify** will cost ${\\\\Theta(logn)}$ for each node under consideration. Because each time, that node will need to go down 1 level in the tree to be considered, the algorithm will choose the correct branch to go down and it will not backtrack back up. Therefore, the longest path this algorithm can take is from root to a leaf, which is the height of the tree. The height of the binary tree has ${n}$ nodes is ${\\\\Theta(logn)}$.\\n\\n**Algorithm:**\\n\\n- We represent the array and sort the elements to get a max-heap tree. Therefore, the root of this tree (this will correspond to the element with index ${i = 0}$ in the array, for ${i = 0, 1,..., n-1}$) will be the largest element.\\n- We swap the location of root ${arr[0]}$ with the last element of the array ${arr[n-1]}$. At this point, the largest element of the array is in the last index.\\n- Repeat steps 1 and 2 with the rest of the array ${arr[0:n-2]}$,...\\n\\n**Algorithm analysis:**\\nBuilding a max-heap tree from an unsorted array needs ${\\\\Theta(n)}$ functiohn calls **max_heapify**, each **max_heapify** cost ${\\\\Theta(logn)}$ time. Thus, the whole algorithm has a time complexity of ${\\\\Theta(nlogn)}$.\\nHowever, the heap sort algorithm has an advantage over merge sort in that it only uses ${\\\\Theta(1)}$ temporary memory, while merge sort is ${\\\\Theta(n)}$. If the memory factor is also important in your system (eg. small memory systems like embedded systems, etc.), you should consider using heap sort rather than merge sort.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef max_heapify(arr, length, index):\\n    l = (index + 1) * 2 - 1\\n    r = (index + 1) * 2\\n    largest = index\\n    if l < length and arr[index] < arr[l]:\\n        largest = l\\n    if r < length and arr[largest] < arr[r]:\\n        largest = r\\n    if largest != index:\\n        arr[index], arr[largest] = arr[largest], arr[index]\\n        max_heapify(arr, length, largest)\\n\\ndef heap_sort(arr):\\n    length = len(arr)\\n    last = math.floor(length / 2)\\n    # T\u1ea1i \u0111\xe2y, ch\u1ec9 duy\u1ec7t t\u1eeb ph\u1ea7n t\u1eed n/2 \u0111\u1ed5 v\u1ec1, v\xec ph\u1ea7n t\u1eed t\u1eeb n/2 + 1,..., n \u0111\u1ec1u l\xe0 leaves. C\xe1c leaves \u0111\xe3 \u0111\u01b0\u1ee3c th\u1ecfa m\xe3n t\xednh ch\u1ea5t max-heap\\n    for i in range(last - 1, -1, -1):\\n        max_heapify(arr, length, i)\\n    for i in range(length - 1, 0, -1):\\n        arr[i], arr[0] = arr[0], arr[i]\\n        max_heapify(arr, i, 0)\\n\\nheap_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Quick sort\\n\\nThe quick sort algorithm, developed by a British computer scientist Tony Hoare in 1959, uses the principle of divide and conquer to sort an array.\\n\\n**Algorithm:**\\n\\n- **Split array:**\\n  - Select any element (called pivot), ${A[m]}$. If we choose a good pivot, our algorithm will run very fast. However, it will be difficult to know which element is considered a good pivot. There are a few common pivot selections:\\n    - Choose a random pivot.\\n    - Select the leftmost or rightmost element.\\n    - Take 3 elements: first, middle, last of the array and pick the median from them.\\n  - Split our array into 2 subparts: ${A[l:m-1]}$ consists of elements smaller than ${A[m]}$, and ${A[m+1:r]}$ consists of elements larger than ${A[m]}$. The image below shows more visually how to divide the array, with pivot always taking the last element\\n\\n![Quick Sort 2](./images/quick_2.PNG)\\n\\n- **Conquer:** recursively sort the above 2 subsections using quick sort.\\n- **Merge:** no need to combine the divided subsections because the final result is already sorted array.\\n- **The complete recursive algorithm:**\\n  - Select a pivot. Split array into 2 parts based on pivot.\\n  - Apply quick sort on the smaller half of the pivot.\\n  - Apply quick sort on half larger than pivot.\\n\\n**Algorithm analysis:**\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(k) + T(n-k-1) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith ${k}$ is the number of elements which are smaller than pivot. The time complexity for **partitioning** process is ${\\\\Theta(n)}$.\\n\\n- **Best case:** occurs when the **partitioning** algorithm always divides our array into exactly 2 equal or nearly equal parts.\\n\\n![Quick Sort](./images/quick.PNG)\\n\\nThus, in the best case, the time complexity will be ${\\\\Theta(nlogn)}$.\\n\\n- **Worst case:** occurs when the **partitioning** algorithm always chooses the largest or the smallest number as the pivot. If we choose the pivot using the \\"always pick the last element of the array\\" strategy, the worst case will happen when the array is sorted in descending order. At this moment\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(0) + T(n-1) + \\\\Theta(n) = T(n-1) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith the base case being ${\\\\Theta(1)}$ then in the worst case, time complexity will be ${T(n) = \\\\Theta(1) + \\\\Theta(n) + \\\\Theta(n) +...+ \\\\Theta(n) = \\\\Theta(n^2)}$\\n\\nAlthough the worst case of quick sort is much slower than other sorting algorithms, in practice the partition loop can be implemented efficiently on almost all data structures, because it contains relatively fewer \\"constant factors\\" (operators that require constant time ${\\\\Theta(1)}$) than other algorithms, and if two algorithms have the same time complexity ${\\\\Theta(nlogn)}$, the algorithm with fewer \\"constant factors\\" runs faster. Furthermore, the worst case of quick sort will rarely happen. However, with a very large amount of data and stored in external memory, merge sort will be preferred over quick sort.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef partition(arr, l, r):\\n    pointer = l - 1\\n    pivot = arr[r]\\n    for j in range(l, r):\\n        if arr[j] < pivot:\\n            pointer += 1\\n            arr[pointer], arr[j] = arr[j], arr[pointer]\\n    arr[pointer+1], arr[r] = arr[r], arr[pointer+1]\\n    return pointer + 1\\n\\ndef quick_sort(arr, l, r):\\n    if l < r:\\n        pivot_index = partition(arr, l, r)\\n        quick_sort(arr, l, pivot_index - 1)\\n        quick_sort(arr, pivot_index+1, r)\\n\\nquick_sort(ini_arr, 0, len(ini_arr) - 1)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n## Counting sort\\n\\nAn interesting algorithm I learned that even runs at linear time is counting sort. This algorithm is applied almost exclusively to integers, it is difficult or impossible to apply to real numbers.\\n\\n**Algorithm:**\\n\\n- Assume the elements of the original array ${A[0, 1, 2,..., n-1]}$ contains elements with values \u200b\u200bin the range ${[0, k]}$. Counting sort creates another temporary array to count as array ${B[0, 1, 2,..., k-1]}$ including ${k}$ elements. The ${i^{th}}$ element of ${B}$ will contain the number of elements ${A[j]}$ satisfy ${A[j] = i }$ with ${j = 0, 1, 2,..., n-1}$.\\n- From ${B}$, if we flatten it, we will get an array ${C[0, 1, 2,..., n-1]}$ contains ordered elements. To be more intuitive, I have the following example\\n\\n![Counting Sort](./images/counting.PNG)\\n\\nFor the case ${A}$ contains negative elements, we find the smallest element of ${A}$ and store that element of ${A}$ at the index ${0}$ of the array ${B}$ (because there cannot exist negative index in an array).\\n\\n**Algorithm analysis:**\\n\\n- Create empty array ${B}$ cost ${\\\\Theta(k)}$ time.\\n- Calculate the elements of the array ${B}$ based on ${A}$ cost ${\\\\Theta(n)}$ time.\\n- Flatten ${B}$ to have ${C}$ will cost ${\\\\Theta(n)}$ time.\\n\\nTotal time complexity will be ${\\\\Theta(n+k)}$.\\n\\nThis analysis will be clearly written in the comments of the code.\\n\\n- **Space complexity:** ${\\\\Theta(n+k)}$\\n\\n**Python Code**\\n\\n```python\\nini_arr = [-10, -5, -20, -10, -1, -5, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef counting_sort(A):\\n    min_ele = int(min(A))\\n    max_ele = int(max(A))\\n    range_ele = max_ele - min_ele + 1\\n    B = []\\n    # This costs O(k) time\\n    for i in range(range_ele):\\n        B.append(0)\\n    ###############################\\n\\n    # This costs O(n) time\\n    for i in range(len(A)):\\n        B[A[i] - min_ele] += 1\\n    ###############################\\n\\n    C = []\\n    # We have sum(B)= n = len(A) and append() cost constant time O(1). So this step costs O(n) time\\n    for i in range(range_ele):\\n        for j in range(B[i]):\\n            C.append(i + min_ele)\\n    ###############################\\n\\n    # ----\x3e In total, the algorithm costs O(n+k) time complexity\\n    return C\\n\\nprint(counting_sort(ini_arr))\\n```\\n\\nOutput\\n\\n```python\\n[-20, -10, -10, -5, -5, -1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n**Counting sort comments:**\\n\\n- As we saw above, counting sort has a time complexity and a space complexity ${\\\\Theta(n+k)}$, o counting sort is very efficient when the range of the input data is small, not much larger than ${n}$ (or when ${k}$ is quite small). For example, if ${k}$ is large, about ${k = \\\\Theta(n^2)}$, then time complexity and space complexity will be ${\\\\Theta(n^2)}$, that is very bad.\\n- Counting sort can also be suitable for problems where the elements of the array are another data structure, but that data structure must have a ${key}$ is the integer to represent each object in that data structure.\\n- Counting sort is also a subroutine for a more powerful algorithm called Radix sort, a sort algorithm that runs at linear time with values ${k}$ greater than in counting sort, which is ${k = n^{\\\\Theta(1)}}$ (constant power of ${n}$).\\n- As mentioned in the introduction, you arrange the bowls in order from small to large, accumulating the number of bowls by size into blocks one by one. That is using counting sort. As the example below, equal numbers are cubed by color.\\n\\n![Counting Sort 2](./images/counting_2.PNG)\\n\\n## Additional notes\\n\\nSorting algorithms are quite interesting. One of the things that makes people feel the most comfortable is seeing their bedrooms organized and clean, the same is true when we look at other things arranged. **[The website about animations of sorting algorithms](https://www.toptal.com/developers/sorting-algorithms)** that give you a sense of satisfaction and relaxation.\\n\\n## References\\n\\n[Sorting Algorithms](https://www.interviewbit.com/tutorial/sorting-algorithms/#sorting-algorithms)\\n\\n[Sorting Algorithms](https://brilliant.org/wiki/sorting-algorithms/)\\n\\n[QuickSort](https://www.geeksforgeeks.org/quick-sort/)"},{"id":"peak-finding/","metadata":{"permalink":"/blog/peak-finding/","editUrl":"https://github.com/LTranData/blogs/edit/main/blog/2021-02-18-peak-finding/index.md","source":"@site/blog/2021-02-18-peak-finding/index.md","title":"Peak Finding Algorithm","description":"Peak Finding Algorithm","date":"2021-02-18T00:00:00.000Z","formattedDate":"February 18, 2021","tags":[{"label":"peak","permalink":"/blog/tags/peak"},{"label":"peak finding","permalink":"/blog/tags/peak-finding"},{"label":"algorithms","permalink":"/blog/tags/algorithms"}],"readingTime":5.07,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/LTranData","imageURL":"https://github.com/LTranData.png","key":"tranlam"}],"frontMatter":{"slug":"peak-finding/","title":"Peak Finding Algorithm","description":"Peak Finding Algorithm","authors":"tranlam","tags":["peak","peak finding","algorithms"],"image":"./images/intro.PNG"},"prevItem":{"title":"Fundamental Sorting Algorithms","permalink":"/blog/sorting-algorithms/"}},"content":"![Intro](./images/intro.PNG)\\n\\nToday I will talk about an extremely basic algorithm that I and most of you who are starting to learn about algorithmic programming have done. That is the Peak Finding algorithm.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction to the algorithm\\n\\nIn an array, a number is said to be a \\"peak\\" if and only if its adjacent elements are less than or equal to the element in question. Imagine that there is a mountain range like this\\n\\n![Peaks](./images/peaks.PNG)\\n\\nThe red arrows above point to the peaks of a mountain range, because those points are higher than the neighboring points around it (the points on the mountainside).\\n\\nTo be more intuitive in programming, let\'s take an example with the following array:\\n![1D Array](./images/1Darr.PNG)\\n\\nConsidering the array of symbols above, the element at position 3 is called a peak if and only if **${c \\\\ge b}$** and **${c \\\\ge d}$**. The 9th element is called a peak if and only if **${i \\\\ge h}$**.\\n\\nNotice that:\\n\\n- In an array, there will always be at least one peak.\\n- This problem of ours will be to find one peak, not all the peaks.\\n\\n## Finding peak in 1-dimensional array\\n\\nSuppose we have a 1-dimensional array of **${n}$** elements, find a peak of that array.\\n\\n### Linear Traversing\\n\\n**Idea:** Iterate through each element of the array and check if the element under consideration satisfies the property of being a peak.\\n\\n**Algorithm analysis:** Each element being browsed will have conditional statements to check if the element is a peak, these conditional statements take constant time ${\\\\Theta(1)}$. In the worst case, we\'ll have to go through all **${n}$** elements of the array to find the peak. Therefore, the worst case of the algorithm will have a time complexity of ${\\\\Theta(n)}$.\\n\\n### Binary Search\\n\\n**Idea:** In this way, we will always look at the middle of the traversed array and decide which half of the array to traverse to find the peak.\\n\\n**Algorithm:**\\n\\n- Look at the location ${\\\\frac{n}{2}}$.\\n- If ${a[\\\\frac{n}{2}] \\\\lt a[\\\\frac{n}{2} - 1]}$, we look at the left half (the elements ${1, 2,...,\\\\frac{n}{2} - 1)}$ of the array under consideration to find the peak.\\n- If ${a[\\\\frac{n}{2}] \\\\lt a[\\\\frac{n}{2} + 1]}$, we look at the right half (the elements ${\\\\frac{n}{2} + 1, \\\\frac{n}{2} + 2,..., n)}$ of the array under consideration to find the peak.\\n- If both conditions are not satisfied, we return the position element ${\\\\frac{n}{2}}$ is a peak.\\n\\nTo explain this, I have a drawing to make it more intuitive\\n\\n![1D Expression](./images/1Dexp.PNG)\\n\\nThe red arrow points to the current position. Suppose we are standing on a position in the mountains, so that we can climb to the top, we will always look to the side that we see its position higher than where we are standing, and that is also the algorithm to solve for this problem.\\n\\n**Algorithm analysis:** Using divide and conquer, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(\\\\frac{n}{2}) + \\\\Theta(1)}$\\n\\n</p>\\n\\nTime complexity for comparative conditionals ${\\\\Theta(1)}$, base case here is ${T(1) = \\\\Theta(1)}$.\\nFrom that, ${T(n) = \\\\Theta(1) + \\\\Theta(1) +...+ \\\\Theta(1) = \\\\Theta(log{_2}{n})}$.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [10, 20, 15, 2, 23, 90, 67]\\n\\ndef peak_finding(arr):\\n    length = len(arr)\\n    middle = math.floor(length / 2)\\n    if length == 1:\\n        return arr[0]\\n    if length == 2:\\n        return arr[0] if (arr[0] >= arr[1]) else arr[1]\\n    if arr[middle] < arr[middle - 1]:\\n        return peak_finding(arr[:middle])\\n    elif arr[middle] < arr[middle + 1]:\\n        return peak_finding(arr[middle + 1:])\\n    else:\\n        return arr[middle]\\n\\n\\nprint(peak_finding(ini_arr))\\n```\\n\\nOutput\\n\\n```python\\n20\\n```\\n\\n## Finding peak in 2-dimensional array\\n\\nSuppose we have a 2-dimensional array **${m \\\\times n}$** represented as a matrix of m rows and n columns\\n![2D Matrix](./images/2Dmat.PNG)\\n\\nAn element is considered to be a peak if and only if it is greater than or equal to all adjacent elements vertically and horizontally.\\n\\n### Direct traversal\\n\\n**Idea:** Iterate through each element of the array and check if the element under consideration satisfies the property of being a peak.\\n\\n**Algorithm analysis:** the worst case of the algorithm will have a time complexity of ${\\\\Theta(m \\\\times n)}$.\\n\\n### Greedy Ascent Algorithm\\n\\n**Idea:** We start at a random point. With the point under consideration, we compare it with 4 adjacent points vertically and horizontally, if any value is greater than the point under consideration, we will consider the next point to be that point.\\n\\n**Algorithm analysis:** At first glance, the algorithm seems to be more efficient, but its worst case is still ${\\\\Theta(m \\\\times n)}$ when we have to traverse most of the elements.\\n\\n### Jamming Binary Search Algorithm\\n\\n**Idea:** We rely on Binary Search as applied to the 1-dimensional array above.\\n\\n**Algorithm:**\\n\\n- Select the column in the middle ${i = \\\\frac{n}{2}}$. Find the maximum value of that column. Assume that value is at position ${(j, i)}$.\\n- Compare elements at positions ${(j, i - 1), (j, i), (j, i + 1)}$.\\n- We choose the left-part submatrix if ${a[j, i - 1] \\\\gt a[j, i]}$, choose the right-part submatrix if ${a[j, i + 1] \\\\gt a[j, i]}$ to consider the next step.\\n- Otherwise, we return the value ${a[j, i]}$ is a peak.\\n\\n**Algorithm analysis:** The base case here will be that we only have a single column, find the maximum value of that column. From this, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(m, n) = T(m, \\\\frac{n}{2}) + \\\\Theta(m) }$\\n\\n</p>\\n\\nWith ${T(m, 1) = \\\\Theta(m)}$.\\nTherefore, ${T(m, n) = \\\\Theta(m) + \\\\Theta(m) +...+ \\\\Theta(m) = \\\\Theta(mlog{_2}{n})}$.\\n\\n**Python Code**\\n\\n```python\\nimport numpy as np\\nimport math\\nini_matrix = np.array([[14, 13, 12, 6], [15, 9, 11, 7], [\\n                      16, 17, 19, 92], [17, 18, 17, 12]])\\n\\ndef peak_finding_2d(matrix):\\n    (rows, cols) = matrix.shape\\n    j = math.floor(cols / 2)\\n    i_in_max = np.argmax(matrix[:, j])\\n    if cols == 1:\\n        return matrix[i_in_max, j]\\n    if cols == 2:\\n        peak_1 = np.amax(matrix[:, 0])\\n        peak_2 = np.amax(matrix[:, 1])\\n        return peak_1 if peak_1 >= peak_2 else peak_2\\n    if matrix[i_in_max, j] < matrix[i_in_max, j-1]:\\n        return peak_finding_2d(matrix[:, :j])\\n    elif matrix[i_in_max, j] < matrix[i_in_max, j+1]:\\n        return peak_finding_2d(matrix[:, (j+1):])\\n    else:\\n        return matrix[i_in_max, j]\\n\\n\\nprint(peak_finding_2d(ini_matrix))\\n```\\n\\nOutput\\n\\n```python\\n92\\n```"}]}')}}]);