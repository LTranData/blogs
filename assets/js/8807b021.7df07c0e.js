"use strict";(self.webpackChunkkgajera_blog=self.webpackChunkkgajera_blog||[]).push([[1957],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>g});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var o=a.createContext({}),c=function(e){var n=a.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(o.Provider,{value:n},e.children)},h="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),h=c(t),u=i,g=h["".concat(o,".").concat(u)]||h[u]||d[u]||r;return t?a.createElement(g,s(s({ref:n},p),{},{components:t})):a.createElement(g,s({ref:n},p))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,s=new Array(r);s[0]=u;var l={};for(var o in n)hasOwnProperty.call(n,o)&&(l[o]=n[o]);l.originalType=e,l[h]="string"==typeof e?e:i,s[1]=l;for(var c=2;c<r;c++)s[c]=t[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},9850:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var a=t(7462),i=(t(7294),t(3905));const r={slug:"spark-catalyst-optimizer-and-spark-session-extension",title:"Spark catalyst optimizer v\xe0 Spark session extension",authors:"tranlam",tags:["Bigdata","Spark","Apache"],image:"./images/spark-catalyst-optimizer.JPG"},s=void 0,l={permalink:"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension",editUrl:"https://github.com/lam1051999/blogs/edit/main/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md",source:"@site/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md",title:"Spark catalyst optimizer v\xe0 Spark session extension",description:"Spark catalyst optimizer n\u1eb1m trong ph\u1ea7n core c\u1ee7a Spark SQL v\u1edbi m\u1ee5c \u0111\xedch t\u1ed1i \u01b0u c\xe1c truy v\u1ea5n c\xf3 c\u1ea5u tr\xfac \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi d\u1ea1ng SQL ho\u1eb7c qua c\xe1c API DataFrame/Dataset, gi\u1ea3m thi\u1ec3u th\u1eddi gian v\xe0 chi ph\xed ch\u1ea1y c\u1ee7a \u1ee9ng d\u1ee5ng. Khi s\u1eed d\u1ee5ng Spark, th\u01b0\u1eddng m\u1ecdi ng\u01b0\u1eddi xem catalyst optimizer nh\u01b0 l\xe0 m\u1ed9t black box, khi ch\xfang ta m\u1eb7c nhi\xean cho r\u1eb1ng n\xf3 ho\u1ea1t \u0111\u1ed9ng m\u1ed9t c\xe1ch th\u1ea7n b\xed m\xe0 kh\xf4ng th\u1ef1c s\u1ef1 quan t\xe2m b\xean trong n\xf3 x\u1ea3y ra nh\u1eefng g\xec. \u1ede b\xe0i vi\u1ebft n\xe0y, m\xecnh s\u1ebd \u0111i v\xe0o t\xecm hi\u1ec3u b\xean trong logic c\u1ee7a n\xf3 th\u1ef1c s\u1ef1 th\u1ebf n\xe0o, c\xe1c th\xe0nh ph\u1ea7n, v\xe0 c\xe1ch m\xe0 Spark session extension tham gia \u0111\u1ec3 thay \u0111\u1ed5i c\xe1c plan c\u1ee7a catalyst.",date:"2023-01-07T00:00:00.000Z",formattedDate:"January 7, 2023",tags:[{label:"Bigdata",permalink:"/blogs/blog/tags/bigdata"},{label:"Spark",permalink:"/blogs/blog/tags/spark"},{label:"Apache",permalink:"/blogs/blog/tags/apache"}],readingTime:16.3,truncated:!0,authors:[{name:"Tr\u1ea7n L\xe2m",title:"Data Engineer",url:"https://github.com/lam1051999",imageURL:"https://github.com/lam1051999.png",key:"tranlam"}],frontMatter:{slug:"spark-catalyst-optimizer-and-spark-session-extension",title:"Spark catalyst optimizer v\xe0 Spark session extension",authors:"tranlam",tags:["Bigdata","Spark","Apache"],image:"./images/spark-catalyst-optimizer.JPG"},nextItem:{title:"MySQL series - Indexing",permalink:"/blogs/blog/mysql-series-mysql-indexing"}},o={image:t(2941).Z,authorsImageUrls:[void 0]},c=[{value:"1. Tree v\xe0 Node",id:"1-tree-v\xe0-node",level:3},{value:"2. Rules",id:"2-rules",level:3},{value:"3. C\xe1c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Catalyst trong Spark SQL",id:"3-c\xe1c-ho\u1ea1t-\u0111\u1ed9ng-c\u1ee7a-catalyst-trong-spark-sql",level:3},{value:"3.1. Parsing v\xe0 Analyzing",id:"31-parsing-v\xe0-analyzing",level:4},{value:"3.2. Logical plan optimizations",id:"32-logical-plan-optimizations",level:4},{value:"3.3. Physical planning",id:"33-physical-planning",level:4},{value:"3.4. Code generation",id:"34-code-generation",level:4},{value:"4. Spark session extension",id:"4-spark-session-extension",level:3},{value:"4.1. Custom parser rule",id:"41-custom-parser-rule",level:4},{value:"4.2. Custom analyzer rule",id:"42-custom-analyzer-rule",level:4},{value:"4.3. Custom optimization",id:"43-custom-optimization",level:4},{value:"4.4. Custom physical planning",id:"44-custom-physical-planning",level:4},{value:"4.5. V\xed d\u1ee5 code c\u1ea5u h\xecnh ph\u1ea7n logical plan optimization trong Catalyst optimizer",id:"45-v\xed-d\u1ee5-code-c\u1ea5u-h\xecnh-ph\u1ea7n-logical-plan-optimization-trong-catalyst-optimizer",level:4},{value:"4.5.1. Khi kh\xf4ng apply extension",id:"451-khi-kh\xf4ng-apply-extension",level:5},{value:"4.5.2. Khi c\xf3 extension",id:"452-khi-c\xf3-extension",level:5},{value:"5. T\xe0i li\u1ec7u tham kh\u1ea3o",id:"5-t\xe0i-li\u1ec7u-tham-kh\u1ea3o",level:3}],p={toc:c};function h(e){let{components:n,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Spark catalyst optimizer n\u1eb1m trong ph\u1ea7n core c\u1ee7a Spark SQL v\u1edbi m\u1ee5c \u0111\xedch t\u1ed1i \u01b0u c\xe1c truy v\u1ea5n c\xf3 c\u1ea5u tr\xfac \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi d\u1ea1ng SQL ho\u1eb7c qua c\xe1c API DataFrame/Dataset, gi\u1ea3m thi\u1ec3u th\u1eddi gian v\xe0 chi ph\xed ch\u1ea1y c\u1ee7a \u1ee9ng d\u1ee5ng. Khi s\u1eed d\u1ee5ng Spark, th\u01b0\u1eddng m\u1ecdi ng\u01b0\u1eddi xem catalyst optimizer nh\u01b0 l\xe0 m\u1ed9t black box, khi ch\xfang ta m\u1eb7c nhi\xean cho r\u1eb1ng n\xf3 ho\u1ea1t \u0111\u1ed9ng m\u1ed9t c\xe1ch th\u1ea7n b\xed m\xe0 kh\xf4ng th\u1ef1c s\u1ef1 quan t\xe2m b\xean trong n\xf3 x\u1ea3y ra nh\u1eefng g\xec. \u1ede b\xe0i vi\u1ebft n\xe0y, m\xecnh s\u1ebd \u0111i v\xe0o t\xecm hi\u1ec3u b\xean trong logic c\u1ee7a n\xf3 th\u1ef1c s\u1ef1 th\u1ebf n\xe0o, c\xe1c th\xe0nh ph\u1ea7n, v\xe0 c\xe1ch m\xe0 Spark session extension tham gia \u0111\u1ec3 thay \u0111\u1ed5i c\xe1c plan c\u1ee7a catalyst."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spark catalyst optimizer",src:t(2941).Z,width:"1280",height:"720"})),(0,i.kt)("h3",{id:"1-tree-v\xe0-node"},"1. Tree v\xe0 Node"),(0,i.kt)("p",null,"C\xe1c th\xe0nh ph\u1ea7n ch\xednh trong Catalyst \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng c\xe2y v\xe0 c\xe1c node, \u0111\u01b0\u1ee3c k\u1ebf th\u1eeba t\u1eeb class ",(0,i.kt)("inlineCode",{parentName:"p"},"TreeNode"),", ho\u1eb7c c\xe1c class con c\u1ee7a n\xf3. Class ",(0,i.kt)("inlineCode",{parentName:"p"},"TreeNode")," n\xe0y c\xf3 t\u1eadp c\xe1c node con \u1ee9ng v\u1edbi thu\u1ed9c t\xednh ",(0,i.kt)("inlineCode",{parentName:"p"},"children"),", ki\u1ec3u d\u1eef li\u1ec7u ",(0,i.kt)("inlineCode",{parentName:"p"},"Seq[BaseType]"),", do v\u1eady, m\u1ed9t ",(0,i.kt)("inlineCode",{parentName:"p"},"TreeNode")," c\xf3 th\u1ec3 c\xf3 0 ho\u1eb7c nhi\u1ec1u c\xe1c node con. C\xe1c object n\xe0y l\xe0 immutable v\xe0 \u0111\u01b0\u1ee3c thao t\xe1c b\u1eb1ng nh\u1eefng functional transformation, khi\u1ebfn cho vi\u1ec7c debug optimizer tr\u1edf n\xean d\u1ec5 d\xe0ng h\u01a1n v\xe0 c\xe1c ho\u1ea1t \u0111\u1ed9ng song song tr\u1edf n\xean d\u1ec5 \u0111o\xe1n h\u01a1n.",(0,i.kt)("br",{parentName:"p"}),"\n","Hai class quan tr\u1ecdng l\xe0 ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," v\xe0 ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkPlan")," \u0111\u1ec1u l\xe0 subclass c\u1ee7a ",(0,i.kt)("inlineCode",{parentName:"p"},"QueryPlan"),", class k\u1ebf th\u1eeba tr\u1ef1c ti\u1ebfp t\u1eeb ",(0,i.kt)("inlineCode",{parentName:"p"},"TreeNode"),". Trong s\u01a1 \u0111\u1ed3 Catalyst b\xean tr\xean, 3 th\xe0nh ph\u1ea7n \u0111\u1ea7u l\xe0 c\xe1c logical plans, c\xe1c node trong logical plan th\u01b0\u1eddng l\xe0 c\xe1c to\xe1n t\u1eed \u0111\u1ea1i s\u1ed1 nh\u01b0 join, and, or,... 2 th\xe0nh ph\u1ea7n \u0111\u1eb1ng sau l\xe0 c\xe1c spark plan (physical plan), c\xe1c node th\u01b0\u1eddng l\xe0 c\xe1c to\xe1n t\u1eed low-level nh\u01b0 ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledHashJoinExec"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"SortMergeJoinExec"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"BroadcastHashJoinExec"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"FileSourceScanExec"),",... C\xe1c leaf node s\u1ebd \u0111\u1ecdc d\u1eef li\u1ec7u t\u1eeb c\xe1c source, storage, memory,... c\xf2n root node c\u1ee7a c\xe2y l\xe0 to\xe1n t\u1eed ngo\xe0i c\xf9ng v\xe0 tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c t\xednh to\xe1n."),(0,i.kt)("h3",{id:"2-rules"},"2. Rules"),(0,i.kt)("p",null,"\u0110\u1ec3 thao t\xe1c tr\xean TreeNode ta s\u1eed d\u1ee5ng c\xe1c Rule, c\xe1c Rule th\u1ef1c ch\u1ea5t ch\u1ee9a c\xe1c h\xe0m bi\u1ebfn \u0111\u1ed5i t\u1eeb c\xe2y n\xe0y sang c\xe2y kh\xe1c. Th\u01b0\u1eddng c\xe1c h\xe0m n\xe0y \u0111\u01b0\u1ee3c vi\u1ebft s\u1eed d\u1ee5ng pattern matching trong scala \u0111\u1ec3 t\xecm c\xe1c matching t\u01b0\u01a1ng \u1ee9ng trong subtree c\u1ee7a n\xf3 v\xe0 thay th\u1ebf b\u1eb1ng c\xe1c c\u1ea5u tr\xfac kh\xe1c.\nC\xe1c c\xe2y cung c\u1ea5p c\xe1c h\xe0m transform c\xf3 th\u1ec3 \xe1p d\u1ee5ng pattern matching n\xe0y \u0111\u1ec3 bi\u1ebfn \u0111\u1ed5i c\xe2y nh\u01b0 ",(0,i.kt)("inlineCode",{parentName:"p"},"transform"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"transformDown"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"transformUp"),",..."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"package org.apache.spark.sql.catalyst.trees\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\n   * When `rule` does not apply to a given node it is left unchanged.\n   * Users should not expect a specific directionality. If a specific directionality is needed,\n   * transformDown or transformUp should be used.\n   *\n   * @param rule the function used to transform this nodes children\n*/\ndef transform(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformDown(rule)\n}\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\n   * When `rule` does not apply to a given node it is left unchanged.\n   * Users should not expect a specific directionality. If a specific directionality is needed,\n   * transformDown or transformUp should be used.\n   *\n   * @param rule   the function used to transform this nodes children\n   * @param cond   a Lambda expression to prune tree traversals. If `cond.apply` returns false\n   *               on a TreeNode T, skips processing T and its subtree; otherwise, processes\n   *               T and its subtree recursively.\n   * @param ruleId is a unique Id for `rule` to prune unnecessary tree traversals. When it is\n   *               UnknownRuleId, no pruning happens. Otherwise, if `rule` (with id `ruleId`)\n   *               has been marked as in effective on a TreeNode T, skips processing T and its\n   *               subtree. Do not pass it if the rule is not purely functional and reads a\n   *               varying initial state for different invocations.\n*/\ndef transformWithPruning(cond: TreePatternBits => Boolean,\nruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n: BaseType = {\n    transformDownWithPruning(cond, ruleId)(rule)\n}\n\n/**\n   * Returns a copy of this node where `rule` has been recursively applied to it and all of its\n   * children (pre-order). When `rule` does not apply to a given node it is left unchanged.\n   *\n   * @param rule the function used to transform this nodes children\n*/\ndef transformDown(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformDownWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\n}\n\ndef transformDownWithPruning(cond: TreePatternBits => Boolean,\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n  : BaseType = {\n    /* More code */    \n}\n\ndef transformUp(rule: PartialFunction[BaseType, BaseType]): BaseType = {\n    transformUpWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\n}\n\ndef transformUpWithPruning(cond: TreePatternBits => Boolean,\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\n  : BaseType = {\n    /* More code */    \n}\n\n/* ... */\n")),(0,i.kt)("p",null,"D\u01b0\u1edbi \u0111\xe2y l\xe0 v\xed d\u1ee5 \u0111\u01a1n gi\u1ea3n v\u1ec1 s\u1eed d\u1ee5ng transform v\xe0 parttern matching \u0111\u1ec3 bi\u1ebfn \u0111\u1ed5i m\u1ed9t Treenode sang Treenode kh\xe1c"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'package com.tranlam\n\nimport org.apache.spark.sql.catalyst.expressions.{Add, BinaryOperator, Expression, IntegerLiteral, Literal, Multiply, Subtract, UnaryMinus}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nobject TestTransform {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName("test_transform").setMaster("local[*]")\n    val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n    val firstExpr: Expression = UnaryMinus(Multiply(Subtract(Literal(11), Literal(2)), Subtract(Literal(9), Literal(5))))\n    val transformed: Expression = firstExpr transformDown {\n      case BinaryOperator(l, r) => Add(l, r)\n      case IntegerLiteral(i) if i > 5 => Literal(1)\n      case IntegerLiteral(i) if i < 5 => Literal(0)\n    }\n    println(firstExpr) // -((11 - 2) * (9 - 5))\n    println(transformed) // -((1 + 0) + (1 + 5))\n    spark.sql(s"SELECT ${firstExpr.sql}").show()\n    spark.sql(s"SELECT ${transformed.sql}").show()\n  }\n}\n')),(0,i.kt)("p",null,"Trong v\xed d\u1ee5 tr\xean, h\xe0m transformDown \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng, n\xf3 \u0111i qua c\xe1c node c\u1ee7a 1 c\xe2y v\xe0 s\u1eed d\u1ee5ng parttern matching \u0111\u1ec3 tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 kh\xe1c. N\u1ebfu nh\u01b0 node \u0111\xf3 l\xe0 d\u1ea1ng binary operator nh\u01b0 Multiply, Subtract, n\xf3 s\u1ebd bi\u1ebfn \u0111\u1ed5i th\xe0nh ph\xe9p c\u1ed9ng Add. N\u1ebfu node l\xe0 h\u1eb1ng s\u1ed1 nguy\xean l\u1edbn h\u01a1n 5, n\xf3 s\u1ebd bi\u1ebfn \u0111\u1ed5i v\u1ec1 1, h\u1eb1ng s\u1ed1 b\xe9 h\u01a1n 5 s\u1ebd bi\u1ebfn \u0111\u1ed5i th\xe0nh 0, h\u1eb1ng s\u1ed1 b\u1eb1ng 5 th\xec gi\u1eef nguy\xean gi\xe1 tr\u1ecb."),(0,i.kt)("h3",{id:"3-c\xe1c-ho\u1ea1t-\u0111\u1ed9ng-c\u1ee7a-catalyst-trong-spark-sql"},"3. C\xe1c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Catalyst trong Spark SQL"),(0,i.kt)("p",null,"Spark Catalyst s\u1eed d\u1ee5ng c\xe1c ph\xe9p bi\u1ebfn \u0111\u1ed5i c\xe2y trong 4 phase ch\xednh: (1) ph\xe2n t\xedch logical plan \u0111\u1ec3 duy\u1ec7t c\xe1c relation trong plan \u0111\xf3, (2) logical plan optimization, (3) physical planning, (4) code generation \u0111\u1ec3 compile c\xe1c query th\xe0nh Java bytecode. "),(0,i.kt)("h4",{id:"31-parsing-v\xe0-analyzing"},"3.1. Parsing v\xe0 Analyzing"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spark catalyst parseing analyzing",src:t(5437).Z,width:"642",height:"519"})),(0,i.kt)("p",null,"\u1ede phase n\xe0y, c\xe1c Catalyst rule v\xe0 Catalog object s\u1ebd \u0111\u01b0\u1ee3c Spark SQL s\u1eed d\u1ee5ng \u0111\u1ec3 ki\u1ec3m tra xem c\xe1c relation trong c\xe2u query c\u1ee7a ch\xfang ta c\xf3 t\u1ed3n t\u1ea1i hay kh\xf4ng, c\xe1c thu\u1ed9c t\xednh c\u1ee7a relation nh\u01b0 c\u1ed9t, t\xean c\u1ed9t c\u0169ng \u0111\u01b0\u1ee3c ki\u1ec3m tra n\xf3 c\xf3 chu\u1ea9n hay kh\xf4ng, syntax \u0111\xe3 \u0111\xfang ch\u01b0a v\xe0 resolve c\xe1c relation \u0111\xf3."),(0,i.kt)("p",null,'V\xed d\u1ee5, nh\xecn v\xe0o plan c\xe2u query d\u01b0\u1edbi \u0111\xe2y, \u0111\u1ea7u ti\xean Spark SQL s\u1ebd bi\u1ebfn \u0111\u1ed5i query v\u1ec1 m\u1ed9t parsed tree \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 "unresolved logical plan" v\u1edbi c\xe1c thu\u1ed9c t\xednh v\xe0 datatypes ch\u01b0a x\xe1c \u0111\u1ecbnh, ch\u01b0a \u0111\u01b0\u1ee3c g\xe1n v\u1edbi m\u1ed9t table (ho\u1eb7c alias) c\u1ee5 th\u1ec3 n\xe0o. Sau \u0111\xf3 n\xf3 s\u1ebd'),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"T\xecm ki\u1ebfm relation theo t\xean t\u1eeb Catalog object."),(0,i.kt)("li",{parentName:"ul"},"Mapping c\xe1c thu\u1ed9c t\xednh nh\u01b0 c\u1ed9t c\u1ee7a input v\u1edbi c\xe1c relation \u0111\xe3 t\xecm \u0111\u01b0\u1ee3c."),(0,i.kt)("li",{parentName:"ul"},"Quy\u1ebft \u0111\u1ecbnh xem c\xe1c thu\u1ed9c t\xednh n\xe0o s\u1ebd tr\u1ecf t\u1edbi c\xf9ng gi\xe1 tr\u1ecb \u0111\u1ec3 g\xe1n cho n\xf3 m\u1ed9t unique ID (ph\u1ee5c v\u1ee5 m\u1ee5c \u0111\xedch v\u1ec1 sau \u0111\u1ec3 t\u1ed1i \u01b0u c\xe1c expressions nh\u01b0 ",(0,i.kt)("inlineCode",{parentName:"li"},"col = col"),")."),(0,i.kt)("li",{parentName:"ul"},"Cast c\xe1c expression v\u1ec1 datatype c\u1ee5 th\u1ec3 (v\xed d\u1ee5, ch\xfang ta s\u1ebd kh\xf4ng bi\u1ebft datatype tr\u1ea3 v\u1ec1 c\u1ee7a ",(0,i.kt)("inlineCode",{parentName:"li"},"col * 2")," cho t\u1edbi khi col \u0111\u01b0\u1ee3c resolved v\xe0 \u0111\u01b0\u1ee3c x\xe1c \u0111\u1ecbnh datatype).")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM test.describe_abc;\n\n== Parsed Logical Plan ==\n'Project [*]\n+- 'UnresolvedRelation [test, describe_abc], [], false\n\n== Analyzed Logical Plan ==\nid: int, name: string\nProject [id#5833, name#5834]\n+- SubqueryAlias spark_catalog.test.describe_abc\n   +- Relation test.describe_abc[id#5833,name#5834] parquet\n\n== Optimized Logical Plan ==\nRelation test.describe_abc[id#5833,name#5834] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet test.describe_abc[id#5833,name#5834] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://bigdataha/user/hive/warehouse/test.db/describe_abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string>\n")),(0,i.kt)("h4",{id:"32-logical-plan-optimizations"},"3.2. Logical plan optimizations"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spark LP optimization",src:t(7113).Z,width:"642",height:"519"})),(0,i.kt)("p",null,"Catalyst \xe1p d\u1ee5ng c\xe1c standard optimization rule cho logical plan \u0111\u01b0\u1ee3c \u0111\u01b0\u1ee3c ph\xe2n t\xedch \u1edf b\u01b0\u1edbc tr\xean, v\u1edbi c\xe1c d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cache. \u1ede \u0111\xe2y, cost-based optimization \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 sinh ra nhi\u1ec1u plans, v\xe0 sau \u0111\xf3 t\xednh to\xe1n cost cho t\u1eebng plan. Ph\u1ea7n n\xe0y bao g\u1ed3m c\xe1c rule nh\u01b0 "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Constant folding: lo\u1ea1i b\u1ecf c\xe1c expression t\xednh to\xe1n m\u1ed9t gi\xe1 tr\u1ecb m\xe0 c\xf3 ta c\xf3 th\u1ec3 x\xe1c \u0111\u1ecbnh tr\u01b0\u1edbc khi code ch\u1ea1y, v\xed d\u1ee5 nh\u01b0 ",(0,i.kt)("inlineCode",{parentName:"li"},"y = x * 2 * 2"),", compiler s\u1ebd kh\xf4ng sinh ra 2 multiply instruction m\xe0 n\xf3 s\u1ebd thay th\u1ebf tr\u01b0\u1edbc c\xe1c gi\xe1 tr\u1ecb c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c t\xednh to\xe1n ",(0,i.kt)("inlineCode",{parentName:"li"},"y = x * 4"),"."),(0,i.kt)("li",{parentName:"ul"},"Predicate pushdown: push down c\xe1c ph\u1ea7n c\u1ee7a query t\u1edbi n\u01a1i d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef, filter l\u01b0\u1ee3ng l\u1edbn d\u1eef li\u1ec7u, c\u1ea3i thi\u1ec7n network traffic."),(0,i.kt)("li",{parentName:"ul"},"Projection: ch\u1ecdn \u0111\xfang c\xe1c c\u1ed9t \u0111\u01b0\u1ee3c select, s\u1ed1 c\u1ed9t \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb storage t\u1edbi Spark \xedt h\u01a1n, ph\u1ee5c v\u1ee5 \u0111\u1ecdc c\xe1c columnar storage nhanh h\u01a1n v\xe0 ch\u1ec9 \u0111\u1ecdc c\xe1c c\u1ed9t c\u1ea7n thi\u1ebft."),(0,i.kt)("li",{parentName:"ul"},"Boolean expression simplification: v\xed d\u1ee5, A and (A or B) = A(A+B) = A.A + A.B = A + A.B = A.(1+B) = A."),(0,i.kt)("li",{parentName:"ul"},"V\xe0 nhi\u1ec1u c\xe1c rule kh\xe1c... ")),(0,i.kt)("p",null,"Catalyst optimizer c\u1ee7a Spark s\u1ebd bao g\u1ed3m c\xe1c batch of rule, m\u1ed9t s\u1ed1 rule c\xf3 th\u1ec3 t\u1ed3n t\u1ea1i trong nhi\u1ec1u batch. Th\u01b0\u1eddng c\xe1c batch rule n\xe0y s\u1ebd \u0111\u01b0\u1ee3c ch\u1ea1y 1 l\u1ea7n tr\xean plan \u0111\xf3, tuy nhi\xean, c\xf3 m\u1ed9t s\u1ed1 batch s\u1ebd ch\u1ea1y l\u1eb7p \u0111i l\u1eb7p l\u1ea1i cho \u0111\u1ebfn m\u1ed9t s\u1ed1 l\u1ea7n duy\u1ec7t nh\u1ea5t \u0111\u1ecbnh."),(0,i.kt)("h4",{id:"33-physical-planning"},"3.3. Physical planning"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spark PP planning",src:t(8632).Z,width:"642",height:"519"})),(0,i.kt)("p",null,"Spark SQL nh\u1eadn v\xe0o logical plan v\xe0 sinh ra m\u1ed9t ho\u1eb7c nhi\u1ec1u physical plan, sau \u0111\xf3 n\xf3 s\u1ebd ch\u1ecdn physical plan ph\xf9 h\u1ee3p d\u1ef1a v\xe0o c\xe1c cost model. C\xe1c cost model th\u01b0\u1eddng d\u1ef1a v\xe0o c\xe1c ch\u1ec9 s\u1ed1 th\u1ed1ng k\xea c\u1ee7a c\xe1c relation, \u0111\u1ecbnh l\u01b0\u1ee3ng c\xe1c th\u1ed1ng k\xea ch\u1ea3y v\xe0o m\u1ed9t node trong TreeNode nh\u01b0"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"K\xedch th\u01b0\u1edbc d\u1eef li\u1ec7u ch\u1ea3y v\xe0o node."),(0,i.kt)("li",{parentName:"ul"},"L\u01b0\u1ee3ng b\u1ea3n ghi t\u1eebng b\u1ea3ng."),(0,i.kt)("li",{parentName:"ul"},"C\xe1c ch\u1ec9 s\u1ed1 th\u1ed1ng k\xea li\xean quan t\u1edbi c\u1ed9t nh\u01b0: l\u01b0\u1ee3ng gi\xe1 tr\u1ecb ph\xe2n bi\u1ec7t, gi\xe1 tr\u1ecb l\u1edbn nh\u1ea5t gi\xe1 tr\u1ecb nh\u1ecf nh\u1ea5t, gi\xe1 tr\u1ecb \u0111\u1ed9 d\xe0i trung b\xecnh v\xe0 l\u1edbn nh\u1ea5t c\u1ee7a c\u1ed9t, histogram c\xe1c gi\xe1 tr\u1ecb c\u1ee7a c\u1ed9t,...")),(0,i.kt)("p",null,"M\u1ed9t s\u1ed1 h\u01b0\u1edbng ti\u1ebfp c\u1eadn c\u1ee7a Spark SQL cho ph\u1ea7n cost model n\xe0y"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Size-only approach: ch\u1ec9 s\u1eed d\u1ee5ng th\u1ed1ng k\xea v\u1ec1 k\xedch th\u01b0\u1edbc v\u1eadt l\xfd c\u1ee7a d\u1eef li\u1ec7u ch\u1ea3y v\xe0o node, c\xf3 th\u1ec3 th\xeam ch\u1ec9 s\u1ed1 s\u1ed1 b\u1ea3n ghi trong m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p."),(0,i.kt)("li",{parentName:"ul"},"Cost-based approach: th\u1ed1ng k\xea c\xe1c th\xf4ng tin li\xean quan \u0111\u1ebfn m\u1ee9c c\u1ed9t cho c\xe1c node Aggregate, Filter, Join, Project (l\u01b0u \xfd, cost-based approach ch\u1ec9 \u0111\u01b0\u1ee3c \xe1p d\u1ee5ng cho c\xe1c node lo\u1ea1i n\xe0y, v\u1edbi nh\u1eefng node lo\u1ea1i kh\xe1c, n\xf3 s\u1ebd tr\u1edf v\u1ec1 s\u1eed d\u1ee5ng size-only approach), c\u1ea3i thi\u1ec7n k\xedch th\u01b0\u1edbc v\xe0 l\u01b0\u1ee3ng b\u1ea3n ghi cho c\xe1c node \u0111\xf3.")),(0,i.kt)("p",null,"Cost-based approach \u0111\u01b0\u1ee3c ch\u1ecdn n\u1ebfu ta set ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.sql.cbo.enabled=true"),". B\xean c\u1ea1nh \u0111\xf3, c\xe1c th\u1ed1ng k\xea v\u1ec1 b\u1ea3ng v\xe0 c\u1ed9t c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c thu th\u1eadp \u0111\u1ec3 Spark c\xf3 th\u1ec3 d\u1ef1a v\xe0o \u0111\xf3 t\xednh to\xe1n, b\u1eb1ng vi\u1ec7c ch\u1ea1y c\xe1c l\u1ec7nh ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html"},"ANALYZE"))),(0,i.kt)("h4",{id:"34-code-generation"},"3.4. Code generation"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spark codegen",src:t(4059).Z,width:"642",height:"519"})),(0,i.kt)("p",null,"Sau khi \u0111\xe3 l\u1ef1a ch\u1ecdn \u0111\u01b0\u1ee3c physical plan ph\xf9 h\u1ee3p \u0111\u1ec3 ch\u1ea1y, Catalyst s\u1ebd compile m\u1ed9t c\xe2y c\xe1c plans h\u1ed7 tr\u1ee3 codegen th\xe0nh m\u1ed9t h\xe0m Java duy nh\u1ea5t, v\u1ec1 Java bytecode \u0111\u1ec3 ch\u1ea1y tr\xean driver v\xe0 c\xe1c executor. Ph\u1ea7n codegen n\xe0y c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 ch\u1ea1y r\u1ea5t nhi\u1ec1u khi m\xe0 Spark SQL th\u01b0\u1eddng ho\u1ea1t \u0111\u1ed9ng tr\xean c\xe1c in-memory dataset, vi\u1ec7c x\u1eed l\xfd d\u1eef li\u1ec7u th\u01b0\u1eddng g\u1eafn ch\u1eb7t v\u1edbi CPU. Catalyst d\u1ef1a v\xe0o m\u1ed9t t\xednh n\u0103ng c\u1ee7a Scala l\xe0 quasiquotes \u0111\u1ec3 th\u1ef1c hi\u1ec7n \u0111\u01a1n gi\u1ea3n ho\xe1 ph\u1ea7n codegen n\xe0y (quasiquotes cho ph\xe9p x\xe2y d\u1ef1ng c\xe1c abstract syntax tree (ASTs), sau \u0111\xf3 s\u1ebd input v\xe0o Scala compiler \u0111\u1ec3 t\u1ea1o ra bytecode)."),(0,i.kt)("h3",{id:"4-spark-session-extension"},"4. Spark session extension"),(0,i.kt)("p",null,"Spark session extension l\xe0 m\u1ed9t t\xednh n\u0103ng m\u1edf r\u1ed9ng c\u1ee7a Spark gi\xfap cho ta c\xf3 th\u1ec3 custom c\xe1c ph\u1ea7n trong Catalyst optimizer \u0111\u1ec3 n\xf3 ho\u1ea1t \u0111\u1ed9ng theo t\u1eebng ng\u1eef c\u1ea3nh c\u1ee7a ta."),(0,i.kt)("h4",{id:"41-custom-parser-rule"},"4.1. Custom parser rule"),(0,i.kt)("p",null,"Nh\u01b0 \u1edf tr\xean tr\xecnh b\xe0y, ban \u0111\u1ea7u query c\u1ee7a ta s\u1ebd ph\u1ea3i \u0111i qua b\u1ed9 parsing \u0111\u1ec3 ki\u1ec3m tra t\xednh h\u1ee3p l\u1ec7 c\u1ee7a query. Spark cung c\u1ea5p m\u1ed9t interface cho ta c\xf3 th\u1ec3 implement \u1edf stage n\xe0y l\xe0 ",(0,i.kt)("inlineCode",{parentName:"p"},"ParserInterface")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'package org.apache.spark.sql.catalyst.parser\n\n@DeveloperApi\ntrait ParserInterface {\n  @throws[ParseException]("Text cannot be parsed to a LogicalPlan")\n  def parsePlan(sqlText: String): LogicalPlan\n\n  @throws[ParseException]("Text cannot be parsed to an Expression")\n  def parseExpression(sqlText: String): Expression\n\n  @throws[ParseException]("Text cannot be parsed to a TableIdentifier")\n  def parseTableIdentifier(sqlText: String): TableIdentifier\n\n  @throws[ParseException]("Text cannot be parsed to a FunctionIdentifier")\n  def parseFunctionIdentifier(sqlText: String): FunctionIdentifier\n\n  @throws[ParseException]("Text cannot be parsed to a multi-part identifier")\n  def parseMultipartIdentifier(sqlText: String): Seq[String]\n\n  @throws[ParseException]("Text cannot be parsed to a schema")\n  def parseTableSchema(sqlText: String): StructType\n\n  @throws[ParseException]("Text cannot be parsed to a DataType")\n  def parseDataType(sqlText: String): DataType\n\n  @throws[ParseException]("Text cannot be parsed to a LogicalPlan")\n  def parseQuery(sqlText: String): LogicalPlan\n}\n')),(0,i.kt)("p",null,"Ch\xfang ta s\u1ebd implement interface \u0111\xf3 v\xe0 inject rule n\xe0y v\xe0o job Spark nh\u01b0 sau"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class CustomerParserRule(sparkSession: SparkSession, delegateParser: ParserInterface) extends ParserInterface {\n  /* Overwrite those methods here */\n}\n\nval customerParserRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions => {\n  extensionBuilder.injectParser(CustomerParserRule)\n}\n")),(0,i.kt)("h4",{id:"42-custom-analyzer-rule"},"4.2. Custom analyzer rule"),(0,i.kt)("p",null,"Analyzer rule bao g\u1ed3m m\u1ed9t s\u1ed1 lo\u1ea1i rule nh\u01b0 resolution rule, check rule. C\xe1c rule n\xe0y \u0111\u01b0\u1ee3c inject th\xf4ng qua c\xe1c h\xe0m"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"injectResolutionRule"),": inject c\xe1c rule c\u1ee7a ta cho resolution phase."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"injectPostHocResolutionRule"),": ch\u1ea1y c\xe1c rule c\u1ee7a ta sau resolution phase."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"injectCheckRule"),": th\xeam c\xe1c rule \u0111\u1ec3 ki\u1ec3m tra m\u1ed9t s\u1ed1 logic c\u1ee7a c\xe1c logical plan, v\xed d\u1ee5 nh\u01b0 ta mu\u1ed1n ki\u1ec3m tra c\xe1c logic nghi\u1ec7p v\u1ee5, ho\u1eb7c ki\u1ec3m tra xem c\xe1c rule n\xe0o \u0111\xe3 ch\u1ea1y xong,...")),(0,i.kt)("p",null,"\u0110\u1ec3 inject resolution rule, ta k\u1ebf th\u1eeba m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng c\u1ee7a Spark ",(0,i.kt)("inlineCode",{parentName:"p"},"Rule[LogicalPlan]")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class CustomAnalyzerResolutionRule(sparkSession: SparkSession) extends Rule[LogicalPlan] {\n  override def apply(plan: LogicalPlan): LogicalPlan = {\n    /* Code for resolution rule */\n  }\n}\n\nval customAnalyzerResolutionRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\n  extensionBuilder.injectResolutionRule(CustomAnalyzerResolutionRule)\n}\n")),(0,i.kt)("p",null,"\u0110\u1ec3 inject check rule, ta k\u1ebf th\u1eeba l\u1edbp h\xe0m class ",(0,i.kt)("inlineCode",{parentName:"p"},"Function1[LogicalPlan, Unit]")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class CustomAnalyzerCheckRule(sparkSession: SparkSession) extends (LogicalPlan => Unit) {\n  override def apply(plan: LogicalPlan): Unit = {\n    /* Code for check rule */\n  }\n}\n\nval customAnalyzerCheckRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\n  extensionBuilder.injectCheckRule(CustomAnalyzerCheckRule)\n}\n")),(0,i.kt)("h4",{id:"43-custom-optimization"},"4.3. Custom optimization"),(0,i.kt)("p",null,"\u0110\u1ec3 custom ph\u1ea7n logical plan optimization, ta s\u1ebd k\u1ebf th\u1eeba l\u1edbp tr\u1eebu t\u01b0\u1ee3ng ",(0,i.kt)("inlineCode",{parentName:"p"},"Rule[LogicalPlan]")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class CustomOptimizer(sparkSession: SparkSession) extends Rule[LogicalPlan] {\n  override def apply(plan: LogicalPlan): LogicalPlan = {\n    /* Code for custom logical optimier */\n  }\n}\n\nval customOptimizerFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\n  extensionBuilder.injectOptimizerRule(CustomOptimizer)\n}\n")),(0,i.kt)("h4",{id:"44-custom-physical-planning"},"4.4. Custom physical planning"),(0,i.kt)("p",null,"\u0110\u1ec3 c\u1ea5u h\xecnh ph\u1ea7n chi\u1ebfn thu\u1eadt ch\u1ea1y cho Spark Catalyst optimizer, ch\xfang ta k\u1ebf th\u1eeba l\u1edbp tr\u1eebu t\u01b0\u1ee3ng ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkStrategy")," v\xe0 implement h\xe0m ",(0,i.kt)("inlineCode",{parentName:"p"},"apply")," c\u1ee7a class \u0111\xf3"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class CustomStrategy(sparkSession: SparkSession) extends SparkStrategy {\n  override def apply(plan: LogicalPlan): Seq[SparkPlan] = {\n    /* Code for custom spark strategy/physical planning */\n  }\n}\n\nval customStrategyFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\n  extensionBuilder.injectPlannerStrategy(CustomStrategy)\n}\n")),(0,i.kt)("h4",{id:"45-v\xed-d\u1ee5-code-c\u1ea5u-h\xecnh-ph\u1ea7n-logical-plan-optimization-trong-catalyst-optimizer"},"4.5. V\xed d\u1ee5 code c\u1ea5u h\xecnh ph\u1ea7n logical plan optimization trong Catalyst optimizer"),(0,i.kt)("p",null,"Ph\u1ea7n n\xe0y m\xecnh s\u1ebd code m\u1ed9t v\xed d\u1ee5 v\u1ec1 thay \u0111\u1ed5i logical plan optimization b\u1eb1ng Spark extension. M\u1ed9t extension \u0111\u01a1n gi\u1ea3n c\xf3 code nh\u01b0 d\u01b0\u1edbi \u0111\xe2y"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"/* class CustomProjectFilterExtension ======================================= */\npackage extensions\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.catalyst.plans.logical._\nimport org.apache.spark.sql.catalyst.rules.Rule\n// create an extension that \ncase class CustomProjectFilterExtension(spark: SparkSession) extends Rule[LogicalPlan] {\n  override def apply(plan: LogicalPlan): LogicalPlan = {\n    val fixedPlan = plan transformDown {\n      case Project(expression, Filter(condition, child)) =>\n          Filter(condition, child)\n    }\n    fixedPlan\n  }\n}\n\n/* class AllExtensions ======================================= */\npackage extensions\nimport org.apache.spark.sql.SparkSessionExtensions\n// inject the extension to SparkSessionExtensions\nclass AllExtensions extends (SparkSessionExtensions => Unit) {\n  override def apply(ext: SparkSessionExtensions): Unit = {\n    ext.injectOptimizerRule(CustomProjectFilterExtension)\n  }\n}\n")),(0,i.kt)("p",null,"Class ",(0,i.kt)("inlineCode",{parentName:"p"},"CustomProjectFilterExtension")," \u1edf tr\xean bi\u1ebfn \u0111\u1ed5i ph\xe9p Filter (l\u1ecdc row), Project (ch\u1ecdn c\u1ed9t trong l\xfac scan file) th\xe0nh ch\u1ec9 c\xf2n ph\xe9p Filter. Khi \u0111\xf3, m\u1eb7c d\xf9 ta \u0111\xe3 ch\u1ecdn c\u1ed9t nh\u01b0ng n\xf3 v\u1eabn scan t\u1ea5t c\u1ea3 c\xe1c c\u1ed9t trong file. "),(0,i.kt)("p",null,"Ta compile project"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"# compile jar file\nmvn clean package && mvn dependency:copy-dependencies\n")),(0,i.kt)("h5",{id:"451-khi-kh\xf4ng-apply-extension"},"4.5.1. Khi kh\xf4ng apply extension"),(0,i.kt)("p",null,"Ta kh\u1edfi t\u1ea1o ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-shell")," kh\xf4ng truy\u1ec1n extension"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"# kh\u1edfi t\u1ea1o spark-shell\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr ' ' ','),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar\n\n# ki\u1ec3m tra spark.sql.extensions\nscala> spark.conf.get(\"spark.sql.extensions\")\nres0: String = null\n\n# explain m\u1ed9t query c\xf3 Filter v\xe0 Project\nscala> spark.sql(\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel='Resort Hotel') a\").explain(extended = true)\n\n== Parsed Logical Plan ==\n'Project ['hotel, 'is_canceled]\n+- 'SubqueryAlias a\n   +- 'Project [*]\n      +- 'Filter ('hotel = Resort Hotel)\n         +- 'UnresolvedRelation [test, hotel_bookings], [], false\n\n== Analyzed Logical Plan ==\nhotel: string, is_canceled: bigint\nProject [hotel#0, is_canceled#1L]\n+- SubqueryAlias a\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\n      +- Filter (hotel#0 = Resort Hotel)\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\n\n== Optimized Logical Plan ==\nProject [hotel#0, is_canceled#1L]\n+- Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\n   +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\n\n== Physical Plan ==\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\n+- *(1) ColumnarToRow\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint>\n")),(0,i.kt)("p",null,"Ta th\u1ea5y r\u1eb1ng ",(0,i.kt)("inlineCode",{parentName:"p"},"Optimized Logical Plan")," c\xf3 c\u1ea3 ph\xe9p Project v\xe0 Filter, do ta filter ",(0,i.kt)("inlineCode",{parentName:"p"},"WHERE hotel='Resort Hotel'")," v\xe0 project ",(0,i.kt)("inlineCode",{parentName:"p"},"SELECT hotel, is_canceled"),". T\u1edbi physical plan, n\xf3 c\u0169ng ch\u1ec9 scan 2 c\u1ed9t ",(0,i.kt)("inlineCode",{parentName:"p"},"FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L]"),"."),(0,i.kt)("h5",{id:"452-khi-c\xf3-extension"},"4.5.2. Khi c\xf3 extension"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"# kh\u1edfi t\u1ea1o spark-shell v\u1edbi extension\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr ' ' ','),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar --conf spark.sql.extensions=extensions.AllExtensions\n\n# ki\u1ec3m tra spark.sql.extensions\nscala> spark.conf.get(\"spark.sql.extensions\")\nres0: String = extensions.AllExtensions\n\n# explain m\u1ed9t query c\xf3 Filter v\xe0 Project gi\u1ed1ng h\u1ec7t c\xe2u b\xean tr\xean\nscala> spark.sql(\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel='Resort Hotel') a\").explain(extended = true)\n\n== Parsed Logical Plan ==\n'Project ['hotel, 'is_canceled]\n+- 'SubqueryAlias a\n   +- 'Project [*]\n      +- 'Filter ('hotel = Resort Hotel)\n         +- 'UnresolvedRelation [test, hotel_bookings], [], false\n\n== Analyzed Logical Plan ==\nhotel: string, is_canceled: bigint\nProject [hotel#0, is_canceled#1L]\n+- SubqueryAlias a\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\n      +- Filter (hotel#0 = Resort Hotel)\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\n+- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\n\n== Physical Plan ==\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\n+- *(1) ColumnarToRow\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint,lead_time:bigint,arrival_date_year:bigint,arrival_date_mon...\n")),(0,i.kt)("p",null,"Khi n\xe0y, ",(0,i.kt)("inlineCode",{parentName:"p"},"Optimized Logical Plan")," kh\xf4ng c\xf2n ph\xe9p Project n\u1eefa, m\xe0 ch\u1ec9 c\xf2n ph\xe9p Filter, l\xe0m cho l\xfac t\u1edbi b\u01b0\u1edbc physical plan, n\xf3 scan t\u1ea5t c\u1ea3 c\xe1c c\u1ed9t trong b\u1ea3ng ",(0,i.kt)("inlineCode",{parentName:"p"},"FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields]"),"."),(0,i.kt)("p",null,"\u1ede tr\xean m\xecnh \u0111\xe3 tr\xecnh b\xe0y c\u1ee5 th\u1ec3 v\u1ec1 c\xe1c th\xe0nh ph\u1ea7n c\u1ee7a Spark Catalyst optimizer v\xe0 c\xe1ch vi\u1ebft c\xe1c spark session extension \u0111\u1ec3 can thi\u1ec7p thay \u0111\u1ed5i c\xe1c plan c\u1ee7a Catalyst, c\u0169ng \u0111\xe3 c\xf3 v\xed d\u1ee5 code c\u1ee5 th\u1ec3 \u0111\u1ec3 ch\u1ee9ng minh \u0111i\u1ec1u n\xe0y. \u1ede b\xe0i vi\u1ebft sau, m\xecnh s\u1ebd tr\xecnh b\xe0y th\xeam m\u1ed9t ph\u1ea7n n\u1eefa l\xe0 t\xednh n\u0103ng m\u1edbi trong Spark 3.0, \u0111\xf3 l\xe0 Spark Adaptive Query Execution, t\xednh n\u0103ng c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 job Spark \u1edf runtime."),(0,i.kt)("h3",{id:"5-t\xe0i-li\u1ec7u-tham-kh\u1ea3o"},"5. T\xe0i li\u1ec7u tham kh\u1ea3o"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html"},"Deep Dive into Spark SQL's Catalyst Optimizer")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://www.unraveldata.com/resources/catalyst-analyst-a-deep-dive-into-sparks-optimizer/"},"Spark Catalyst Pipeline: A Deep Dive Into Spark\u2019s Optimizer")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://medium.com/@pratikbarhate/extending-apache-spark-catalyst-for-custom-optimizations-9b491efdd24f"},"Extending Apache Spark Catalyst for Custom Optimizations")))}h.isMDXComponent=!0},7113:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/catalyst-pipeline-LP-optimization-f6ab83bcb7f4fe16849cb54ee255ac1b.PNG"},8632:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/catalyst-pipeline-PP-planning-965d236e9af5638f8e5b6a15278a4331.PNG"},4059:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/catalyst-pipeline-codegen-4303b28d4eb24d57d52bc29fca45a337.PNG"},5437:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/catalyst-pipeline-parsing-analyzing-434229ee4a24f7a81d885d239a4ea344.PNG"},2941:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/spark-catalyst-optimizer-03f0f2a4cb6e0c86499aaa51ba69b065.JPG"}}]);