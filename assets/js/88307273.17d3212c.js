"use strict";(self.webpackChunkkgajera_blog=self.webpackChunkkgajera_blog||[]).push([[9735],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>m});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var p=r.createContext({}),l=function(e){var t=r.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),h=l(a),g=n,m=h["".concat(p,".").concat(g)]||h[g]||u[g]||i;return a?r.createElement(m,o(o({ref:t},c),{},{components:a})):r.createElement(m,o({ref:t},c))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=g;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[h]="string"==typeof e?e:n,o[1]=s;for(var l=2;l<i;l++)o[l]=a[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}g.displayName="MDXCreateElement"},3264:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=a(7462),n=(a(7294),a(3905));const i={slug:"mini-spark3-authorizer-part-2",title:"Authorize Spark SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",authors:"tranlam",tags:["Bigdata","Spark","Ranger","Apache"],image:"./images/banner.PNG"},o=void 0,s={permalink:"/blogs/blog/mini-spark3-authorizer-part-2",editUrl:"https://github.com/lam1051999/blogs/edit/main/blog/2023-07-03-mini-spark3-authorizer-part-2/index.md",source:"@site/blog/2023-07-03-mini-spark3-authorizer-part-2/index.md",title:"Authorize Spark SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",description:"In the previous blog, I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of Spark Catalyst Optimizer in order to archive authorization in Spark SQL with Ranger.",date:"2023-07-03T00:00:00.000Z",formattedDate:"July 3, 2023",tags:[{label:"Bigdata",permalink:"/blogs/blog/tags/bigdata"},{label:"Spark",permalink:"/blogs/blog/tags/spark"},{label:"Ranger",permalink:"/blogs/blog/tags/ranger"},{label:"Apache",permalink:"/blogs/blog/tags/apache"}],readingTime:1.375,truncated:!0,authors:[{name:"Lam Tran",title:"Data Engineer",url:"https://github.com/lam1051999",imageURL:"https://github.com/lam1051999.png",key:"tranlam"}],frontMatter:{slug:"mini-spark3-authorizer-part-2",title:"Authorize Spark SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger",authors:"tranlam",tags:["Bigdata","Spark","Ranger","Apache"],image:"./images/banner.PNG"},nextItem:{title:"Authorize Spark SQL With Apache Ranger Part 1 - Ranger installation",permalink:"/blogs/blog/mini-spark3-authorizer-part-1"}},p={image:a(2150).Z,authorsImageUrls:[void 0]},l=[{value:"1. Spark installation",id:"1-spark-installation",level:3},{value:"2. Build a mini Spark Session Extension for Ranger authorization",id:"2-build-a-mini-spark-session-extension-for-ranger-authorization",level:3}],c={toc:l};function h(e){let{components:t,...i}=e;return(0,n.kt)("wrapper",(0,r.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"In the ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blogs/blog/mini-spark3-authorizer-part-1"},"previous blog")),", I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension"},"Spark Catalyst Optimizer"))," in order to archive authorization in Spark SQL with Ranger."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"banner",src:a(2150).Z,width:"751",height:"286"})),(0,n.kt)("h3",{id:"1-spark-installation"},"1. Spark installation"),(0,n.kt)("p",null,"Firstly, I will install Apache Spark 3.3.0 on my local machine. It is pretty easy with a few below steps."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Get Spark build with hadoop, you can find specific version here: https://archive.apache.org/dist/spark/\ncd ~\nwget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\ntar -xvf spark-3.3.0-bin-hadoop3.tgz\ncd spark-3.3.0-bin-hadoop3\n\n# Configure some environment variables\nexport SPARK_HOME=~/spark-3.3.0-bin-hadoop3\nexport PATH=$PATH:$SPARK_HOME/bin\n\n# Check if Spark is installed properly\nspark-shell\n")),(0,n.kt)("h3",{id:"2-build-a-mini-spark-session-extension-for-ranger-authorization"},"2. Build a mini Spark Session Extension for Ranger authorization"),(0,n.kt)("p",null,"The idea to make the authorizer is first inspired by ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/yaooqinn/spark-ranger"},"this GitHub repository"),". Currently, this repository has been archived and it is only compatible with Spark 2.4 and below, which not help our use case (we use Spark 3.3.0). After weeks trying multiple solutions but not have a result, I end up to customize the repository to work with Spark 3.3.0 myself."),(0,n.kt)("p",null,"Spark makes a huge update to migrate from 2.4 to 3.0 (see detail ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-24-to-30"},"here")," and ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/releases/spark-release-3-0-0.html"},"here"),") which create many new features. So we need to add more code and configure the project dependencies correctly in order not to be conflict with Spark 3 dependencies (since Ranger and Spark 3 use the same library ",(0,n.kt)("inlineCode",{parentName:"p"},"jersey")," to build RESTful Web Services, Ranger use 1.x and Spark 3 use 2.x)."),(0,n.kt)("p",null,"The idea is to create a Spark Session Extension to customize the logical plan optimization phase of Spark Catalyst Optimizer, and since logical plan is represented as ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension#1-treenode"},"TreeNode"))))}h.isMDXComponent=!0},2150:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/banner-f7f67c4c18838de8d49230557182b09d.PNG"}}]);