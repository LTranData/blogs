"use strict";(self.webpackChunklamtran_blog=self.webpackChunklamtran_blog||[]).push([[765],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>d});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),c=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},p=function(e){var a=c(e.components);return r.createElement(l.Provider,{value:a},e.children)},f="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},u=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),f=c(t),u=n,d=f["".concat(l,".").concat(u)]||f[u]||m[u]||o;return t?r.createElement(d,s(s({ref:a},p),{},{components:t})):r.createElement(d,s({ref:a},p))}));function d(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,s=new Array(o);s[0]=u;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i[f]="string"==typeof e?e:n,s[1]=i;for(var c=2;c<o;c++)s[c]=t[c];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},623:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>f,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var r=t(7462),n=(t(7294),t(3905));const o={slug:"cloud-native-data-platform/",title:"Cloud Native Data Platform",description:"Cloud Native Data Platform",authors:"tranlam",tags:["Kubernetes","Cloud","Data Platform"],image:"./images/architecture.jpg"},s=void 0,i={permalink:"/blog/cloud-native-data-platform/",editUrl:"https://github.com/LTranData/blogs/edit/main/blog/2025-02-20-cloud-native-data-platform/index.md",source:"@site/blog/2025-02-20-cloud-native-data-platform/index.md",title:"Cloud Native Data Platform",description:"Cloud Native Data Platform",date:"2025-02-20T00:00:00.000Z",formattedDate:"February 20, 2025",tags:[{label:"Kubernetes",permalink:"/blog/tags/kubernetes"},{label:"Cloud",permalink:"/blog/tags/cloud"},{label:"Data Platform",permalink:"/blog/tags/data-platform"}],readingTime:3.91,truncated:!0,authors:[{name:"Lam Tran",title:"Data Engineer",url:"https://github.com/LTranData",imageURL:"https://github.com/LTranData.png",key:"tranlam"}],frontMatter:{slug:"cloud-native-data-platform/",title:"Cloud Native Data Platform",description:"Cloud Native Data Platform",authors:"tranlam",tags:["Kubernetes","Cloud","Data Platform"],image:"./images/architecture.jpg"},prevItem:{title:"db-schemachange",permalink:"/blog/db-schemachange/"},nextItem:{title:"Understanding Snowflake micro-partitions",permalink:"/blog/understanding-snowflake-micro-partitions/"}},l={image:t(4483).Z,authorsImageUrls:[void 0]},c=[{value:"Architecture",id:"architecture",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Install MinIO",id:"install-minio",level:3},{value:"Install Spark cluster",id:"install-spark-cluster",level:3},{value:"Install Airflow",id:"install-airflow",level:3},{value:"Install Hive metastore",id:"install-hive-metastore",level:3},{value:"Install Kafka",id:"install-kafka",level:3},{value:"Install sources",id:"install-sources",level:3},{value:"Install Kafka Connect",id:"install-kafka-connect",level:3},{value:"Install Trino",id:"install-trino",level:3},{value:"GitHub",id:"github",level:2}],p={toc:c};function f(e){let{components:a,...o}=e;return(0,n.kt)("wrapper",(0,r.Z)({},p,o,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"This platform leverages cloud-native technologies to build a flexible and efficient data pipeline. It supports various data ingestion, processing, and storage needs, enabling real-time and batch data analytics. The architecture is designed to handle structured, semi-structured, and unstructured data from diverse external sources."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"banner image",src:t(4483).Z,width:"1822",height:"1052"})),(0,n.kt)("h2",{id:"architecture"},"Architecture"),(0,n.kt)("p",null,"The platform is organized into distinct layers:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Ingestion Layer:")," Captures data from external sources using Debezium for streaming and Spark for batch."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Storage Layer:")," Stores streaming data in Kafka and streaming/batch data in MinIO, there is a Hive metastore on top of the object storage with Delta Lake tables."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Transformation Layer:")," Transforms and processes data using Spark, Trino, and dbt."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Provisioning Layer:")," Expose data for multiple downstream applications through various connectors."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Service Layer:")," Resource management, orchestration, data governance, and monitoring.")),(0,n.kt)("h2",{id:"getting-started"},"Getting Started"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Create Kubernetes namespace\nk create namespace data-platform\n")),(0,n.kt)("h3",{id:"install-minio"},"Install MinIO"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Add repository\nhelm repo add minio-operator https://operator.min.io\nhelm search repo minio-operator\nhelm repo update\n\n# Install client\nbrew install minio/stable/mc\n\n# Download operator config for reference\ncurl -sLo infra/services/minio/operator/values.yaml https://raw.githubusercontent.com/minio/operator/master/helm/operator/values.yaml\n\n# Download tenant config for reference\ncurl -sLo infra/services/minio/tenant/values.yaml https://raw.githubusercontent.com/minio/operator/master/helm/tenant/values.yaml\n# Make sure you configure `externalCertSecret` and `requestAutoCert` so that the server use Self-signed certificate instead of auto-generated certificate\n\n# Install server\nmake -f scripts/minio/Makefile generate-self-signed-cert\nmake -f scripts/minio/Makefile register-self-signed-cert\nmake -f scripts/minio/Makefile install\n\n# Port forward for MinIO service and set up alias, & is to run it in background\nk port-forward service/myminio-hl 9000 -n data-platform &\nk port-forward service/myminio-console 9443 -n data-platform &\n\n# Because we are using the Self-signed certificate, hence specify flag --insecure here\n# Alias for Tenant service\nmc alias set myminio https://localhost:9000 minio minio123 --insecure\n\n# Create a bucket\nmc mb myminio/mybucket --insecure\nmc mb myminio/hive-warehouse --insecure\n")),(0,n.kt)("h3",{id:"install-spark-cluster"},"Install Spark cluster"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Add repository\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\nhelm search repo spark-operator\nhelm repo update\n\n# Download Spark config for reference\ncurl -sLo infra/services/spark/values.yaml https://raw.githubusercontent.com/kubeflow/spark-operator/refs/heads/master/charts/spark-operator-chart/values.yaml\n\n# Install Spark Operator and build Spark application base image\nmake -f scripts/spark/Makefile install\nmake -f scripts/spark/Makefile build-spark-application-dockerfile\n\n# Running a Spark application to write file to MinIO with Delta Lake table format\nmake -f scripts/spark/Makefile build-spark-write-minio-dockerfile\nk apply -f pipeline/spark-write-minio/job.yaml\n\n# Release Docker images\nmake -f scripts/spark/Makefile release-docker-images\n\n# Go to https://localhost:9443/browser/mybucket/user_data to view data files\n")),(0,n.kt)("h3",{id:"install-airflow"},"Install Airflow"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"helm repo add apache-airflow https://airflow.apache.org\nhelm search repo apache-airflow\nhelm repo update\n\n# Download Airflow config for reference\ncurl -sLo infra/services/airflow/operator/values.yaml https://raw.githubusercontent.com/apache/airflow/refs/heads/main/chart/values.yaml\n\n# Install Airflow Operator\nmake -f scripts/airflow/Makefile build-custom-dockerfile\nmake -f scripts/airflow/Makefile release-docker-images\nmake -f scripts/airflow/Makefile install\n\n# Port forward for Airflow webserver\nk port-forward service/airflow-operator-webserver 8080 -n data-platform &\n")),(0,n.kt)("h3",{id:"install-hive-metastore"},"Install Hive metastore"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Download Postgres config for reference\ncurl -sLo infra/services/hive/database/values.yaml https://raw.githubusercontent.com/bitnami/charts/refs/heads/main/bitnami/postgresql/values.yaml\n\n# Install Hive metastore\nmake -f scripts/hive/Makefile build-metastore-custom-dockerfile\nmake -f scripts/hive/Makefile build-schematool-custom-dockerfile\nmake -f scripts/hive/Makefile release-docker-images\nmake -f scripts/hive/Makefile install\n\n# Port forward for Hive metastore database and thrift\nk port-forward service/hive-metastore-postgres-postgresql 5432 -n data-platform &\nk port-forward service/hive-metastore 9083 -n data-platform &\n\n# export HADOOP_ROOT_LOGGER=DEBUG,console && hadoop fs -ls s3a://hive-warehouse/\n# hadoop org.apache.hadoop.conf.Configuration\n# hdfs getconf -confKey [key]\n\n# Create a Hive table to test Hive metastore\nmake -f scripts/spark/Makefile build-spark-create-hive-table-dockerfile\nk apply -f pipeline/spark-create-hive-table/job.yaml\n")),(0,n.kt)("h3",{id:"install-kafka"},"Install Kafka"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Download Kafka config for reference\ncurl -sLo infra/services/kafka/operator/values.yaml https://raw.githubusercontent.com/bitnami/charts/refs/heads/main/bitnami/kafka/values.yaml\n\n# Install Kafka\nmake -f scripts/kafka/Makefile generate-self-signed-cert-keystore-truststore\nmake -f scripts/kafka/Makefile register-self-signed-cert-keystore-truststore\nmake -f scripts/kafka/Makefile install\n\n# Port forward for Kafka service\nk port-forward service/kafka-operator 9092 -n data-platform &\n\n# Create a client pod for accessing Kafka cluster data\nmake -f scripts/kafka/Makefile create-kafka-client-pod\nkafka-console-producer.sh \\\n    --producer.config /tmp/client.properties \\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\n    --topic test\n\n# Consume those messages\nkafka-console-consumer.sh \\\n    --consumer.config /tmp/client.properties \\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\n    --topic test \\\n    --from-beginning\n\n# List Kafka topics\nkafka-topics.sh \\\n    --command-config /tmp/client.properties \\\n    --bootstrap-server kafka-operator.data-platform.svc.cluster.local:9092 \\\n    --list\n")),(0,n.kt)("h3",{id:"install-sources"},"Install sources"),(0,n.kt)("p",null,"This is the installation of source systems with databases so that we can integrate the CDC from these to our datalake with Kafka Connector"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Install Postgres source\nmake -f scripts/sources/Makefile install-postgres\n\n# Port forward for Postgres source\nk port-forward service/postgres-source-postgresql 5432 -n data-platform &\n")),(0,n.kt)("h3",{id:"install-kafka-connect"},"Install Kafka Connect"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"make -f scripts/kafka-connect/Makefile install\n\n# Port forward for Kafka Connect REST endpoint\nk port-forward service/kafka-connect-operator-cp-kafka-connect 8083 -n data-platform &\n\n# List all installed plugins\ncurl -sS localhost:8083/connector-plugins\n\n# Create Postgres connector\nmake -f scripts/kafka-connect/Makefile create-postgres-connector\nmake -f scripts/kafka-connect/Makefile get-all-connectors\n")),(0,n.kt)("h3",{id:"install-trino"},"Install Trino"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# Add repository\nhelm repo add trino https://trinodb.github.io/charts\nhelm search repo trino\nhelm repo update\n\n# Download operator config for reference\ncurl -sLo infra/services/trino/operator/values.yaml https://raw.githubusercontent.com/trinodb/charts/refs/heads/main/charts/trino/values.yaml\n\n# Install Trino\nmake -f scripts/trino/Makefile build-trino-custom-dockerfile\nmake -f scripts/trino/Makefile release-docker-images\nmake -f scripts/trino/Makefile install\n\n# Port forward for Trino\nk port-forward service/trino-operator-trino 8089:8080 -n data-platform &\n")),(0,n.kt)("h2",{id:"github"},"GitHub"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://github.com/LTranData/data-platform"},"https://github.com/LTranData/data-platform"))))}f.isMDXComponent=!0},4483:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/architecture-4f516c425e6385f933a9e0642e949a64.jpg"}}]);