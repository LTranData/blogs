"use strict";(self.webpackChunklamtran_blog=self.webpackChunklamtran_blog||[]).push([[1635],{6404:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"how-is-memory-managed-in-spark","metadata":{"permalink":"/blogs/blog/how-is-memory-managed-in-spark","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2023-07-07-how-is-memory-managed-in-spark/index.md","source":"@site/blog/2023-07-07-how-is-memory-managed-in-spark/index.md","title":"How Is Memory Managed In Spark?","description":"Spark is an in-memory data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute tasks across multiple computers. Spark applications are memory heavy, hence, it is obvious that memory management plays a very important role in the whole system.","date":"2023-07-07T00:00:00.000Z","formattedDate":"July 7, 2023","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"}],"readingTime":7.97,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"how-is-memory-managed-in-spark","title":"How Is Memory Managed In Spark?","authors":"tranlam","tags":["Bigdata","Spark","Apache"],"image":"./images/banner.PNG"},"nextItem":{"title":"State Management In React","permalink":"/blogs/blog/state-management-react"}},"content":"Spark is an in-memory data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute tasks across multiple computers. Spark applications are memory heavy, hence, it is obvious that memory management plays a very important role in the whole system.\\n\\n![banner image](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Spark executor\\n\\n![cluster overview](./images/cluster-overview.PNG)\\n\\nSpark sends application code (defined by JAR or Python files passed to SparkContext) to each executor which will launch a JVM process for code execution. There are two types of memory in JVM.\\n\\n- **On-Heap memory:** refers to objects that will be present in the Java heap (and also subject to GC).\\n- **Off-Heap memory:** refers to (serialized) objects that are managed by EHCache, but stored outside the heap (and also not subject to GC).\\n\\nThe Off-Heap store is used to avoid the overhead of GC on a heap that is several Megabytes or Gigabytes large. It is slightly slower than the On-Heap memory, but still faster than the disk store.\\n\\n### 2. Spark memory manager\\n\\nBefore Spark 1.6, a simple scheme for memory management was adopted, which is Static Memory Manager (SMM). The size of Storage Memory and Execution Memory and other memory is fixed during application execution and it has been deprecated because of the lack of flexibility.\\n\\nFrom Spark 1.6+, Spark came up with Unified Memory Manager (UMM) with dynamic memory allocation, shared by storage and execution. Thus, when Execution Memory is not used, the Storage Memory can borrow all the available memory and vice versa, by calling acquireMemory() to make changes to memory pools. Therefore, UMM has lots of advantages compared to SMM.\\n\\n- Memory can be switched between Storage Memory and Execution Memory.\\n- When our application has no cache, all memory can be used by execution and thus prevent data spilling to disk.\\n- The application will be able to spend a minimum amount for Storage Memory for cached data and let the execution borrow the remaining.\\n- Dynamically improve performance without requiring the user to configure the memory portion for each manually.\\n\\n#### 2.1. On-Heap Memory\\n\\nThe size of the On-Heap memory can be configured either by passing `--executor-memory` to command lines or setting `spark.executor.memory` to the Spark application, in the same format as JVM memory strings with a size unit suffix (\\"k\\", \\"m\\", \\"g\\" or \\"t\\") (e.g. 512m, 2g). This amount of memory can be breakdown into the below types.\\n\\n![on heap overview](./images/on-heap-overview.PNG)\\n\\nDefault values of those configurations in Spark v3.3.0\\n\\n```bash\\nspark.executor.memory=1g\\nspark.memory.fraction=0.6\\nspark.memory.storageFraction=0.5\\nUsable Memory = On-Heap Memory - Reserved Memory\\n```\\n\\n##### 2.1.1. Researved memory\\n\\nReserved Memory is the memory reserved for the system and is used to store Spark\'s internal objects. Its size is hardcoded `private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024` in `org.apache.spark.memory.UnifiedMemoryManager`. If you want to make any modifications, you need to change the Spark source code and recompile it. Spark will require On-Heap memory greater or equal to 1.5 times of Reserved Memory or it will fail to initialize Spark session.\\n\\n```bash\\nspark-shell --conf spark.executor.memory=300m\\n\\njava.lang.IllegalArgumentException: Executor memory 314572800 must be at least 471859200. Please increase executor memory using the --executor-memory option or spark.executor.memory in Spark configuration.\\n\\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:229)\\n    ...\\n```\\n\\n##### 2.1.2. User memory\\n\\nUser Memory is the memory used to store user-defined data structures, any UDFs created by the user, the data needed for RDD conversion operations, etc. This memory segment is not managed by Spark and Spark will not be aware of/maintain it.\\n\\n```bash\\nUser Memory = Usable Memory * (1 - spark.memory.fraction)\\n```\\n\\n##### 2.1.3. Spark memory\\n\\n```bash\\nSpark Memory = Usable Memory * spark.memory.fraction\\n```\\n\\nThis memory pool is managed by Spark. Divided into two types of memory\\n\\n- **Execution Memory:** Used for processing tasks, storing objects required during the execution of the tasks. When this pool has no space left, it will spill to the disk. Execution Memory tends to remain shorter than Storage Memory since it will be evicted immediately after each operation, making space for the next ones.\\n\\n```bash\\nStorage Memory = Spark Memory * (1 - spark.memory.storageFraction)\\n```\\n\\n- **Storage Memory:** used for storing the cached data (data with persist option MEMORY in it), broadcast variables, and data deserialization. When this region is full, cache data will be either written to disk or recomputed based on configuration. Spark also clears space for new cache requests by eliminating the old cache objects with the Least Recently Used (LRU) mechanism.\\n\\n```bash\\nStorage Memory = Spark Memory * spark.memory.storageFraction\\n```\\n\\n##### 2.1.3. Dynamic memory allocation between Storage Memory and Execution Memory\\n\\n- Storage Memory can borrow space from Execution Memory only if blocks are not used in Execution Memory.\\n- Execution Memory can also borrow space from Storage Memory if blocks are not used in Storage Memory.\\n- If blocks from Execution Memory are used by Storage Memory, and Execution needs more memory, it can forcefully evict the excess blocks occupied by Storage Memory.\\n- If blocks from Storage Memory are used by Execution Memory and Storage needs more memory, it cannot forcefully evict the excess blocks occupied by Execution Memory, it will end up having less memory area. It will wait until Spark releases the excess blocks stored by Execution Memory and then occupies them.\\n\\n#### 2.2. Off-Heap Memory\\n\\nMost Spark operations happened entirely in On-Heap memory and utilize the mighty help of GC that sometimes can cause GC overhead. To minimize this effect, Spark introduces the Off-Heap memory for certain operations, which will reduce the impact of GC in the application.\\n\\nOff-Heap memory means allocating memory objects (serialized to a byte array) to memory outside the heap of the JVM, which is directly managed by the operating system. This memory does not bound to GC but calls the Java API (sun.misc.Unsafe) for unsafe operations such as C which uses malloc() to use operating system memory.\\n\\nData accessing in this region can be slightly slower than accessing the On-Heap memory, but still faster than disk, and the user has to manually deal with managing the allocated memory. Data on Off-Heap memory can still be persisted even when the executor getting killed (data cache on On-Heap memory would be gone).\\n\\nThis memory region is disabled by default but can be enabled by setting these configurations.\\n\\n```bash\\nspark.memory.offHeap.enabled = true (false by default)\\nspark.memory.offHeap.size = ?g (0 by default)\\n```\\n\\nOff-Heap memory includes only Storage Memory and Execution Memory, which will be distributed in the following manner.\\n\\n![off heap overview](./images/off-heap-overview.PNG)\\n\\nTherefore, the total memory of Storage Memory or Execution Memory will be the sum of each in both On-Heap and Off-Heap memories.\\n\\n### 3. Spark memory calculation example\\n\\nDespite we pass `spark.executor.memory` to On-Heap memory, the maximum amount of memory that the JVM will attempt to use will be slightly smaller than `spark.executor.memory`, which will be calculated with the below Java program.\\n\\n```java\\npublic class Helper {\\n    public static void main(String[] args) {\\n        long maxMem = Runtime.getRuntime().maxMemory();\\n        System.out.println(maxMem);\\n    }\\n}\\n```\\n\\n```bash\\nspark.executor.memory=1024 (as 1GB in MB)\\njava -Xmx${spark_executor_memory}m -cp target/calculate-1.0-SNAPSHOT.jar Helper\\n954728448 (which is 0.88916015625 GB)\\n```\\n\\nA small Python program to calculate the memory of each memory category, with the help of the Java code above.\\n\\n```python\\n# MB will be the smallest unit\\nfrom distutils.util import strtobool\\nimport subprocess\\n\\ndef get_valid_input(message, f, error_message):\\n    amount = None\\n    while amount is None:\\n        try:\\n            amount = f(input(message))\\n            return amount\\n        except ValueError:\\n            print(error_message)\\n\\ndef get_jvm_max_mem(mem):\\n    command = [\\"java\\", f\\"-Xmx{int(mem)}m\\", \\"-cp\\", \\"calculate/target/calculate-1.0-SNAPSHOT.jar\\", \\"Helper\\"]\\n    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\\n    text = p.stdout.read()\\n    ret = p.wait()\\n    if ret == 0:\\n        return float(text.decode(\\"utf-8\\"))\\n\\nif __name__ == \\"__main__\\":\\n    RESERVED_SYSTEM_MEMORY = 300\\n    SPARK_MEMORY_FRACTION=0.6\\n    SPARK_MEMORY_STORAGEFRACTION=0.5\\n    GB_TO_MB_RATE = 1024\\n    spark_executor_memory = get_valid_input(\\"Amount of spark.executor.memory (in GB): \\", float, \\"Invalid value for spark.executor.memory, must be a number\\") * GB_TO_MB_RATE\\n    spark_executor_memory = get_jvm_max_mem(spark_executor_memory) / pow(GB_TO_MB_RATE, 2)\\n    spark_memory_offheap_enabled = get_valid_input(\\"Option spark.memory.offHeap.enabled: \\", strtobool, \\"Invalid value for spark.memory.offHeap.enabled, must be a boolean string (true, false, True, False,...)\\")\\n    if spark_memory_offheap_enabled:\\n        spark_memory_offheap_size = get_valid_input(\\"Amount of spark.memory.offHeap.size (in GB): \\", float, \\"Invalid value for spark.memory.offHeap.size, must be a number\\") * GB_TO_MB_RATE\\n    on_heap_user_memory = (spark_executor_memory - RESERVED_SYSTEM_MEMORY) * (1 - SPARK_MEMORY_FRACTION)\\n    on_heap_spark_memory = (spark_executor_memory - RESERVED_SYSTEM_MEMORY) * SPARK_MEMORY_FRACTION\\n    on_heap_spark_storage_memory = on_heap_spark_memory * SPARK_MEMORY_STORAGEFRACTION\\n    on_heap_spark_execution_memory = on_heap_spark_memory * (1 - SPARK_MEMORY_STORAGEFRACTION)\\n    total_spark_memory = on_heap_spark_memory\\n    print(\\"\\\\n\\")\\n    print(f\\"------------------ On-Heap Memory: {spark_executor_memory} MB ------------------\\")\\n    print(f\\"Researved Memory: {RESERVED_SYSTEM_MEMORY} MB\\")\\n    print(f\\"User Memory: {on_heap_user_memory} MB\\")\\n    print(f\\"Spark Memory: {on_heap_spark_memory} MB\\")\\n    print(f\\"\\\\tStorage Memory: {on_heap_spark_storage_memory} MB\\")\\n    print(f\\"\\\\tExecution Memory: {on_heap_spark_execution_memory} MB\\")\\n    if spark_memory_offheap_enabled:\\n        off_heap_spark_storage_memory = spark_memory_offheap_size * SPARK_MEMORY_STORAGEFRACTION\\n        off_heap_spark_execution_memory = spark_memory_offheap_size * (1 - SPARK_MEMORY_STORAGEFRACTION)\\n        print(\\"\\\\n\\")\\n        print(f\\"------------------ Off-Heap Memory: {spark_memory_offheap_size} MB ------------------\\")\\n        print(f\\"Storage Memory: {off_heap_spark_storage_memory} MB\\")\\n        print(f\\"Execution Memory: {off_heap_spark_execution_memory} MB\\")\\n        total_spark_memory += spark_memory_offheap_size\\n    print(\\"\\\\n\\")\\n    print(f\\"------------------ Total Spark Memory (Spark Memory + Off-Heap Memory): {total_spark_memory} MB ({total_spark_memory / GB_TO_MB_RATE} GB) ------------------\\")\\n```\\n\\nSo for an application with `spark.executor.memory=1g`, `spark.memory.offHeap.enabled=true`, `spark.memory.offHeap.size=512m`.\\n\\n```bash\\npython calculate.py\\n\\nAmount of spark.executor.memory (in GB): 1\\nOption spark.memory.offHeap.enabled: true\\nAmount of spark.memory.offHeap.size (in GB): 0.5\\n\\n\\n------------------ On-Heap Memory: 910.5 MB ------------------\\nResearved Memory: 300 MB\\nUser Memory: 244.20000000000002 MB\\nSpark Memory: 366.3 MB\\n        Storage Memory: 183.15 MB\\n        Execution Memory: 183.15 MB\\n\\n\\n------------------ Off-Heap Memory: 512.0 MB ------------------\\nStorage Memory: 256.0 MB\\nExecution Memory: 256.0 MB\\n\\n\\n------------------ Total Spark Memory (Spark Memory + Off-Heap Memory): 878.3 MB (0.85771484375 GB) ------------------\\n```\\n\\nWe start a spark shell with the following configurations.\\n\\n```bash\\nspark-shell --conf spark.executor.memory=1g --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=512m\\n```\\n\\nThen go to `http://localhost:4040/executors/`.\\n\\n![memory calculation](./images/memory-calculation.PNG)\\n\\nWe can see that the total amount of Spark memory is exactly like our calculation.\\n\\nThat is how we calculate the memory in Spark. The source code can be found at: [https://github.com/lam1051999/spark_memory_calculator](https://github.com/lam1051999/spark_memory_calculator). See you in the next blogs.\\n\\n### 4. References\\n\\n[Spark Memory Management](https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794)\\n\\n[Dive into Spark memory](https://luminousmen.com/post/dive-into-spark-memory)"},{"id":"state-management-react","metadata":{"permalink":"/blogs/blog/state-management-react","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2023-06-26-state-management-react/index.md","source":"@site/blog/2023-06-26-state-management-react/index.md","title":"State Management In React","description":"State is a very crucial part of React applications which will help update the information of the React components to change UI accordingly and make our application interactive with clients. In this article, I will walk you through the usage of state, its characteristic, and how we can use state in an efficient way.","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"Web development","permalink":"/blogs/blog/tags/web-development"},{"label":"React","permalink":"/blogs/blog/tags/react"},{"label":"Html","permalink":"/blogs/blog/tags/html"},{"label":"Css","permalink":"/blogs/blog/tags/css"}],"readingTime":10.46,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"state-management-react","title":"State Management In React","authors":"tranlam","tags":["Web development","React","Html","Css"],"image":"./images/showcase.PNG"},"prevItem":{"title":"How Is Memory Managed In Spark?","permalink":"/blogs/blog/how-is-memory-managed-in-spark"},"nextItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","permalink":"/blogs/blog/mini-spark3-authorizer-part-2"}},"content":"State is a very crucial part of React applications which will help update the information of the React components to change UI accordingly and make our application interactive with clients. In this article, I will walk you through the usage of state, its characteristic, and how we can use state in an efficient way.\\n\\n![showcase image](./images/showcase.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Virtual DOM\\n\\nIf you ever work with `React` or `Vue`, you probably know about a concept called Virtual DOM which those libraries/frameworks used to update the real DOM tree in the browser. Virtual DOM in React is a programming concept where the representation of a UI is kept in memory and synced with the real DOM.\\n\\n![virtual dom](./images/virtual_dom.PNG)\\n\\nIn the picture above, whenever we have a UI in the browser on the left, we can inspect that UI and we can see the real DOM tree which contains several Html elements such as `div`, `ul`, `li`,... React will maintain a JavaScript object on the right to describe things on the real DOM and updates made to that object will be synchronized to the real DOM and modify the UI accordingly.\\n\\nThus, the Virtual DOM is a lightweight version of the real DOM, it provides a mechanism that abstracts manual DOM manipulations away from the developers, helping us to write more predictable code to interact with the real DOM.\\n\\n### 2. React state\\n\\nState is a plain JavaScript object used by React to represent a piece of information about the component\'s current situation. We modify state to manipulate the Virtual DOM.\\n\\n![state change](./images/state_change.PNG)\\n\\nWhenever state changes, it will modify some properties of the Virtual DOM. React will compare the new Virtual DOM with the old one to detect those changes and will synchronize them to the real DOM. The interesting thing here is that update is only applied at the node where there is an actual change. So, how can React do the diffing algorithm in an efficient way?\\n\\n### 3. Diffing algorithm\\n\\nDiffing is the algorithm that React uses in order to find differences between two Virtual DOM trees and update efficiently the real DOM. In this section, I will just explain very high-level rules used by this algorithm.\\n\\nWhen diffing two trees, React will first compare the two root elements.\\n\\n#### 3.1. Rule #1\\n\\nTwo elements of different types will produce different trees.\\n\\n![rule 1](./images/rule_1.PNG)\\n\\nAs shown in the picture, going from `<div>` to `<span>` will lead to full rebuild, its children will get unmounted and have their state destroyed.\\n\\n#### 3.2. Rule #2\\n\\nWith the DOM elements of the same type, but different attributes, React knows to only modify the attribute that have been changed.\\n\\nIn this example, when the `className` attribute of the `<input>` is changed, React will only update that attribute instead of rebuilding the whole element.\\n\\n![rule 2 1](./images/rule_2_1.PNG)\\n\\nWith object value attribute, React updates only the properties that have been changed.\\n\\n![rule 2 2](./images/rule_2_2.PNG)\\n\\n#### 3.3. Rule #3\\n\\nThis rule is about detecting the differences while recursing on children elements.\\n\\n![rule 3 1](./images/rule_3_1.PNG)\\n\\nReact iterates over both lists of lists of children at the same time, and generates mutation whenever there\'s a difference. As shown in above example, React will know to just generate the `List item 3` and append to the `ul` element.\\n\\nSo what if we prepend the list element instead of append? Because of iterating two lists at the same time and comparing, React will assume that all of those elements have been changed and rebuild all of them, which is a very bad case.\\n\\n![rule 3 2](./images/rule_3_2.PNG)\\n\\nIn order to get rid of the above case, we come to the `rule #4`\\n\\n#### 3.4. Rule #4\\n\\nDevelopers can hint at which elements may be stable across different renders with a `key` prop, so that elements with the same key will be compared with each other\\n\\n![rule 4](./images/rule_4.PNG)\\n\\nIn the picture above, we add a `key` property to each `li` element so React will know to pair check the elements with the `key=\\"item-1\\"` and `key=\\"item-2\\"` and only add the element with `key=\\"item-3\\"` to the new DOM\\n\\n### 4. State usage with useState hook\\n\\nThe `useState` hook is a built-in React hook that allows you to manage state in a functional component.\\n\\nConsider an example below\\n\\n```js\\n// define React state in functional components\\nconst [count, setCount] = useState(0);\\n\\n// update state\\nsetCount(count + 1);\\nsetCount((prevCount) => prevCount + 1);\\n```\\n\\n`useState` returns an array contains the state (`count`) and a function used to update the state (`setCount`). The set function will take either the new value for the state or a callback function that returns new value as the argument.\\n\\nThere are some considerations when using state\\n\\n- **`setState` function is asynchronous:** consider the below example, whenever we click the button to increase the count, we see the console logs the old value of state before it is updated. Because `setState` function is asynchronous, it will be pulled to an event loop, move to the lifecycle `Call stack -> WebAPIs -> Callback Queue -> Call stack -> being executed -> Pop out the stack` (you can refer to event loop **[here](https://www.webdevolution.com/blog/Javascript-Event-Loop-Explained)**), but basically, in JavaScript, in the same code block, the synchronous code will always run before the asynchronous code, that\'s why we see the console logs the value of state before it is updated.\\n\\nCode: `UseStateM1.jsx`, tab `Mistake 1`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM1.jsx\\"\\n></iframe>\\n\\n- **Changes to state should be made by `setState` function:** in this example, we can see that the count actually changes in the console log but the UI is not updated. Because we modify the state value directly and it will not cause the rerender of the component. In this case, we need to update state value by `setState` function: `setCount(count + 3)`.\\n\\nCode: `UseStateM2.jsx`, tab `Mistake 2`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM2.jsx\\"\\n></iframe>\\n\\n- **State should be treated as immutable:** each time we use `setState` function, we need to pass a brand new value to the function in order to rerender the component. React uses Shallow Comparision to check if the state is changed or not. In JavaScript, with primitive datatypes such as numbers, strings,... SC will compare their values, with object datatypes such as object, array,... SC will compare their references. Consider the example below, when we modify the state object directly, the component will not be rerendered and the UI will not be updated because the state actually holds the reference to the user object, not its value. In order to fix this, we use the spread operator to create a new object to make a new reference, and update properties in that object `{ ...prevUser, name: \\"No one\\", age: 30 }`, and our component will work as expected.\\n\\nCode: `UseStateM3.jsx`, tab `Mistake 3`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM3.jsx\\"\\n></iframe>\\n\\n- **Pass function as argument to `setState` whenever the state value depends on its previous value:** in the example below, some might expect the value of `count` will increase 2 at a time, but it actually increases 1 because `setState` is an asynchronous function, it will not be executed immediately but pulled to the event loop, and those two `setState` functions will receive the same value `count = 0`. To fix this, we need to use `setCount((prev) => prev + 1)`, we make the output value depends on the previous value so no matter the order of execution of that two `setState`, the value will be increased by 2 at a time.\\n\\nCode: `UseStateM4.jsx`, tab `Mistake 4`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/components/UseStateM4.jsx\\"\\n></iframe>\\n\\n### 5. Update components in different DOM tree branches\\n\\nState is often used within the body of a component and modify information about the component or its children, so, how can we sit in a component and modify the UI of other components that are in different scope?\\n\\n![update different dom tree branches](./images/update_different_dom_tree_branches.PNG)\\n\\n#### 5.1. Using callback functions\\n\\nIn this approach, we are using callback functions in the child component to update state of the parent component, we also pass the state value from the parent to another child component to update its UI.\\n\\nCode: `ByCallback.jsx`, tab `By Callback`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByCallback.jsx\\"\\n></iframe>\\n\\nIn the code above, we define state in the `Root` component and pass the functions used to update state to `RootLeftLeft`, and pass the state value to `RootRightRight`. Now, we can sit in the `RootLeftLeft` and modify the UI of `RootRightRight` by updating the state in their common parent component.\\n\\nPros of this approach\\n\\n- It does what we want, which is to update the UI of the component from other scopes.\\n- Easy to use and pretty straightforward, use the React built-in hooks.\\n\\nCons\\n\\n- It will cause a lot of rerenders each time because the `Root` component is being rerendered, and so do its children.\\n- Ugly code when we have many duplicates and we also need to pass the props in the intermediate components in the way from `Root` component to our target components.\\n\\n#### 5.2. Using React context\\n\\nBy using this approach, we can get rid of the ugly code problem in the first approach.\\n\\nCode: `ByContext.jsx`, tab `By Context`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByContext.jsx\\"\\n></iframe>\\n\\nWe define the `CountContext` context and provide it to our application part, we register the state value and the functions used to update state to the context, then, we use it directly in our target components, by using React `useContext` hook.\\n\\nEven we don\'t have the smelly code anymore, this approach still cannot solve the performance problem when all components of the tree are still being rerendered.\\n\\n#### 5.3. Using Redux\\n\\nRedux is a powerful state management library that will help us avoid rerendering too many components.\\n\\nRedux has a concept called a single source of truth when we maintain our whole application state in a single store.\\n\\nThe state is Redux is read-only as we cannot mutate directly, the only way to change state is to emit an action.\\n\\nChanges will be made by pure functions, so reducers are the pure functions, their output will only depend on the inputs and will not cause any side effects.\\n\\nCode: `ByRedux.jsx`, tab `By Redux`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/ByRedux.jsx\\"\\n></iframe>\\n\\nIn this example, we define `store` as our single source of truth, with a reducer to manage the count value and provide it within our application part. We create a slice and define functions to mutate the value in `store`. Action creators can be extracted from the slice and then they will be used everywhere within the context that `store` is provided.\\n\\nIn this way, the only component that gets rerendered is `RootRightRight`, when its props changed over time.\\n\\n### 6. Performance considerations when interacting with state\\n\\nThere are some useful hooks/functions used to cache/memorize things related to state to avoid components from rerendering, which are\\n\\n- `React.memo()` for memorizing React components.\\n- `useMemo()` for memorizing some values.\\n- `useCallback()` for memorizing some callback functions.\\n\\nI will apply theses to the **[ByContext](#52-using-react-context)** example above.\\n\\nCode: `Memo.jsx`, tab `Memo`\\n\\n<iframe\\n  style={{\\n    width: \\"100%\\",\\n    height: 900,\\n    outline: \\"1px solid #252525\\",\\n    border: 0,\\n    borderRadius: 8,\\n    marginBottom: 16,\\n    zIndex: 100\\n  }}\\n  src=\\"https://codesandbox.io/s/state-management-24xt26?file=/src/pages/Memo.jsx\\"\\n></iframe>\\n\\nI use `useCallback` to memorize the callback functions used to update state, and prevent them from being recreated every time the `Root` component rerenders. Because the state value `count` changes over time so I will not cache that value.\\n\\nI also use `React.memo()` to memorize the `RootLeft` component to prevent its rerendering when `Root` component rerenders (can also apply to other static components in the code, such as `RootRightLeft`,...).\\n\\nNow, when we hit the increment/decrement buttons, we can see that event `Root` component rerenders, `RootLeft` and `RootLeftLeft` will not be rerendered.\\n\\nThis is pretty much about state management in React. Hope you enjoy reading it. See you in the next blogs.\\n\\n### 7. References\\n\\n[Reconciliation](https://legacy.reactjs.org/docs/reconciliation.html)\\n\\n[What is Diffing Algorithm ?](https://www.geeksforgeeks.org/what-is-diffing-algorithm/)\\n\\n[Javascript Event Loop Explained](https://www.webdevolution.com/blog/Javascript-Event-Loop-Explained)"},{"id":"mini-spark3-authorizer-part-2","metadata":{"permalink":"/blogs/blog/mini-spark3-authorizer-part-2","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md","source":"@site/blog/2023-05-01-mini-spark3-authorizer-part-2/index.md","title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","description":"In the previous blog, I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of Spark Catalyst Optimizer in order to archive authorization in Spark SQL with Ranger.","date":"2023-05-01T00:00:00.000Z","formattedDate":"May 1, 2023","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Ranger","permalink":"/blogs/blog/tags/ranger"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"}],"readingTime":5.615,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mini-spark3-authorizer-part-2","title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","authors":"tranlam","tags":["Bigdata","Spark","Ranger","Apache"],"image":"./images/banner.PNG"},"prevItem":{"title":"State Management In React","permalink":"/blogs/blog/state-management-react"},"nextItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","permalink":"/blogs/blog/mini-spark3-authorizer-part-1"}},"content":"In the **[previous blog](/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md)**, I have successfully installed a standalone Ranger service. In this article, I show you how we can customize the logical plan phase of **[Spark Catalyst Optimizer](/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md)** in order to archive authorization in Spark SQL with Ranger.\\n\\n![banner](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Spark installation\\n\\nFirstly, I will install Apache Spark 3.3.0 on my local machine. It is pretty easy with a few below steps.\\n\\n```bash\\n# Get Spark build with hadoop, you can find specific version here: https://archive.apache.org/dist/spark/\\ncd ~\\nwget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\\ntar -xvf spark-3.3.0-bin-hadoop3.tgz\\ncd spark-3.3.0-bin-hadoop3\\n\\n# Configure some environment variables\\nexport SPARK_HOME=~/spark-3.3.0-bin-hadoop3\\nexport PATH=$PATH:$SPARK_HOME/bin\\n\\n# Check if Spark is installed properly\\nspark-shell\\n```\\n\\n### 2. Build a mini Spark Session Extension for Ranger authorization\\n\\nThe idea to make the authorizer is first inspired by [this GitHub repository](https://github.com/yaooqinn/spark-ranger). Currently, this repository has been archived and it is only compatible with Spark 2.4 and below, which does not help our use case (we use Spark 3.3.0). After weeks of trying multiple solutions but not having a result, I end up customizing the repository to work with Spark 3.3.0 myself.\\n\\nSpark makes a huge update to migrate from 2.4 to 3.0 (see detail [here](https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-24-to-30) and [here](https://spark.apache.org/releases/spark-release-3-0-0.html)) which create many new features. So we need to add more code and configure the project dependencies correctly in order not to conflict with Spark 3 dependencies (since Ranger and Spark 3 use the same library `jersey` to build RESTful Web Services, Ranger uses 1.x and Spark 3 uses 2.x).\\n\\nThe idea is to create a Spark Session Extension to customize the logical plan optimization phase of Spark Catalyst Optimizer, and since the logical plan is represented as **[TreeNode](/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md#1-treenode)**, it can be logical commands such as `CreateTableCommand`, `DropTableCommand`, `InsertIntoHiveTable`,... In these commands, we can extract the name of the database, table, and columns,... out of them, we collect them and check if the current user has proper access to those resources by Ranger-provided APIs.\\n\\nThe extension code can be found at: **[https://github.com/lam1051999/mini_spark3_authorizer](https://github.com/lam1051999/mini_spark3_authorizer)**.\\n\\n```bash\\n# Build the project\\ncd ~\\ngit clone https://github.com/lam1051999/mini_spark3_authorizer\\ncd mini_spark3_authorizer/spark3-ranger-custom\\nmvn clean package\\n\\n# Copy output jar to Spark jars folder\\ncp target/spark-ranger-1.0-SNAPSHOT.jar $SPARK_HOME/jars\\n\\n# Download dependency jars\\ncd $SPARK_HOME/jars\\nwget https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar\\nwget https://repo1.maven.org/maven2/com/kstruct/gethostname4j/1.0.0/gethostname4j-1.0.0.jar\\nwget https://repo1.maven.org/maven2/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar\\nwget https://repo1.maven.org/maven2/org/apache/kudu/kudu-spark3_2.12/1.16.0/kudu-spark3_2.12-1.16.0.jar\\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar\\nwget https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar\\n```\\n\\nAfter we have all the additional jars in Spark, we start to configure some properties for the extension.\\n\\n```bash\\n# Get the template configuration file\\ncd $SPARK_HOME/conf\\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-security.xml .\\ncp ~/mini_spark3_authorizer/spark3-conf/conf/ranger-spark-audit.xml .\\n\\n# Modify the downloaded policy directory\\nvi ranger-spark-security.xml\\nranger.plugin.spark.policy.cache.dir=<your_spark_home>/security/policycache\\nmkdir -p $SPARK_HOME/security/policycache\\n\\n# Copy template policy file. The filename need to be exactly below, because in the extension code, plugin appId = ranger_customized, in Ranger Admin UI, the service we are going to create will have the name = spark_sql\\ncp ~/mini_spark3_authorizer/spark3-conf/security/policycache/ranger_customized_spark_sql.json $SPARK_HOME/security/policycache\\n```\\n\\nAt the moment, we go to Ranger Admin UI `http://localhost:6080` with `user/password = admin/YourPassword@123456`. In the `HIVE` plugin folder, create a service with the below input.\\n\\n![service config](./images/service_config.PNG)\\n\\nWe can test if the service policy is accessible through RESTful APIs or not and if it is downloadable.\\n\\n```bash\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies/download/spark_sql\\"\\n```\\n\\nAdd user you are going to test under `Settings -> Users/Groups/Roles -> Add New User`.\\n\\n![user config](./images/user_config.PNG)\\n\\nAdd policy to the service `spark_sql`.\\n\\n![policy config](./images/policy_config.PNG)\\n\\nWe create the database and table that we need.\\n\\n```bash\\n# Create a table to Spark metastore\\nspark-shell\\n\\nscala> val df = spark.read.parquet(\\"file:///Users/tranlammacbook/mini_spark3_authorizer/jobs.parquet\\")\\nscala> df.printSchema\\nroot\\n |-- job_id: string (nullable = true)\\n |-- company_id: string (nullable = true)\\n |-- job_name: string (nullable = true)\\n |-- taglist: string (nullable = true)\\n |-- location: string (nullable = true)\\n |-- three_reasons: string (nullable = true)\\n |-- description: string (nullable = true)\\nscala> spark.sql(\\"CREATE DATABASE test;\\")\\nscala> df.write.mode(\\"overwrite\\").format(\\"parquet\\").saveAsTable(\\"test.jobs\\")\\n```\\n\\nNow we can test our extension. As in the policy configuration, user `lamtran` only has permission to the columns `job_id, company_id, job_name` of table `test.jobs`.\\n\\n```bash\\nspark-shell --conf spark.sql.extensions=mini.spark3.authorizer.RangerSparkSQLExtension --conf spark.sql.proxy-user=lamtran\\n\\n# Check Spark configurations\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = mini.spark3.authorizer.RangerSparkSQLExtension\\nscala> spark.conf.get(\\"spark.sql.proxy-user\\")\\nres1: String = lamtran\\n\\n# Check permitted columns\\nscala> spark.sql(\\"SELECT job_id, company_id, job_name FROM test.jobs LIMIT 10\\").show(truncate=false)\\n+----------------------------------------------------+--------------+----------------------------------------+\\n|job_id                                              |company_id    |job_name                                |\\n+----------------------------------------------------+--------------+----------------------------------------+\\n|kms-technology:jrsr_qa_engineer_kms_labs_bonus      |kms-technology|(Jr/Sr) QA Engineer, KMS Labs - BONUS   |\\n|kms-technology:engineering_manager_bonus            |kms-technology|Engineering Manager - BONUS             |\\n|kms-technology:fullstack_mobile_mobilenodejs_kobiton|kms-technology|Fullstack Mobile (Mobile,NodeJs) Kobiton|\\n|kms-technology:jrsrprincipal_java_developer_bonus   |kms-technology|(Jr/Sr/Principal) Java Developer- BONUS |\\n|kms-technology:product_manager_kms_labs_bonus       |kms-technology|Product Manager, KMS Labs - BONUS       |\\n|kms-technology:sr_it_business_analyst_english_bonus |kms-technology|Sr IT Business Analyst (English) - BONUS|\\n|kms-technology:fullstack_dev_reactjsnodejs_kobiton  |kms-technology|Fullstack Dev (ReactJs,NodeJs) - Kobiton|\\n|kms-technology:senior_ruby_on_rails_engineer_bonus  |kms-technology|Senior Ruby on Rails Engineer - BONUS   |\\n|kms-technology:senior_data_engineer_bonus           |kms-technology|Senior Data Engineer - BONUS            |\\n|kms-technology:srjr_fullstack_nodejsreactjs_bonus   |kms-technology|Sr/Jr Fullstack (NodeJS,ReactJS) - BONUS|\\n+----------------------------------------------------+--------------+----------------------------------------+\\n\\n# Check columns that are not allowed to select\\nscala> spark.sql(\\"SELECT location, taglist, description FROM test.jobs LIMIT 10\\").show(truncate=false)\\n23/07/01 21:35:19 ERROR RangerSparkAuthorizerExtension: \\n+===============================+\\n|Spark SQL Authorization Failure|\\n|-------------------------------|\\n|Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\\n|-------------------------------|\\n|Spark SQL Authorization Failure|\\n+===============================+\\n             \\norg.apache.ranger.authorization.spark.authorizer.SparkAccessControlException: Permission denied: user [lamtran] does not have [SELECT] privilege on [test/jobs/location,taglist,description]\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6(RangerSparkAuthorizer.scala:127)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$6$adapted(RangerSparkAuthorizer.scala:124)\\n  at scala.collection.Iterator.foreach(Iterator.scala:943)\\n  at scala.collection.Iterator.foreach$(Iterator.scala:943)\\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3(RangerSparkAuthorizer.scala:124)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.$anonfun$checkPrivileges$3$adapted(RangerSparkAuthorizer.scala:107)\\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n  at org.apache.ranger.authorization.spark.authorizer.RangerSparkAuthorizer$.checkPrivileges(RangerSparkAuthorizer.scala:107)\\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:58)\\n  at org.apache.spark.sql.extensions.RangerSparkAuthorizerExtension.apply(RangerSparkAuthorizerExtension.scala:14)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\\n  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\\n  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\\n  at scala.collection.immutable.List.foldLeft(List.scala:91)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\\n  at scala.collection.immutable.List.foreach(List.scala:431)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\\n  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\\n  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\\n  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\\n  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:810)\\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:787)\\n  ... 47 elided\\n```\\n\\nNow check the downloaded policy file.\\n\\n```bash\\ncd $SPARK_HOME/security/policycache\\nls -lh\\n-rw-r--r--  1 tranlammacbook  staff    23K Jul  1 21:32 ranger_customized_spark_sql.json\\n```\\n\\nWe will see this is the new policy file and its content is refreshed every 10 seconds when the extension is in use.\\n\\n### 3. Room for improvement\\n\\nIn this blog, we integrate a standalone Spark 3 with a standalone Ranger, but in production, Spark is often used in corporate with a Hadoop data cluster with Kerberos authentication enabled. Ranger will also sit in that Hadoop and do the authorization for many frameworks in Hadoop. Thus, there are a few more things that we need to implement.\\n\\n- Config Spark and Ranger to work with Hadoop, and run Spark job in cluster mode.\\n- Policy refresher needs to be secure and use SPNego protocol (which use Kerberos keytab to generate token) to make RESTful API calls to Ranger service.\\n- The user of the Spark application and the user for authorizing Ranger need to be the keytab principal user.\\n- Auditing is implemented in a meaningful way.\\n\\nI have also successfully implemented all the above functionalities in the production environment. If you need them, contact me on **[Linkedin](https://www.linkedin.com/in/lamtt1005)**."},{"id":"mini-spark3-authorizer-part-1","metadata":{"permalink":"/blogs/blog/mini-spark3-authorizer-part-1","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md","source":"@site/blog/2023-04-30-mini-spark3-authorizer-part-1/index.md","title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","description":"Spark and Ranger are widely used by many enterprises because of their powerful features. Spark is an in-memory data processing framework and Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform. Thus, Ranger can be used to do authorization for Spark SQL and this blog will walk you through the integration of those two frameworks. This is the first part of the series, where we install the Ranger framework on our machine, and additionally, Apache Solr for auditing.","date":"2023-04-30T00:00:00.000Z","formattedDate":"April 30, 2023","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Ranger","permalink":"/blogs/blog/tags/ranger"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"}],"readingTime":4.06,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mini-spark3-authorizer-part-1","title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","authors":"tranlam","tags":["Bigdata","Spark","Ranger","Apache"],"image":"./images/banner.PNG"},"prevItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 2 - Integrate Spark SQL With Ranger","permalink":"/blogs/blog/mini-spark3-authorizer-part-2"},"nextItem":{"title":"Spark Catalyst Optimizer And Spark Session Extension","permalink":"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension"}},"content":"Spark and Ranger are widely used by many enterprises because of their powerful features. Spark is an in-memory data processing framework and Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform. Thus, Ranger can be used to do authorization for Spark SQL and this blog will walk you through the integration of those two frameworks. This is the first part of the series, where we install the Ranger framework on our machine, and additionally, Apache Solr for auditing.\\n\\n![banner](./images/banner.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Build process\\n\\nThis installation comes with these below components.\\n\\n| Component   | Version      |\\n| ----------- | ------------ |\\n| MacOS       | M1/M2        |\\n| MySQL       | 8.0.33       |\\n| Java        | OpenJDK-8-jdk|\\n| Python      | 2.7.18       |\\n| Maven       | 3.6.3        |\\n| Ranger      | 2.0.0        |\\n| Solr        | 7.7.1        |\\n\\nI install the framework on MacOS, but it is similar when it comes to any other Unix or Linux distributions.\\n\\nFirstly, you need to have MySQL, Jdk8, Python 2.7, and Maven installed on your system, follow instructions from anywhere to make sure these components work as expected.\\n\\nAfter that, you can download the source code of Ranger (I use release version 2.0.0) and start to build the source code.\\n\\n```bash\\n# Get source code\\nwget https://downloads.apache.org/ranger/2.0.0/apache-ranger-2.0.0.tar.gz\\ntar -xvf apache-ranger-2.0.0.tar.gz\\ncd apache-ranger-2.0.0\\n\\n# Build source code, it will output the ranger-2.0.0-admin.tar.gz in this repository in target/ folder\\nmvn clean compile package install assembly:assembly -Dmaven.test.skip=true -Drat.skip=true -Dpmd.skip=true -Dfindbugs.skip=true -Dspotbugs.skip=true -Dcheckstyle.skip=true\\n```\\n\\n### 2. Ranger Admin installation\\n\\nAfter the build process, you will have the `ranger-2.0.0-admin.tar.gz` file in the `target/` folder, you can go to the installation step of Ranger Admin. Our target is running a standalone Ranger with Solr for auditing then `ranger-2.0.0-admin.tar.gz` is enough in this case.\\n\\n```bash\\n# Extract the tar file\\ncp ranger-2.0.0-admin.tar.gz ~\\ncd ~\\ntar -xvf ranger-2.0.0-admin.tar.gz\\ncd ranger-2.0.0-admin\\n```\\n\\nWe use MySQL as the database for Ranger and store all the information, setup step needs to connect to MySQL to initialize the database and tables. Thus, we need to have a MySQL connector to connect to MySQL from the setup code.\\n\\n```bash\\n# Install mysql-connector\\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.26.tar.gz\\ntar -xvf mysql-connector-java-8.0.26.tar.gz\\nmv mysql-connector-java-8.0.26/mysql-connector-java-8.0.26.jar mysql-connector-java.jar\\n```\\n\\nNow we are ready to edit the Ranger configuration.\\n\\n```bash\\nvi install.properties\\nSQL_CONNECTOR_JAR=<path_to_mysql_connector>/mysql-connector-java.jar\\n\\ndb_name=ranger\\ndb_user=admin\\ndb_password=password12\\n\\nrangerAdmin_password=YourPassword@123456\\nrangerTagsync_password=YourPassword@123456\\nrangerUsersync_password=YourPassword@123456\\nkeyadmin_password=YourPassword@123456\\n\\naudit_solr_urls=http://localhost:6083/solr/ranger_audits\\n\\nunix_user=<your_mac_user>\\nunix_user_pwd=<your_mac_user_password>\\nunix_group=<your_mac_group>\\n\\nRANGER_PID_DIR_PATH=$PWD/var/run/ranger\\n```\\n\\nCurrently, the configuration for our use case is completed, we now run the setup scripts.\\n\\n```bash\\n# Comment setup user/group because it is currently compatible with Linux\\nvi setup.sh # then comment #setup_unix_user_group\\n\\n# Setup scripts run python2\\npyenv local 2.7.18\\n\\n# After updating the required properties, run setup.sh\\n./setup.sh\\n```\\n\\nOnce the setup scripts are done, you will see this output `Installation of Ranger PolicyManager Web Application is completed.`, that means it is successful. Then, we can run our Ranger Admin service.\\n\\n```bash\\n~/ranger-2.0.0-admin/ews/ranger-admin-services.sh start\\n\\n# Check logs\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/access_log.*\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/catalina.out\\ntail -100f ~/ranger-2.0.0-admin/ews/logs/ranger-admin-*.log\\n```\\n\\nTo access Ranger Admin UI, go to `http://localhost:6080` with `user/password = admin/YourPassword@123456`.\\n\\n![ranger home](./images/ranger_home.PNG)\\n\\nAdditional notes.\\n\\n```bash\\n# Configure policy with\\npolicy.download.auth.users=<your_user>\\ntag.download.auth.users=<your_user>\\n\\n# We can test the policy with Rest APIs provided by Ranger\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies\\" # to get all policies\\ncurl -ivk -H \\"Content-type:application/json\\" -u admin:YourPassword@123456 -X GET \\"http://localhost:6080/service/plugins/policies/download/dev_hive\\" # to get specific policy by service name\\n\\n# Stop ranger admin\\n~/ranger-2.0.0-admin/ews/ranger-admin-services.sh stop \\n```\\n\\n### 3. Solr installation for auditing\\n\\nCurrently, Solr and Elasticsearch have supported sources for audit stores with Ranger. I will install Solr as it is built-in supported and does not require a good amount of infrastructure. This is also a standalone Solr which has no dependency on Zookeeper.\\n\\nIn the same Ranger build that we had done earlier, we would find an installation setup for enabling Solr audits. We also want to change some installation configurations for our specific use case.\\n\\n```bash\\ncd ~/ranger-2.0.0-admin/contrib/solr_for_audit_setup\\n\\n# Change config\\nvi install.properties\\nSOLR_USER=<your_mac_user>\\nSOLR_GROUP=<your_mac_group>\\n\\nSOLR_INSTALL=true\\n\\nJAVA_HOME=<your_java_home>\\nSOLR_DOWNLOAD_URL=http://archive.apache.org/dist/lucene/solr/7.7.1/solr-7.7.1.tgz\\n\\nSOLR_INSTALL_FOLDER=<your_prefix_folder>/data/solr\\nSOLR_RANGER_HOME=<your_prefix_folder>/data/solr/ranger_audit_server\\nSOLR_RANGER_DATA_FOLDER=<your_prefix_folder>/data/solr/ranger_audit_server/data\\nSOLR_LOG_FOLDER=<your_prefix_folder>/var/log/solr/ranger_audits\\n```\\n\\nAfter that, we can run the setup scripts.\\n\\n```bash\\n# Setup directory\\nmkdir -p <SOLR_INSTALL_FOLDER>\\n\\n# Setup scripts run python2\\npyenv local 2.7.18\\n\\n# After updating the required properties, run setup.sh\\nsudo ./setup.sh\\nsudo chown -R <your_mac_user>:<your_mac_group> ~/ranger-2.0.0-admin/contrib\\n```\\n\\nOnce the setup scripts are completed, we can start the Solr service.\\n\\n```bash\\n# Instructions for start/stop Solr\\ncat <SOLR_RANGER_HOME>/install_notes.txt\\n\\n# Start Solr\\n~/ranger-2.0.0-admin/contrib/solr_for_audit_setup/data/solr/ranger_audit_server/scripts/start_solr.sh\\n\\n# Stop Solr\\n~/ranger-2.0.0-admin/contrib/solr_for_audit_setup/data/solr/ranger_audit_server/scripts/stop_solr.sh\\n```\\n\\nGo to `http://localhost:6083`, you can see the information about your Solr service.\\n\\n![solr home](./images/solr_home.PNG)\\n\\nIf you don\'t want to build the source code yourself then go to this repository, I have included the output file of the build process at **[https://github.com/lam1051999/ranger_build_output](https://github.com/lam1051999/ranger_build_output)**.\\n\\nThis is all about installing Ranger with Solr for auditing. In the next blog, I will talk about how we can customize Spark to get a policy from Ranger and do the authorization on Spark SQL."},{"id":"spark-catalyst-optimizer-and-spark-session-extension","metadata":{"permalink":"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md","source":"@site/blog/2023-01-07-spark-catalyst-optimizer-and-spark-extension/index.md","title":"Spark Catalyst Optimizer And Spark Session Extension","description":"Spark catalyst optimizer is located at the core of Spark SQL with the purpose of optimizing structured queries expressed in SQL or through DataFrame/Dataset APIs, minimizing application running time and costs. When using Spark, often people see the catalyst optimizer as a black box, when we assume that it works mysteriously without really caring what happens inside it. In this article, I will go in depth of its logic, its components, and how the Spark session extension participates to change the Catalyst\'s plans.","date":"2023-01-07T00:00:00.000Z","formattedDate":"January 7, 2023","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"}],"readingTime":14.675,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"spark-catalyst-optimizer-and-spark-session-extension","title":"Spark Catalyst Optimizer And Spark Session Extension","authors":"tranlam","tags":["Bigdata","Spark","Apache"],"image":"./images/spark-catalyst-optimizer.JPG"},"prevItem":{"title":"Authorize Spark 3 SQL With Apache Ranger Part 1 - Ranger installation","permalink":"/blogs/blog/mini-spark3-authorizer-part-1"},"nextItem":{"title":"MySQL series - Indexing","permalink":"/blogs/blog/mysql-series-mysql-indexing"}},"content":"Spark catalyst optimizer is located at the core of Spark SQL with the purpose of optimizing structured queries expressed in SQL or through DataFrame/Dataset APIs, minimizing application running time and costs. When using Spark, often people see the catalyst optimizer as a black box, when we assume that it works mysteriously without really caring what happens inside it. In this article, I will go in depth of its logic, its components, and how the Spark session extension participates to change the Catalyst\'s plans.\\n\\n![spark catalyst optimizer](./images/spark-catalyst-optimizer.JPG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. TreeNode\\n\\nThe main components in Catalyst are represented as tree nodes, which are inherited from class `TreeNode`, or its subclasses. Class `TreeNode` has a set of child nodes with the attribute `children`, datatype `Seq[BaseType]`, therefore, one `TreeNode` can have 0 or more child nodes. These objects are immutable and manipulated using functional transformations, making the debug optimizer easier and parallel operations more predictable.\\n\\nThe two important classes are `LogicalPlan` and `SparkPlan` are both subclasses of `QueryPlan`, the class inherits directly from `TreeNode`. In the above Catalyst diagram, the first 3 components are logical plans, the nodes in the logical plan are usually logical operators such as `CreateTableCommand`, `Filter`, `Project`,... the two components behind are spark plans (physical plans), nodes are usually low-level operators like `ShuffledHashJoinExec`, `SortMergeJoinExec`, `BroadcastHashJoinExec`, `FileSourceScanExec`,...\\n\\nLeaf nodes will read data from sources, storage, memory ,... and the root node of the tree is the outermost operator and returns the result of the calculation.\\n\\n### 2. Rules\\n\\nTo manipulate the TreeNode we use rules, rules are actually components that transforms the tree, from one tree to another. In the rule, we implement the logic that transforms the TreeNode, which often uses the pattern matching in Scala to find the corresponding matches in its subtree and replace it with other constructs. Trees provide transformation functions that can apply this pattern matching to transform trees like `transform`, `transformDown`, `transformUp`,...\\n\\n```scala\\npackage org.apache.spark.sql.catalyst.trees\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\\n   * When `rule` does not apply to a given node it is left unchanged.\\n   * Users should not expect a specific directionality. If a specific directionality is needed,\\n   * transformDown or transformUp should be used.\\n   *\\n   * @param rule the function used to transform this nodes children\\n*/\\ndef transform(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformDown(rule)\\n}\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to the tree.\\n   * When `rule` does not apply to a given node it is left unchanged.\\n   * Users should not expect a specific directionality. If a specific directionality is needed,\\n   * transformDown or transformUp should be used.\\n   *\\n   * @param rule   the function used to transform this nodes children\\n   * @param cond   a Lambda expression to prune tree traversals. If `cond.apply` returns false\\n   *               on a TreeNode T, skips processing T and its subtree; otherwise, processes\\n   *               T and its subtree recursively.\\n   * @param ruleId is a unique Id for `rule` to prune unnecessary tree traversals. When it is\\n   *               UnknownRuleId, no pruning happens. Otherwise, if `rule` (with id `ruleId`)\\n   *               has been marked as in effective on a TreeNode T, skips processing T and its\\n   *               subtree. Do not pass it if the rule is not purely functional and reads a\\n   *               varying initial state for different invocations.\\n*/\\ndef transformWithPruning(cond: TreePatternBits => Boolean,\\nruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n: BaseType = {\\n    transformDownWithPruning(cond, ruleId)(rule)\\n}\\n\\n/**\\n   * Returns a copy of this node where `rule` has been recursively applied to it and all of its\\n   * children (pre-order). When `rule` does not apply to a given node it is left unchanged.\\n   *\\n   * @param rule the function used to transform this nodes children\\n*/\\ndef transformDown(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformDownWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\\n}\\n\\ndef transformDownWithPruning(cond: TreePatternBits => Boolean,\\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n  : BaseType = {\\n    /* More code */\\n}\\n\\ndef transformUp(rule: PartialFunction[BaseType, BaseType]): BaseType = {\\n    transformUpWithPruning(AlwaysProcess.fn, UnknownRuleId)(rule)\\n}\\n\\ndef transformUpWithPruning(cond: TreePatternBits => Boolean,\\n    ruleId: RuleId = UnknownRuleId)(rule: PartialFunction[BaseType, BaseType])\\n  : BaseType = {\\n    /* More code */\\n}\\n\\n/* ... */\\n```\\n\\nHere is a simple example of using transform and partn matching to transform one Treenode to another\\n\\n```scala\\npackage com.tranlam\\n\\nimport org.apache.spark.sql.catalyst.expressions.{Add, BinaryOperator, Expression, IntegerLiteral, Literal, Multiply, Subtract, UnaryMinus}\\nimport org.apache.spark.SparkConf\\nimport org.apache.spark.sql.SparkSession\\n\\nobject TestTransform {\\n  def main(args: Array[String]): Unit = {\\n    val sparkConf = new SparkConf().setAppName(\\"test_transform\\").setMaster(\\"local[*]\\")\\n    val spark = SparkSession.builder().config(sparkConf).getOrCreate()\\n    val firstExpr: Expression = UnaryMinus(Multiply(Subtract(Literal(11), Literal(2)), Subtract(Literal(9), Literal(5))))\\n    val transformed: Expression = firstExpr transformDown {\\n      case BinaryOperator(l, r) => Add(l, r)\\n      case IntegerLiteral(i) if i > 5 => Literal(1)\\n      case IntegerLiteral(i) if i < 5 => Literal(0)\\n    }\\n    println(firstExpr) // -((11 - 2) * (9 - 5))\\n    println(transformed) // -((1 + 0) + (1 + 5))\\n    spark.sql(s\\"SELECT ${firstExpr.sql}\\").show()\\n    spark.sql(s\\"SELECT ${transformed.sql}\\").show()\\n  }\\n}\\n```\\n\\nIn the above example, the transformDown function is used, which traverses the nodes of a tree and uses pattern matching to return a different result. If the node is a binary operator like Multiply, Subtract, it will convert to Add. If node is an integer constant greater than 5, it will change to 1, constant less than 5 will change to 0, a constant of 5 will keep the same value.\\n\\n### 3. Catalyst Operations in Spark SQL\\n\\nSpark Catalyst uses tree transformations in four main phases: (1) logical plan analysis to traverse the relations in that plan, (2) logical plan optimization, (3) physical planning, (4) code generation to compile the query into Java bytecode.\\n\\n#### 3.1. Parsing and Analyzing\\n\\n![spark catalyst parseing analyzing](./images/catalyst-pipeline-parsing-analyzing.PNG)\\n\\nIn this phase, Catalyst rules and Catalog objects will be used by Spark SQL to check if the relations in our query exist or not, relation properties such as columns, column names are also checked, the syntax of the query is examined and then resolve those relations.\\n\\nFor example, looking at the query plan below, Spark SQL will first transform the query into a parsed tree called an \\"unresolved logical plan\\" with undefined attributes and datatypes, not yet assigned to a specific table (or alias). Then it will\\n\\n- Search for relation by name from Catalog object.\\n- Mapping properties as columns of input with found relations.\\n- Decide which properties should point to the same value to assign it a unique ID (for the purpose of later optimizing expressions like `col = col`).\\n- Cast expressions of a specific datatype (for example, we won\'t know the return datatype of `col * 2` until col is resolved and the datatype is determined).\\n\\n```sql\\nSELECT * FROM test.describe_abc;\\n\\n== Parsed Logical Plan ==\\n\'Project [*]\\n+- \'UnresolvedRelation [test, describe_abc], [], false\\n\\n== Analyzed Logical Plan ==\\nid: int, name: string\\nProject [id#5833, name#5834]\\n+- SubqueryAlias spark_catalog.test.describe_abc\\n   +- Relation test.describe_abc[id#5833,name#5834] parquet\\n\\n== Optimized Logical Plan ==\\nRelation test.describe_abc[id#5833,name#5834] parquet\\n\\n== Physical Plan ==\\n*(1) ColumnarToRow\\n+- FileScan parquet test.describe_abc[id#5833,name#5834] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://bigdataha/user/hive/warehouse/test.db/describe_abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string>\\n```\\n\\n#### 3.2. Logical plan optimizations\\n\\n![spark LP optimization](./images/catalyst-pipeline-LP-optimization.PNG)\\n\\nCatalyst applies standard optimization rules to the logical plan analyzed in the previous step, with cached data. This section includes rules like\\n\\n- Constant folding: removes expressions that compute a value that we can define before the code runs, for example, with the expression `y = x * 2 * 2`, compiler will not generate 2 multiply instructions, it will first replace the constants before the values \u200b\u200bcan be computed `y = x * 4`.\\n- Predicate pushdown: push down parts of the query to where the data is stored, filter large amounts of data, improve network traffic.\\n- Projection: read only selected columns, less columns will be passed from the storage to Spark, significantly efficient with columnar file format such as Parquet.\\n- Boolean expression simplification: eg. A and (A or B) = A(A+B) = A.A + A.B = A + A.B = A.(1+B) = A\\n- Many other rules,\u2026\\n\\nSpark\'s Catalyst optimizer will include batches of rules, some of which can exist in multiple batches. Usually these batch rules will be run once on that plan, however, there are some batches that will run repeatedly until a certain number of passes.\\n\\n#### 3.3. Physical planning\\n\\n![spark PP planning](./images/catalyst-pipeline-PP-planning.PNG)\\n\\nSpark SQL takes a logical plan and generates one or more physical plans, then it chooses the appropriate physical plan based on the cost models. Cost models typically rely on relational statistics, quantifying statistics flowing into a node in a TreeNode such as\\n\\n- Size of data flowing into node.\\n- Number of records per table.\\n- Statistical indexes related to columns such as: number of distinct values and nulls, minimum and maximum value, average and maximum length of the values, an equi-height histogram of the values,...\\n\\nSome Spark SQL approaches to this cost model\\n\\n- Size-only approach: only uses statistics about the physical size of the data flowing into the node, also take the number of records index in some cases.\\n- Cost-based approach: statistics related to column level information for Aggregate, Filter, Join, Project nodes (note, cost-based approach is only applicable to nodes of this type, with other types of nodes, it will revert to using the size-only approach), improving the size and number of records for those nodes.\\n\\nThe cost-based approach is chosen if we set `spark.sql.cbo.enabled=true`. Besides, table and column statistics also need to be collected so that Spark can calculate based on it, by running **[ANALYZE](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html)**\\n\\n#### 3.4. Code generation\\n\\n![spark codegen](./images/catalyst-pipeline-codegen.PNG)\\n\\nAfter selecting the right physical plan to run, Catalyst will compile a tree of plans that support codegen into a single Java function, to Java bytecode to run on drivers and executors. This codegen greatly improves running speed when Spark SQL often works on in-memory datasets, data processing is often tied to the CPU.\\n\\n- Eliminates virtual function calls and thus, reduces the number of CPU instructions.\\n- Leverages CPU registers for intermediate data.\\n\\nCatalyst relies on a Scala feature, quasiquotes, to simplify this part of the codegen (quasiquotes allow building abstract syntax trees (ASTs), which then input the Scala compiler to generate bytecode).\\n\\n### 4. Spark session extension\\n\\nSpark session extension is an extension of Spark that allows us to customize parts of the Catalyst optimizer so that it works in each of our contexts.\\n\\n#### 4.1. Custom parser rule\\n\\nAs shown above, initially our query will have to go through the parsing set to check the validity of the query. Spark provides an interface that we can implement at this stage `ParserInterface`\\n\\n```scala\\npackage org.apache.spark.sql.catalyst.parser\\n\\n@DeveloperApi\\ntrait ParserInterface {\\n  @throws[ParseException](\\"Text cannot be parsed to a LogicalPlan\\")\\n  def parsePlan(sqlText: String): LogicalPlan\\n\\n  @throws[ParseException](\\"Text cannot be parsed to an Expression\\")\\n  def parseExpression(sqlText: String): Expression\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a TableIdentifier\\")\\n  def parseTableIdentifier(sqlText: String): TableIdentifier\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a FunctionIdentifier\\")\\n  def parseFunctionIdentifier(sqlText: String): FunctionIdentifier\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a multi-part identifier\\")\\n  def parseMultipartIdentifier(sqlText: String): Seq[String]\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a schema\\")\\n  def parseTableSchema(sqlText: String): StructType\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a DataType\\")\\n  def parseDataType(sqlText: String): DataType\\n\\n  @throws[ParseException](\\"Text cannot be parsed to a LogicalPlan\\")\\n  def parseQuery(sqlText: String): LogicalPlan\\n}\\n```\\n\\nWe will implement that interface and inject this rule into Spark job as follows\\n\\n```scala\\ncase class CustomerParserRule(sparkSession: SparkSession, delegateParser: ParserInterface) extends ParserInterface {\\n  /* Overwrite those methods here */\\n}\\n\\nval customerParserRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectParser(CustomerParserRule)\\n}\\n```\\n\\n#### 4.2. Custom analyzer rule\\n\\nAnalyzer rule includes several types of rules such as resolution rule, check rule. These rules are injected through functions\\n\\n- `injectResolutionRule`: inject our rules for the resolution phase.\\n- `injectPostHocResolutionRule`: run our rules after the resolution phase.\\n- `injectCheckRule`: add rules to check some logic of logical plans, for example, we want to check business logic, or check which rules have finished running,...\\n\\nTo inject resolution rule, we extend an abstract class of Spark `Rule[LogicalPlan]`\\n\\n```scala\\ncase class CustomAnalyzerResolutionRule(sparkSession: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    /* Code for resolution rule */\\n  }\\n}\\n\\nval customAnalyzerResolutionRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectResolutionRule(CustomAnalyzerResolutionRule)\\n}\\n```\\n\\nTo inject check rule, we inherit the class `Function1[LogicalPlan, Unit]`\\n\\n```scala\\ncase class CustomAnalyzerCheckRule(sparkSession: SparkSession) extends (LogicalPlan => Unit) {\\n  override def apply(plan: LogicalPlan): Unit = {\\n    /* Code for check rule */\\n  }\\n}\\n\\nval customAnalyzerCheckRuleFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectCheckRule(CustomAnalyzerCheckRule)\\n}\\n```\\n\\n#### 4.3. Custom optimization\\n\\nTo customize the logical plan optimization phase, we will inherit the abstract class `Rule[LogicalPlan]`\\n\\n```scala\\ncase class CustomOptimizer(sparkSession: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    /* Code for custom logical optimier */\\n  }\\n}\\n\\nval customOptimizerFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectOptimizerRule(CustomOptimizer)\\n}\\n```\\n\\n#### 4.4. Custom physical planning\\n\\nTo configure the running strategy for Spark Catalyst optimizer, we inherit the abstract class `SparkStrategy` and implement the method `apply` of that class\\n\\n```scala\\ncase class CustomStrategy(sparkSession: SparkSession) extends SparkStrategy {\\n  override def apply(plan: LogicalPlan): Seq[SparkPlan] = {\\n    /* Code for custom spark strategy/physical planning */\\n  }\\n}\\n\\nval customStrategyFunc: SparkSessionExtensions => Unit = (extensionBuilder: SparkSessionExtensions) => {\\n  extensionBuilder.injectPlannerStrategy(CustomStrategy)\\n}\\n```\\n\\n#### 4.5. Example code to configuree logical plan optimization phase in Catalyst optimizer\\n\\nIn this section, I will make an example of changing logical plan optimization phase with Spark extension. A simple extension with code as below\\n\\n```scala\\n/* class CustomProjectFilterExtension ======================================= */\\npackage extensions\\nimport org.apache.spark.sql.SparkSession\\nimport org.apache.spark.sql.catalyst.plans.logical._\\nimport org.apache.spark.sql.catalyst.rules.Rule\\n// create an extension that\\ncase class CustomProjectFilterExtension(spark: SparkSession) extends Rule[LogicalPlan] {\\n  override def apply(plan: LogicalPlan): LogicalPlan = {\\n    val fixedPlan = plan transformDown {\\n      case Project(expression, Filter(condition, child)) =>\\n          Filter(condition, child)\\n    }\\n    fixedPlan\\n  }\\n}\\n\\n/* class AllExtensions ======================================= */\\npackage extensions\\nimport org.apache.spark.sql.SparkSessionExtensions\\n// inject the extension to SparkSessionExtensions\\nclass AllExtensions extends (SparkSessionExtensions => Unit) {\\n  override def apply(ext: SparkSessionExtensions): Unit = {\\n    ext.injectOptimizerRule(CustomProjectFilterExtension)\\n  }\\n}\\n```\\n\\nThe above class `CustomProjectFilterExtension` transforms Filter (row filter), Project (select column while scanning file) operators to only Filter. Then, even though we have selected the column, it still scans all the columns of the file in the storage.\\n\\nCompile project\\n\\n```bash\\n# compile jar file\\nmvn clean package && mvn dependency:copy-dependencies\\n```\\n\\n##### 4.5.1. When not applying extension\\n\\nWe initialize `spark-shell` without passing extension\\n\\n```bash\\n# initialize spark-shell\\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr \' \' \',\'),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar\\n\\n# check spark.sql.extensions\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = null\\n\\n# explain a query that contains Filter and Project operators\\nscala> spark.sql(\\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel=\'Resort Hotel\') a\\").explain(extended = true)\\n\\n== Parsed Logical Plan ==\\n\'Project [\'hotel, \'is_canceled]\\n+- \'SubqueryAlias a\\n   +- \'Project [*]\\n      +- \'Filter (\'hotel = Resort Hotel)\\n         +- \'UnresolvedRelation [test, hotel_bookings], [], false\\n\\n== Analyzed Logical Plan ==\\nhotel: string, is_canceled: bigint\\nProject [hotel#0, is_canceled#1L]\\n+- SubqueryAlias a\\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\\n      +- Filter (hotel#0 = Resort Hotel)\\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Optimized Logical Plan ==\\nProject [hotel#0, is_canceled#1L]\\n+- Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n   +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Physical Plan ==\\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint>\\n```\\n\\nWe see that `Optimized Logical Plan` there are both Project and Filter operations, because we filter `WHERE hotel=\'Resort Hotel\'` and project `SELECT hotel, is_canceled`. Therefore, in the physical plan, it only scans 2 columns `FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L]`.\\n\\n##### 4.5.2. When applying extension\\n\\n```bash\\n# initialize spark-shell with extension\\n$SPARK_330/bin/spark-shell --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr \' \' \',\'),/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/custom-extension-1.0-SNAPSHOT.jar --conf spark.sql.extensions=extensions.AllExtensions\\n\\n# check spark.sql.extensions\\nscala> spark.conf.get(\\"spark.sql.extensions\\")\\nres0: String = extensions.AllExtensions\\n\\n# explain a query that contains Filter and Project operators\\nscala> spark.sql(\\"SELECT hotel, is_canceled FROM (SELECT * FROM test.hotel_bookings WHERE hotel=\'Resort Hotel\') a\\").explain(extended = true)\\n\\n== Parsed Logical Plan ==\\n\'Project [\'hotel, \'is_canceled]\\n+- \'SubqueryAlias a\\n   +- \'Project [*]\\n      +- \'Filter (\'hotel = Resort Hotel)\\n         +- \'UnresolvedRelation [test, hotel_bookings], [], false\\n\\n== Analyzed Logical Plan ==\\nhotel: string, is_canceled: bigint\\nProject [hotel#0, is_canceled#1L]\\n+- SubqueryAlias a\\n   +- Project [hotel#0, is_canceled#1L, lead_time#2L, arrival_date_year#3L, arrival_date_month#4, arrival_date_week_number#5L, arrival_date_day_of_month#6L, stays_in_weekend_nights#7L, stays_in_week_nights#8L, adults#9L, children#10, babies#11L, meal#12, country#13, market_segment#14, distribution_channel#15, is_repeated_guest#16L, previous_cancellations#17L, previous_bookings_not_canceled#18L, reserved_room_type#19, assigned_room_type#20, booking_changes#21L, deposit_type#22, agent#23, ... 8 more fields]\\n      +- Filter (hotel#0 = Resort Hotel)\\n         +- SubqueryAlias spark_catalog.test.hotel_bookings\\n            +- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Optimized Logical Plan ==\\nFilter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- Relation test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] parquet\\n\\n== Physical Plan ==\\n*(1) Filter (isnotnull(hotel#0) AND (hotel#0 = Resort Hotel))\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields] Batched: true, DataFilters: [isnotnull(hotel#0), (hotel#0 = Resort Hotel)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(hotel), EqualTo(hotel,Resort Hotel)], ReadSchema: struct<hotel:string,is_canceled:bigint,lead_time:bigint,arrival_date_year:bigint,arrival_date_mon...\\n```\\n\\nAt this point, `Optimized Logical Plan` there is no longer Project operator, but only Filter operator, so that when it comes to the physical plan step, it scans all the columns in the table `FileScan parquet test.hotel_bookings[hotel#0,is_canceled#1L,lead_time#2L,arrival_date_year#3L,arrival_date_month#4,arrival_date_week_number#5L,arrival_date_day_of_month#6L,stays_in_weekend_nights#7L,stays_in_week_nights#8L,adults#9L,children#10,babies#11L,meal#12,country#13,market_segment#14,distribution_channel#15,is_repeated_guest#16L,previous_cancellations#17L,previous_bookings_not_canceled#18L,reserved_room_type#19,assigned_room_type#20,booking_changes#21L,deposit_type#22,agent#23,... 8 more fields]`.\\n\\nAbove, I have specifically presented the components of Spark Catalyst optimizer and how to write spark session extensions to intervene to change Catalyst\'s plans, there are also specific code examples to demonstrate this. In the next article, I will present one more part that is a new feature in Spark 3.0, which is Spark Adaptive Query Execution, a feature that improves Spark job speed at runtime.\\n\\n### 5. References\\n\\n[Deep Dive into Spark SQL\'s Catalyst Optimizer](https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\\n\\n[Spark Catalyst Pipeline: A Deep Dive Into Spark\u2019s Optimizer](https://www.unraveldata.com/resources/catalyst-analyst-a-deep-dive-into-sparks-optimizer/)\\n\\n[Extending Apache Spark Catalyst for Custom Optimizations](https://medium.com/@pratikbarhate/extending-apache-spark-catalyst-for-custom-optimizations-9b491efdd24f)"},{"id":"mysql-series-mysql-indexing","metadata":{"permalink":"/blogs/blog/mysql-series-mysql-indexing","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-10-15-mysql-indexing/index.md","source":"@site/blog/2022-10-15-mysql-indexing/index.md","title":"MySQL series - Indexing","description":"Indexing is a method to make queries faster, which is a very important part of improving performance. For large data tables, precise indexing will increase the query speed as a whole, however, this is often not taken into account in the table design process. This article talks about the types of indexes and how to properly index them.","date":"2022-10-15T00:00:00.000Z","formattedDate":"October 15, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blogs/blog/tags/my-sql"},{"label":"Database","permalink":"/blogs/blog/tags/database"},{"label":"Data Engineering","permalink":"/blogs/blog/tags/data-engineering"},{"label":"Indexing","permalink":"/blogs/blog/tags/indexing"}],"readingTime":10.425,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-indexing","title":"MySQL series - Indexing","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Indexing"],"image":"./images/indexing.PNG"},"prevItem":{"title":"Spark Catalyst Optimizer And Spark Session Extension","permalink":"/blogs/blog/spark-catalyst-optimizer-and-spark-session-extension"},"nextItem":{"title":"MySQL series - Multiversion concurrency control","permalink":"/blogs/blog/mysql-series-mysql-mvcc"}},"content":"Indexing is a method to make queries faster, which is a very important part of improving performance. For large data tables, precise indexing will increase the query speed as a whole, however, this is often not taken into account in the table design process. This article talks about the types of indexes and how to properly index them.\\n\\n![Indexing](./images/indexing.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Types of index\\n\\nThere are many types of indexes designed for different purposes. Remember, indexes are implemented at the storage engine layer, not at the server layer, so they behave differently in different storage engines. The types of indexes in this article are mainly about indexes in InnoDB.\\n\\n#### 1.1. B-tree index\\n\\nB-tree index uses a balanced tree to store its data, almost all MySQL storage engines support this type of index (or its variant), for example, the NDB Cluster storage engine uses the data structure T-tree for indexing, InnoDB uses B+ tree,...\\n\\nIn B-tree, all values \u200b\u200bare sorted, and leaves are equally spaced from the root of the tree. Below figure is a description of the B-tree data structure.\\n\\n![B Tree](./images/BTree.PNG)\\n\\nB-trees provide the ability to search, access sequential data, insert and delete with logarithmic time complexity ${O(log(n))}$. At the root node, there will be pointers to the child nodes, when we query, the storage engine will know the appropriate subnode branch to browse by looking at the values \u200b\u200bin the node pages, containing the upper and lower threshold information, child nodes in that page. At the leaf page layer, pointers point to data instead of other pages.\\n\\nIn the image above, we only see a node page and leaf pages. In fact, the B-tree has many layers of node pages between the root node and the leaf nodes, the size of the tree depends on the size of the indexed table.\\n\\n##### 1.1.1. Adaptive hash index\\n\\nWhen index values \u200b\u200bare accessed with high frequency, InnoDB will build a hash index for them in memory on top of the B-tree index, making it possible to find this hash value very quickly and efficiently. This mode is automatic by InnoDB, however, you can still disable adaptive hash index if you want.\\n\\n##### 1.1.2. Types of query that are efficient with B-tree index\\n\\nB-tree indexes work well with exact-value, range, or value-prefix query types. These queries are best when we use them on the leftmost column in the indexed set of columns.\\n\\n```sql\\nCREATE TABLE People (\\n     last_name varchar(50) not null,\\n     first_name varchar(50) not null,\\n     dob date not null,\\n     KEY `idx_full_col` (last_name, first_name, dob)\\n) ENGINE=InnoDB;\\n```\\n\\n- Exact match: when the columns in the index are queried to match a certain value, for example `WHERE last_name = \'lam\' AND first_name = \'tran\' AND dob = \'1999-05-10\'`. This type of query will return results very quickly.\\n- Match the leftmost column: for example, if we query to find people with `last_name = \'lam\'`.\\n- Match the first part of the left most column: For example, when we find the person whose last_name starts with the letter \'L\'.\\n- Match a range of values: when we need to get the set of people whose last_name is between \'anh\' and \'lam\'.\\n- Match the leftmost column and a range of the next column values: for example, when we need information about people last_name is \'lam\' and first_name starts with \'t\'.\\n\\n##### 1.1.3. Drawbacks of B-tree index\\n\\n- It won\'t really help when the query condition doesn\'t start with the leftmost column, nor is it good when the query finds people whose last_name ends with a specific letter.\\n- Queries that skip some columns also don\'t take full advantage of the index. For example when looking for people `last_name = \'lam\' AND dob = \'1999-05-10\'` with no condition on first_name.\\n- Indexes of this type will not take advantage of the columns behind the range matching column. For example, the query people `last_name = \'lam\' AND first_name LIKE \'t%\' AND dob = \'1999-05-10\'` will only apply the index on the last_name and first_name columns. For columns with less distinct data, we can overcome this by enumerating all values \u200b\u200binstead of accessing the range of values.\\n\\nThus, the order of the columns in the index is really important, you need to consider the query goal of the application before indexing the columns.\\n\\n#### 1.2. Full-text index\\n\\nThe full-text index searches for keywords in the text string instead of comparing the field\'s value directly. It aids in searching rather than judging what type the data matches. When a column has a full-text index, we can still type a B-tree index on that column.\\n\\n```sql\\nCREATE TABLE tutorial (\\n    id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,\\n    title VARCHAR(200),\\n    description TEXT,\\n    FULLTEXT `idx_full_text` (title,description)\\n) ENGINE=InnoDB;\\n```\\n\\nThe full-text index is used by syntax `MATCH() AGAINST()` with the parameter of `MATCH()` are columns to search, separated by commas. The parameter of `AGAINST()` is a string to search and type of search to perform.\\n\\n##### 1.2.1. Types of full-text index\\n\\n- Natural language search: this mode will interpret the search string as a phrase in natural human language. This mode does not count stopwords as well as words shorter than the minimum number of characters (default is 3 characters with InnoDB).\\n- Boolean search: interprets the search string using special query language rules. The string contains all the words to be searched, it can also contain special operators for advanced searches, such as a word that needs to appear in the string, or a word that is weighted heavier or lighter. Stop words will be ignored in this mode.\\n- Query expansion: is a variation of natural language search. The words in the most relevant rows returned will be added to the search string, and the search will be repeated. The query will return rows in the second search.\\n\\nI won\'t go into each type in detail, because I rarely use the full-text index.\\n\\n### 2. Benefits of indexing\\n\\nSome benefits of indexing\\n\\n- Index helps server save time for browsing and querying.\\n- Index helps the server avoid operations such as sorting data or creating temporary tables.\\n- Index turns random disk access into sequential access, improving read speed\\n\\nSome criteria to evaluate index\\n\\n- Index needs to arrange related rows, closer together.\\n- The sorted rows should be exactly what your application queries need.\\n- Index needs to contain all the columns that your application query filters.\\n\\n### 3. Indexing strategies\\n\\nCreating the right indexes will greatly improve your query speed, which in turn makes your application more responsive to users.\\n\\n#### 3.1. Prefix index for text field\\n\\nConsider Index Selectivity is the ratio between the number of different column values \u200b\u200b/ total records of the table. For columns with high Index Selectivity, indexing on these fields is very effective because MySQL will remove more records when filtering on those columns. For long text fields, we cannot index the whole column length because MySQL won\'t allow that, so we need to find a good enough prefix of that field to index and it will give us a good enough performance.\\n\\nTry with the product data below, we list the top ten sellers that appear the most\\n\\n```sql\\nselect productVendor, count(1) c from `classicmodels`.`products_index`\\ngroup by productVendor\\norder by c desc\\nLIMIT 10;\\n\\n+--------------------------------------------------+----+\\n| productVendor                                    | c  |\\n+--------------------------------------------------+----+\\n| Pressure and Safety Relief Valve                 | 10 |\\n| NEC United Solutions                             |  9 |\\n| SunGard Data Systems                             |  8 |\\n| Zhengzhou Esunny Information Technology Co.,Ltd. |  8 |\\n| Spring Support                                   |  8 |\\n| Ball and Plug Valve                              |  7 |\\n| LSAW Pipe                                        |  7 |\\n| Wood Mackenzie Ltd                               |  7 |\\n| Heat Recovery Steam Generator                    |  7 |\\n| Carbon Steel Flange                              |  7 |\\n+--------------------------------------------------+----+\\n```\\n\\nTry to calculate the frequency of occurrence of length 3 prefix with the field `productVendor`\\n\\n```sql\\nselect LEFT(productVendor, 3), count(1) c from `classicmodels`.`products_index`\\ngroup by LEFT(productVendor, 3)\\norder by c desc\\nLIMIT 10;\\n\\n+------------------------+----+\\n| LEFT(productVendor, 3) | c  |\\n+------------------------+----+\\n| Sha                    | 44 |\\n| Car                    | 16 |\\n| Sun                    | 15 |\\n| Zhe                    | 13 |\\n| Gas                    | 12 |\\n| Sto                    | 11 |\\n| Pre                    | 11 |\\n| Col                    | 11 |\\n| She                    |  9 |\\n| Hea                    |  9 |\\n+------------------------+----+\\n```\\n\\nWe see that the frequency of occurrence of length 3 prefix is a lot more compare to full column values, which equates to fewer distinct values, which equates to a much smaller Index Selectivity. So prefix 3 is not a good choice\\n\\nLet\'s calculate the Index Selectivity with various prefix lengths\\n\\n```sql\\nselect COUNT(DISTINCT LEFT(productVendor, 3))/COUNT(1) AS selectivity_3,\\nCOUNT(DISTINCT LEFT(productVendor, 4))/COUNT(1) AS selectivity_4,\\nCOUNT(DISTINCT LEFT(productVendor, 5))/COUNT(1) AS selectivity_5,\\nCOUNT(DISTINCT LEFT(productVendor, 6))/COUNT(1) AS selectivity_6,\\nCOUNT(DISTINCT LEFT(productVendor, 7))/COUNT(1) AS selectivity_7,\\nCOUNT(DISTINCT LEFT(productVendor, 8))/COUNT(1) AS selectivity_8,\\nCOUNT(DISTINCT LEFT(productVendor, 9))/COUNT(1) AS selectivity_9,\\nCOUNT(DISTINCT LEFT(productVendor, 10))/COUNT(1) AS selectivity_10,\\nCOUNT(DISTINCT LEFT(productVendor, 11))/COUNT(1) AS selectivity_11,\\nCOUNT(DISTINCT productVendor)/COUNT(1) AS selectivity\\nfrom `classicmodels`.`products_index`;\\n\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n| selectivity_3 | selectivity_4 | selectivity_5 | selectivity_6 | selectivity_7 | selectivity_8 | selectivity_9 | selectivity_10 | selectivity_11 | selectivity |\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n|        0.1982 |        0.2164 |        0.2218 |        0.2236 |        0.2236 |        0.2273 |        0.2309 |         0.2491 |         0.2509 |      0.2600 |\\n+---------------+---------------+---------------+---------------+---------------+---------------+---------------+----------------+----------------+-------------+\\n```\\n\\nWe see that the selectivity prefix 11 is very close to the column selectivity value, and is also quite suitable for long text fields like this column, so choosing prefix 11 will balance the size of the index as well as the speed of the query.\\n\\n```sql\\nALTER TABLE `classicmodels`.`products_index` ADD KEY (productVendor(11));\\n```\\n\\n#### 3.2. Index on multiple columns\\n\\nSome mistakes when indexing is indexing each column separately, and creating indexes for all columns in the WHERE statement.\\n\\n```sql\\nCREATE TABLE t (\\n     c1 INT,\\n     c2 INT,\\n     c3 INT,\\n     KEY(c1),\\n     KEY(c2),\\n     KEY(c3)\\n);\\n```\\n\\nSeparate indexes like the one above will usually not optimize performance very much in most situations, because then MySQL can use a tactic called index merge. Index merge will use all the indexes in the query, scan the indexes simultaneously, then merge the results again.\\n\\n- Union index will be used for OR condition\\n- Intersection index will be used for AND condition\\n- Union of intersection index for the union of both 2\\n\\nHere is an example query on 2 index fields but MySQL uses index merge\\n\\n```sql\\nmysql> explain select * from `classicmodels`.`products_index` where productVendor = \'Infor Global Solutions\' OR productScale = \'1:10\'\\\\G\\n*************************** 1. row ***************************\\n           id: 1\\n  select_type: SIMPLE\\n        table: products_index\\n   partitions: NULL\\n         type: index_merge\\npossible_keys: productVendor,productScale\\n          key: productVendor,productScale\\n      key_len: 14,12\\n          ref: NULL\\n         rows: 33\\n     filtered: 100.00\\n        Extra: Using sort_union(productVendor,productScale); Using where\\n```\\n\\nSome considerations when query encounters index merge\\n\\n- If the server intersects the index (AND condition on the indexes), it means that you can create an index containing all the columns related to each other, not each index for each column.\\n- If the server union index (OR condition on the indexes), check if those columns have high Index Selectivity, if the Index Selectivity in some columns is low, it means that the column has few different values, that is, the scan index returns more records for the merge operations that follow it, consuming more CPU and memory. Sometimes, rewriting the query with the UNION statement gives better results than when the server unions the indexes in the index merge.\\n\\nWhen you see the index merge in the EXPLAIN statement, review the query and table structure to check if the current design is optimal.\\n\\n#### 3.3. Choose the correct order of columns to index\\n\\nWhen our index contains many columns, the order of columns in that index is very important, because in B-tree index, the index will be sorted from the leftmost column to the next columns (some disadvantages of B- tree index **[here](#113-drawbacks-of-b-tree-index)**). Therefore, we often choose the columns with the highest Index Selectivity as the leftmost column, order the columns in descending order of Index Selectivity, so that our overall index has high selectivity.\\n\\n```sql\\nselect count(distinct productVendor)/count(1),\\n\\tcount(distinct productScale)/count(1)\\nfrom `classicmodels`.`products_index`;\\n\\n+----------------------------------------+---------------------------------------+\\n| count(distinct productVendor)/count(1) | count(distinct productScale)/count(1) |\\n+----------------------------------------+---------------------------------------+\\n|                                 0.2600 |                                0.0145 |\\n+----------------------------------------+---------------------------------------+\\n```\\n\\nIn the above example, if we index 2 columns `productVendor` and `productScale`, we will usually take `productVendor` as the leftmost column\\n\\n```sql\\nalter table `classicmodels`.`products_index` add key (productVendor, productScale);\\n```\\n\\nSome more considerations about the index to pay attention to such as clustered index, covering index, remove redundant, unused indexes, ... I would like to mention in another article."},{"id":"mysql-series-mysql-mvcc","metadata":{"permalink":"/blogs/blog/mysql-series-mysql-mvcc","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-10-07-mysql-multiversion-concurrency-control/index.md","source":"@site/blog/2022-10-07-mysql-multiversion-concurrency-control/index.md","title":"MySQL series - Multiversion concurrency control","description":"Usually storage engines do not use a simple row lock mechanism, to achieve good performance in a highly concurrent read and write environment, storage engines implement row locking with a certain complexity, the method is often used, is multiversion concurrency control (MVCC).","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blogs/blog/tags/my-sql"},{"label":"Database","permalink":"/blogs/blog/tags/database"},{"label":"Data Engineering","permalink":"/blogs/blog/tags/data-engineering"},{"label":"Transaction","permalink":"/blogs/blog/tags/transaction"}],"readingTime":2.035,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-mvcc","title":"MySQL series - Multiversion concurrency control","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Transaction"],"image":"./images/overall.PNG"},"prevItem":{"title":"MySQL series - Indexing","permalink":"/blogs/blog/mysql-series-mysql-indexing"},"nextItem":{"title":"MySQL series - Transaction In MySQL","permalink":"/blogs/blog/mysql-series-mysql-transaction"}},"content":"Usually storage engines do not use a simple row lock mechanism, to achieve good performance in a highly concurrent read and write environment, storage engines implement row locking with a certain complexity, the method is often used, is multiversion concurrency control (MVCC).\\n\\n![MVCC Overall](./images/overall.PNG)\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Introduction to MVCC\\n\\nMVCC is used in many types of relational databases, it helps us to lock as little data as possible when doing many transactions at once, it can allow us to not be locked when reading data and only lock the necessary rows when writing data.\\n\\n### 2. MVCC in InnoDB\\n\\nMVCC works by taking snapshots of data at some point in time, so a transaction can see the same data no matter how fast or long it takes. However, it also causes different transactions to see different data views of the same table at the same time.\\n\\n![MVCC Detail Example](./images/detail.PNG)\\n\\nInnoDB will assign a transaction id to a transaction every time it starts reading some data. The changes of a record in that transaction will be written to the undo log for data revert, and the rollback pointer of that transaction will point to the location of that undo log. When another session starts reading the mutated record above, InnoDB compares the transaction id of the record with the data view that new session read. If the record is in a state that is invisible to other transactions (e.g. a transaction that changes that record has not been committed), the undo log will be applied on the data view until the record becomes available again, readable by other transactions.\\n\\nAll undo logs recorded are copied to the redo log because they are used for data recovery in the event of a system failure. The size of the undo log and redo log also affects the ability to perform read and write in environments with high concurrent read and write.\\n\\nWhile the benefit is that we are never locked when reading, the storage engine needs to store more data with each record, do more control work, and perform more operations.\\n\\n### 3. Isolation level with MVCC\\n\\nMVCC is only available with REPEATABLE READ and READ COMMITTED modes. MVCC is not compatible with READ UNCOMMITTED because queries will not read records whose version does not match the transaction\'s version. MVCC is not compatible with SERIALIZABLE because of its read locking (For isolation level modes, you can find them **[here](/blog/2022-10-06-mysql-transaction/index.md#3-4-isolation-level-trong-m\xf4i-tr\u01b0\u1eddng-c\xf3-nhi\u1ec1u-\u0111\u1ecdc-ghi-\u0111\u1ed3ng-th\u1eddi)**)."},{"id":"mysql-series-mysql-transaction","metadata":{"permalink":"/blogs/blog/mysql-series-mysql-transaction","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-10-06-mysql-transaction/index.md","source":"@site/blog/2022-10-06-mysql-transaction/index.md","title":"MySQL series - Transaction In MySQL","description":"Poster","date":"2022-10-06T00:00:00.000Z","formattedDate":"October 6, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blogs/blog/tags/my-sql"},{"label":"Database","permalink":"/blogs/blog/tags/database"},{"label":"Data Engineering","permalink":"/blogs/blog/tags/data-engineering"},{"label":"Transaction","permalink":"/blogs/blog/tags/transaction"}],"readingTime":5.34,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-transaction","title":"MySQL series - Transaction In MySQL","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering","Transaction"],"image":"./images/transaction.JPEG"},"prevItem":{"title":"MySQL series - Multiversion concurrency control","permalink":"/blogs/blog/mysql-series-mysql-mvcc"},"nextItem":{"title":"MySQL series - MySQL Architecture Overview","permalink":"/blogs/blog/mysql-series-mysql-architecture"}},"content":"![Poster](./images/transaction.JPEG)\\n\\nThe next article in the MySQL series is about transactions. A very common operation in MySQL in particular and relational databases in general. Let\'s go to the article.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. What is transaction?\\n\\nA transaction is a set of SQL statements put together as a unit of work. If the database successfully runs all SQL statements in that group, it is considered successful. If one of the SQL commands fails, all the SQL commands that have been run or not run will have no effect on the database. An example of a set of SQL statements wrapped in a transaction follows\\n\\n```sql\\n    1  START  TRANSACTION;\\n    2  SELECT balance FROM checking WHERE customer_id = 10233276;\\n    3  UPDATE checking SET balance = balance - 200.00 WHERE customer_id = 10233276;\\n    4  UPDATE savings SET balance = balance - 200.00 WHERE customer_id = 10233276;\\n    5  COMMIT;\\n```\\n\\nTransactions are started by START TRANSACTION and are usually closed by COMMIT (confirm transaction) or ROLLBACK (return to pre-transaction state). If the ${4^{th}}$ statement fails, the ${3^{rd}}$ statement will be rolledback and nothing will happen to affect the old data.\\n\\n### 2. Four data preservation properties in relational database\\n\\n![ACID](./images/acid.PNG)\\n\\nEvery system needs to satisfy four ACID properties to ensure data preservation\\n\\n- Atomicity\\n\\nTransaction needs to act as a unit of work. Either all SQL statements in the transaction are applied or none are applied.\\n\\n- Consistency\\n\\nDatabase needs to be consistent, only being moved from one consistent state to another. The example above, if the error occurs after running the ${3^{rd}}$ statement, the checking account will not lose 200$ when the transaction has not been committed. The total money in the two accounts before and after the transaction remains the same.\\n\\n- Isolation\\n\\nThe result of this transaction will be invisible to other transactions when this transaction is not finished, not committed. For example, when transaction 1 is running between ${3^{rd}}$ and ${4^{th}}$ statements above, another transaction that summarizes the balances of the accounts will still see 200$ in the checking account. When a transaction is uncommitted, no changes will affect the database.\\n\\n- Durability\\n\\nOnce committed, the changes made by the transaction will be permanent, the changes need to be recorded to ensure that the data is not lost if the system fails.\\n\\n### 3. Four isolation level in highly concurrent read and write environments\\n\\nThere are 4 isolation levels related to transactions\\n\\n- READ UNCOMMITTED\\n\\nIn this mode, transactions can see the results of other uncommitted transactions. This mode does not perform much faster than many other modes but easily causes problems when reading wrong data.\\n\\n- READ COMMITTED\\n\\nThe default mode of most databases (but not MySQL), it will lose some of the ACID Isolation properties, this transaction will be visible to changes by other transactions committed after this transaction starts, however changes to this transaction remain invisible until it is committed. This can cause two identical read statements in a transaction to return two different datasets.\\n\\n- REPEATABLE READ\\n\\nThis mode is the default of MySQL. It ensures that within the same transaction, the same read statements will return the same result. But there will also be a small problem that if we select a range of values, another transaction inserts a new record in that range, we will see that new record. Storage engines like InnoDB, XtraDB solve this problem by creating multiple versions of a record that manage concurrent reads and writes.\\n\\n- SERIALIZABLE\\n\\nThis mode solves the problem of reading a range of values \u200b\u200babove by running transactions in order. This mode will lock all the rows it reads, a lot of timeouts and locking occur frequently, concurrent reads and writes will be reduced.\\n\\n![Isolation Level](./images/isolation_levels.PNG)\\n\\n### 4. Transaction deadlock\\n\\nDeadlock occurs when two or more transactions lock the same resources, creating a cycle of dependency\\n\\n```sql\\n-- Transaction 1\\n    START TRANSACTION;\\n    UPDATE StockPrice SET close = 45.50 WHERE stock_id = 4 and date = \u20182020-05-01\u2019;\\n    UPDATE StockPrice SET close = 19.80 WHERE stock_id = 3 and date = \u20182020-05-02\u2019;\\n    COMMIT;\\n-- Transaction 2\\n    START TRANSACTION;\\n    UPDATE StockPrice SET high = 20.12 WHERE stock_id = 3 and date = \u20182020-05-02\u2019;\\n    UPDATE StockPrice SET high = 47.20 WHERE stock_id = 4 and date = \u20182020-05-01\u2019;\\n    COMMIT;\\n```\\n\\nAfter these two transactions finish running the first command, when running the second command. The records with the corresponding id of this transaction are being locked by another transaction, as well as another transaction that is locked by this transaction. InnoDB will return an error if a dependency circle is detected. The way InnoDB handles deadlock is that it will rollback the transaction with the fewest locked rows.\\n\\n![Deadlock](./images/deadlock.JPEG)\\n\\n### 5. Transaction logging\\n\\nTransaction logging makes transaction execution more efficient. Instead of updating directly to the disk table every time there is a change, it updates to the copy of the data in memory. Then the transaction log will be written to disk with append mode, this operation is very fast because only sequential I/O is required in disk, more cost-effective, after a while these changes will be applied to the actual data on disk. Because this log is written on disk, it will be durable, if the system fails after writing the transaction log to disk but before updating the changes to the main data, the storage engine can still recover those changes.\\n\\n![Transaction Log](./images/transaction_log.PNG)\\n\\n### 6. Autocommit\\n\\nBy default, INSERT, UPDATE, and DELETE statements are wrapped in temporary transactions and committed as soon as they run, this is AUTOCOMMIT mode. To enable this mode run the sentence SET AUTOCOMMIT = 1; otherwise, SET AUTOCOMMIT = 0. Some special commands can commit a transaction while in an open transaction, such as DDL statements. We can set the isolation level for MySQL by running the SET TRANSACTION ISOLATION LEVEL command, after running this isolation level will take effect in subsequent transactions. You can set it in the configuration file for the whole server, or just set it in your session\\n\\n```sql\\nSET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;\\n```\\n\\nWe should not process tables with different storage engines in the same transaction, because there are some storage engines that will not support data rollback (MyISAM storage engine), if some error occurs during transaction execution, only some tables will be rolled back causing loss of consistency.\\n\\nThis is the end of the article, see you in the next blogs."},{"id":"mysql-series-mysql-architecture","metadata":{"permalink":"/blogs/blog/mysql-series-mysql-architecture","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-10-04-mysql-architecture/index.md","source":"@site/blog/2022-10-04-mysql-architecture/index.md","title":"MySQL series - MySQL Architecture Overview","description":"Hello everyone, recently, I did some research in MySQL because I think whoever doing data engineering should go in-depth with a certain relational database. Once you get a deep understanding of one RDBMS, you can easily learn the other RDBMS since they have many similarities. For the next few blogs, I will have a series about MySQL, and this is the first article.","date":"2022-10-04T00:00:00.000Z","formattedDate":"October 4, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"MySQL","permalink":"/blogs/blog/tags/my-sql"},{"label":"Database","permalink":"/blogs/blog/tags/database"},{"label":"Data Engineering","permalink":"/blogs/blog/tags/data-engineering"}],"readingTime":4.45,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"mysql-series-mysql-architecture","title":"MySQL series - MySQL Architecture Overview","authors":"tranlam","tags":["Bigdata","MySQL","Database","Data Engineering"],"image":"./images/architecture.PNG"},"prevItem":{"title":"MySQL series - Transaction In MySQL","permalink":"/blogs/blog/mysql-series-mysql-transaction"},"nextItem":{"title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","permalink":"/blogs/blog/spark-kafka-docker"}},"content":"Hello everyone, recently, I did some research in MySQL because I think whoever doing data engineering should go in-depth with a certain relational database. Once you get a deep understanding of one RDBMS, you can easily learn the other RDBMS since they have many similarities. For the next few blogs, I will have a series about MySQL, and this is the first article.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. MySQL architecture components\\n\\nMySQL is widely used not only in small applications but also in large enterprises, thanks to the features of its flexible architecture.\\n\\n![Architecture](./images/architecture.PNG)\\n\\nThe top layer is the Clients layer, this layer is usually not unique to MySQL. They are services like connection handling, authentication, security, etc.\\n\\nThe second layer is the layer that contains the code for query analysis, optimization and contains built-in functions that interact with the database such as dates, times, math, encryption, etc. All features are compatible with many storage engines like stored procedures, triggers, views, etc.\\n\\nThe third layer is storage engines, which are responsible for storing and retrieving data in MySQL. Each storage engine has its own good and bad sides. The MySQL server interacts with them by the storage engine API, which contains many low-level functions, operations such as starting a transaction, finding records with the corresponding primary key. Storage engines only respond to requests from the server, while parsing queries are made at the second layer.\\n\\n### 2. Connection and security management\\n\\nWith the default configuration, each connection from the client will occupy one thread, and queries will run in that thread. The server will have a cache of threads ready to use, so they won\'t need to be created and destroyed every time there is a new connection from the client.\\n\\nWhen the client connects, the server will need to authenticate that connection based on the host, username, and password. After connecting, the server will check whether the client has permissions to specific database resources (eg, SELECT permission on which table on which database,\u2026).\\n\\n### 3. MySQL optimizer\\n\\n![Overall](./images/overall.PNG)\\n\\nWhen running, MySQL will\\n\\n- Looking in the query cache to check if the query\'s results can be found, it will return the results immediately, otherwise it performs the next steps. The memory size of the query cache is assigned in the variable `query_cache_size`, if this variable is updated, MySQL will clear all cached queries one by one and re-initialize the query cache (this can be time consuming).\\n- Parse the queries into a tree containing the query\'s information. The query can be completely rewritten, the order in which the tables are read will be different, how the index should be selected, etc. We can interfere with that analysis by using hints to determine the order. What will it be like to run? At this time, the parser will build a parse tree for the query, besides, it also checks the syntax of the query.\\n- The preprocessor will check some other constraints such as this table, column or database exist or not, the user\'s authority to query to which resources.\\n- Then, the parse tree will be passed through an optimizer to convert it into a query execution plan. A MySQL query can be run in many ways, the optimizer will try to optimize the cost as much as possible (unit is 4KB data page), this cost can be seen by running `SHOW STATUS LIKE \'Last_query_cost\';`. The optimizer doesn\'t really care which storage engines are used, but the choice of storage engine has a big impact on how well the server optimizes the queries because the server needs information from storage engines such as the statistics of the tables, the cost of performing the operations, how the index is supported or the computing power of the storage engines to run more optimally. The optimizer may not be able to choose the best plan to run because the statistics from the storage engines are not absolute, the cost metric may not be equivalent to the cost of running the query, MySQL will try to reduce the cost but not the speed of the query, or user-defined functions will not be evaluated by the optimizer.\\n- The Query execution plan is a tree that contains each step to generate the results for the query, the server will perform those steps many times until there are no more records to retrieve. Query execution engine communicates with storage engines by storage engine APIs to perform operations according to the query execution plan.\\n- MySQL storage engines are a management system with each database being a subpath in that filesystem\'s data path. When creating a table, the table information is stored in the .frm file (for example, the table `users` is stored in the .frm file named `user.frm`).\\n- Next, the query will be run and return the results to the client. MySQL also stores the results of the query in the query cache.\\n\\nFinally, re-caching the results of frequently used queries can improve performance. Previously, MySQL had a query cache in its architecture, as a bottleneck in highly concurrent read and write environments, this query cache in new versions has been deprecated, instead, we often use other data cache methods such as Redis, ...\\n\\nThe above is an overview of the MySQL architecture and the process of running a query. See you in the next posts."},{"id":"spark-kafka-docker","metadata":{"permalink":"/blogs/blog/spark-kafka-docker","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-09-11-spark-kafka-docker/index.md","source":"@site/blog/2022-09-11-spark-kafka-docker/index.md","title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","description":"Architecture","date":"2022-09-11T00:00:00.000Z","formattedDate":"September 11, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"},{"label":"Kafka","permalink":"/blogs/blog/tags/kafka"},{"label":"Docker","permalink":"/blogs/blog/tags/docker"}],"readingTime":8.67,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"spark-kafka-docker","title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","authors":"tranlam","tags":["Bigdata","Spark","Apache","Kafka","Docker"],"image":"./images/architecture.PNG"},"prevItem":{"title":"MySQL series - MySQL Architecture Overview","permalink":"/blogs/blog/mysql-series-mysql-architecture"},"nextItem":{"title":"Create A Standalone Spark Cluster With Docker","permalink":"/blogs/blog/spark-cluster-docker"}},"content":"![Architecture](./images/architecture.PNG)\\n\\nHi guys, I\'m back after a long time without writing anything. Today, I want to share about how to create a Spark Streaming pipeline that consumes data from Kafka, everything is built on Docker.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Design overview\\n\\nThe model is containerized by Docker. Includes the following components\\n\\n- Producer: is a Kafka Producer that produces fake data about an user information using Java Faker and produce messages onto Kafka.\\n- Kafka cluster: includes brokers to store data and Zookeeper to manage those brokers.\\n- Spark cluster: is a Spark cluster consisting of 3 nodes: 1 driver and 2 workers to consume data from Kafka.\\n- Schema Registry: provides a restful interface to store and retrieve schemas, helping Kafka producers and consumers work together according to standards. Since the two ends of producing and consuming messages from two Kafka ends are independent, the consumer does not need to know how the producer sends the message with the format, the Schema Registry acts as an intermediary for the two parties to register the message format with each other, avoiding system errors.\\n- Postgres: is the database to provide configurations for the Spark Streaming application and in this article is also the place to store the streaming data after processing by Spark.\\n\\n### 2. Build necessary Docker images and containers\\n\\n#### 2.1. Create a Spark cluster\\n\\nAs in the **[previous article](/blog/2022-01-01-spark-cluster-docker/index.md)** I wrote about how to build a Spark cluster on Docker, in this article I take advantage of that cluster. However, there is a slight change, leaving out some things to fit this article. You can find the build image script **[here](https://github.com/lam1051999/spark_kafka_docker/tree/main/spark_cluster)**. So we have the necessary images for the Spark cluster. Here is the container configuration in docker-compose.yml\\n\\n```yml\\nspark-master:\\n  image: spark-master\\n  container_name: spark-master\\n  ports:\\n    - 8080:8080\\n    - 7077:7077\\n    - 4040:4040\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\nspark-worker-1:\\n  image: spark-worker\\n  container_name: spark-worker-1\\n  environment:\\n    - SPARK_WORKER_CORES=1\\n    - SPARK_WORKER_MEMORY=1024m\\n  ports:\\n    - 18081:8081\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\n  depends_on:\\n    - spark-master\\nspark-worker-2:\\n  image: spark-worker\\n  container_name: spark-worker-2\\n  environment:\\n    - SPARK_WORKER_CORES=1\\n    - SPARK_WORKER_MEMORY=1024m\\n  ports:\\n    - 28081:8081\\n  volumes:\\n    - /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target:/execution_files\\n  depends_on:\\n    - spark-master\\n```\\n\\n#### 2.2. Add Zookeeper, Kafka, Postgres, Schema Registry containers\\n\\nNext will be on Zookeeper, Kafka, Postgres and Schema Registry containers\\n\\n```yml\\nzookeeper:\\n  image: confluentinc/cp-zookeeper:3.3.1\\n  container_name: zookeeper\\n  ports:\\n    - \\"2181:2181\\"\\n  environment:\\n    ZOOKEEPER_CLIENT_PORT: 2181\\n    ZOOKEEPER_TICK_TIME: 2000\\nkafka:\\n  image: confluentinc/cp-kafka:3.3.1\\n  container_name: kafka\\n  depends_on:\\n    - zookeeper\\n  ports:\\n    - \\"29092:29092\\"\\n  environment:\\n    KAFKA_BROKER_ID: 1\\n    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\\n    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\\n    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\\n    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\\n    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\\n\\ndb:\\n  image: postgres\\n  container_name: db-postgres\\n  volumes:\\n    - ./data/db:/var/lib/postgresql/data\\n  ports:\\n    - \\"5432:5432\\"\\n  environment:\\n    - POSTGRES_NAME=postgres\\n    - POSTGRES_USER=postgres\\n    - POSTGRES_PASSWORD=postgres\\n\\nschema-registry:\\n  image: confluentinc/cp-schema-registry:3.3.1\\n  container_name: schema-registry\\n  depends_on:\\n    - zookeeper\\n    - kafka\\n  ports:\\n    - \\"8081:8081\\"\\n  environment:\\n    SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181\\n    SCHEMA_REGISTRY_HOST_NAME: schema-registry\\n```\\n\\nTo sum up, we have a complete docker-compose.yml file like **[this](https://github.com/lam1051999/spark_kafka_docker/blob/main/spark_ex/docker-compose.yml)**. Then we start the containers with\\n\\n```bash\\ndocker-compose up -d\\n```\\n\\nNote, this starts all containers at once, some Kafka and Schema Registry instances will fail because it depends on Zookeeper. Wait for the Zookeeper container to finish up and then restart the Kafka container and the Schema Registry (you can also check the Zookeeper service by implementing some healthcheck techniques).\\n\\n### 3. Create a Kafka Producer that produce fake data using Java Faker\\n\\nNext, we create a Kafka Producer to fire dummy data in Java. First, we need to create a schema on the Schema Registry. Because the Schema Registry provides a restful interface, we can easily interact with it by calling GET, POST,... The schema we use in this article will have the following form.\\n\\n```json\\n{\\n  \\"namespace\\": \\"com.cloudurable.phonebook\\",\\n  \\"type\\": \\"record\\",\\n  \\"name\\": \\"Employee\\",\\n  \\"doc\\": \\"Represents an Employee at a company\\",\\n  \\"fields\\": [\\n    { \\"name\\": \\"id\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person id\\" },\\n    { \\"name\\": \\"firstName\\", \\"type\\": \\"string\\", \\"doc\\": \\"The persons given name\\" },\\n    { \\"name\\": \\"nickName\\", \\"type\\": [\\"null\\", \\"string\\"], \\"default\\": null },\\n    { \\"name\\": \\"lastName\\", \\"type\\": \\"string\\" },\\n    { \\"name\\": \\"age\\", \\"type\\": \\"int\\", \\"default\\": -1 },\\n    { \\"name\\": \\"emails\\", \\"type\\": \\"string\\", \\"doc\\": \\"The person email\\" },\\n    {\\n      \\"name\\": \\"phoneNumber\\",\\n      \\"type\\": {\\n        \\"type\\": \\"record\\",\\n        \\"name\\": \\"PhoneNumber\\",\\n        \\"fields\\": [\\n          { \\"name\\": \\"areaCode\\", \\"type\\": \\"string\\" },\\n          { \\"name\\": \\"countryCode\\", \\"type\\": \\"string\\", \\"default\\": \\"\\" },\\n          { \\"name\\": \\"prefix\\", \\"type\\": \\"string\\" },\\n          { \\"name\\": \\"number\\", \\"type\\": \\"string\\" }\\n        ]\\n      }\\n    },\\n    { \\"name\\": \\"status\\", \\"type\\": \\"string\\" }\\n  ]\\n}\\n```\\n\\nFirst, to POST this schema to the Schema Registry, we must convert this schema to escaped json, visit **[this website](https://www.freeformatter.com/json-escape.html)**. Then use the POST method to push the schema as follows\\n\\n```bash\\ncurl -X POST -H \\"Content-Type: application/vnd.schemaregistry.v1+json\\" \\\\\\n  --data \'{\\"schema\\": \\"{\\\\\\"namespace\\\\\\": \\\\\\"com.cloudurable.phonebook\\\\\\",\\\\\\"type\\\\\\": \\\\\\"record\\\\\\",\\\\\\"name\\\\\\": \\\\\\"Employee\\\\\\",\\\\\\"doc\\\\\\" : \\\\\\"Represents an Employee at a company\\\\\\",\\\\\\"fields\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"id\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The person id\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"firstName\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The persons given name\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"nickName\\\\\\", \\\\\\"type\\\\\\": [\\\\\\"null\\\\\\", \\\\\\"string\\\\\\"], \\\\\\"default\\\\\\" : null},{\\\\\\"name\\\\\\": \\\\\\"lastName\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"age\\\\\\",  \\\\\\"type\\\\\\": \\\\\\"int\\\\\\", \\\\\\"default\\\\\\": -1},{\\\\\\"name\\\\\\": \\\\\\"emails\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"doc\\\\\\": \\\\\\"The person email\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"phoneNumber\\\\\\",  \\\\\\"type\\\\\\":{ \\\\\\"type\\\\\\": \\\\\\"record\\\\\\",   \\\\\\"name\\\\\\": \\\\\\"PhoneNumber\\\\\\",\\\\\\"fields\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"areaCode\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"countryCode\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"default\\\\\\" : \\\\\\"\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"prefix\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"},{\\\\\\"name\\\\\\": \\\\\\"number\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"}]}},{\\\\\\"name\\\\\\": \\\\\\"status\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\"}]}\\"}\' \\\\\\n  http://localhost:8081/subjects/personinformation-value/versions\\n```\\n\\nAfter that, GET back to check if the schema is up or not\\n\\n```bash\\ncurl -X GET http://localhost:8081/subjects/personinformation-value/versions/ // check all versions\\ncurl -X GET http://localhost:8081/subjects/personinformation-value/versions/1 // check schema version 1\\n```\\n\\nNow that Kafka is up, the schema is on the Schema Registry, the rest is to push the message to that topic. Write a class as follows (see full code **[here](https://github.com/lam1051999/spark_kafka_docker/tree/main/KafkaClient)**), and run, then the data will be uploaded to Kafka with the above chema.\\n\\n```java\\npackage kafkaclient;\\n\\nimport com.github.javafaker.Faker;\\nimport io.confluent.kafka.serializers.KafkaAvroSerializerConfig;\\nimport org.apache.avro.Schema;\\nimport org.apache.avro.generic.GenericData;\\nimport org.apache.avro.generic.GenericRecord;\\nimport org.apache.kafka.clients.producer.KafkaProducer;\\nimport org.apache.kafka.clients.producer.Producer;\\nimport io.confluent.kafka.serializers.KafkaAvroSerializer;\\nimport org.apache.kafka.clients.producer.ProducerConfig;\\nimport org.apache.kafka.clients.producer.ProducerRecord;\\nimport org.apache.kafka.common.serialization.StringSerializer;\\n\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.Properties;\\n\\npublic class KafkaProducerExample {\\n    private final static String TOPIC = \\"personinformation\\";\\n    private final static String BOOTSTRAP_SERVERS = \\"localhost:29092\\";\\n    private final static String SCHEMA_REGISTRY_URL = \\"http://localhost:8081\\";\\n    private final static String LOCAL_SCHEMA_PATH = \\"src/main/resources/person.avsc\\";\\n    private final static Schema schema;\\n\\n    private final static int nPersons = 1000;\\n\\n    static {\\n        try {\\n            schema = new Schema.Parser().parse(new File(LOCAL_SCHEMA_PATH));\\n        } catch (IOException e) {\\n            throw new RuntimeException(e);\\n        }\\n    }\\n\\n    private static Producer<String, GenericRecord> createProducer(){\\n        Properties props = new Properties();\\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);\\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());\\n        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\\n\\n        return new KafkaProducer<>(props);\\n    }\\n\\n    static void runProducer() {\\n        final Producer<String, GenericRecord> producer = createProducer();\\n        Faker faker = new Faker();\\n\\n        for (int i = 0; i < nPersons; i ++){\\n            String id = faker.idNumber().valid();\\n            String firstName = faker.name().firstName();\\n            String nickName = faker.name().username();\\n            String lastName = faker.name().lastName();\\n            int age = faker.number().numberBetween(18, 90);\\n            String emails = faker.internet().safeEmailAddress();\\n            String areaCode = String.valueOf(faker.number().numberBetween(200, 500));\\n            String countryCode = String.valueOf(faker.number().numberBetween(80, 85));\\n            String prefix = String.valueOf(faker.number().numberBetween(400, 600));\\n            String number = String.valueOf(faker.number().numberBetween(1234, 6789));\\n\\n            GenericRecord phoneNumber = new GenericData.Record(schema.getField(\\"phoneNumber\\").schema());\\n            phoneNumber.put(\\"areaCode\\", areaCode);\\n            phoneNumber.put(\\"countryCode\\", countryCode);\\n            phoneNumber.put(\\"prefix\\", prefix);\\n            phoneNumber.put(\\"number\\", number);\\n\\n            StatusEnum status = StatusEnum.getRandomStatus();\\n\\n            GenericRecord personInfo = new GenericData.Record(schema);\\n            personInfo.put(\\"id\\", id);\\n            personInfo.put(\\"firstName\\", firstName);\\n            personInfo.put(\\"nickName\\", nickName);\\n            personInfo.put(\\"lastName\\", lastName);\\n            personInfo.put(\\"age\\", age);\\n            personInfo.put(\\"emails\\", emails);\\n            personInfo.put(\\"phoneNumber\\", phoneNumber);\\n            personInfo.put(\\"status\\", status.toString());\\n\\n            ProducerRecord<String, GenericRecord> data = new ProducerRecord<String, GenericRecord>(TOPIC, String.format(\\"%s %s %s\\", firstName, lastName, nickName), personInfo);\\n            producer.send(data);\\n            System.out.println(\\"Send successfully!!!\\");\\n            try {\\n                Thread.sleep(2000);\\n            }catch (Exception e){\\n                e.printStackTrace();\\n            }\\n        }\\n    }\\n\\n    public static void main(String[] args) {\\n        try {\\n            runProducer();\\n        }catch (Exception e){\\n            e.printStackTrace();\\n        }\\n    }\\n}\\n```\\n\\nAbove, every 2 seconds we will push 1 message to Kafka, pushing a total of 1000 messages.\\n\\n### 4. Submit Spark job\\n\\n#### 4.1. Configure the Postgres database\\n\\nBefore we can run the job, we need to configure Postgres with the following tables\\n\\n- Configuration for Spark applications\\n\\n```sql\\nCREATE TABLE spark_launcher_config (\\n    id serial primary  key,\\n    \\"desc\\" varchar(1000) NULL,\\n    app_name varchar(255) NULL,\\n    properties text,\\n    created timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\'),\\n    modified timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\')\\n)\\n\\nINSERT INTO public.spark_launcher_config\\n    (id, \\"desc\\", app_name, properties, created, modified)\\n    VALUES(2, \'kafka_ingest\', \'ingest_avro_from_kafka\', \'{\\n    \\"appname\\": \\"ingest_avro_from_kafka\\",\\n    \\"master\\": \\"spark://spark-master:7077\\",\\n    \\"duration\\": \\"10\\",\\n    \\"groupId\\": \\"ingest_avro_from_kafka\\",\\n    \\"zookeeper.hosts\\": \\"zookeeper:2181\\",\\n    \\"checkpoint\\": \\"./spark_checkpoint/ingest_avro_from_kafka\\",\\n    \\"zookeeper.timeout\\": \\"40000\\",\\n    \\"spark.sql.shuffle.partitions\\": \\"10\\",\\n    \\"spark.sql.sources.partitionOverwriteMode\\": \\"dynamic\\",\\n    \\"spark.sql.hive.verifyPartitionPath\\": \\"true\\",\\n    \\"spark.streaming.kafka.maxRatePerPartition\\": 10000,\\n    \\"_kafka_.bootstrap.servers\\": \\"kafka:9092\\",\\n    \\"_kafka_.group.id\\": \\"ingest_avro_from_kafka\\",\\n    \\"_kafka_.auto.offset.reset\\": \\"earliest\\",\\n    \\"_kafka_.max.poll.interval.ms\\": 5000000,\\n    \\"_kafka_.max.poll.records\\": 10000,\\n    \\"_kafka_.schema.registry.url\\": \\"http://schema-registry:8081\\",\\n    \\"_kafka_.auto.commit\\": \\"false\\",\\n    \\"_kafka_.session.timeout.ms\\": \\"50000\\",\\n    \\"_kafka_.heartbeat.interval.ms\\": \\"25000\\",\\n    \\"_kafka_.request.timeout.ms\\": \\"50000\\"\\n    }\', \'2022-04-12 09:35:27.511\', \'2022-04-12 09:35:27.511\');\\n```\\n\\n- Topic consumption configuration table\\n\\n```sql\\nCREATE TABLE spark_ingest_config (\\n    id serial primary key,\\n    app_name varchar(255) not null unique,\\n    type varchar(255)  NULL,\\n    \\"order\\" int NULL,\\n    topic varchar(255) not null unique,\\n    status int not null DEFAULT 0,\\n    fields text,\\n    temp_view_first varchar(255)  NULL,\\n    sql_parser text,\\n    prd_id varchar(255)  NULL,\\n    keys varchar(255)  NULL,\\n    path_hdfs varchar(255) NOT NULL,\\n    table_dest varchar(255) NOT NULL,\\n    impala_driver varchar(255) null DEFAULT \'\',\\n    impala_url varchar(255) null DEFAULT \'\',\\n    kafka_msg_type kkmt DEFAULT \'avro_flat\',\\n    json_schema text,\\n    repartition_des int not null DEFAULT 1,\\n    msg_type mst DEFAULT \'NOT_DEFINE\',\\n    created timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\'),\\n    modified timestamp without time zone DEFAULT (now() at time zone \'Asia/Ho_Chi_Minh\')\\n)\\n\\nINSERT INTO public.spark_ingest_config\\n(id, app_name, \\"type\\", \\"order\\", topic, status, fields, temp_view_first, sql_parser, prd_id, keys, path_hdfs, table_dest, impala_driver, impala_url, kafka_msg_type, json_schema, repartition_des, msg_type, created, modified)\\nVALUES(1, \'ingest_avro_from_kafka\', \'insert\', 0, \'personinformation\', 1, \'firstName,\\nnickName,\\nlastName,\\nage,\\nemails,\\nphoneNumber,\\nstatus\', \'ingest_avro_from_kafka_personinformation\', \'select\\n\\tcast(firstName as STRING) as firstName,\\n\\tcast(nickName as STRING) as nickName,\\n\\tcast(lastName as STRING) as lastName,\\n\\tcast(age as INT) as age,\\n\\tcast(emails as STRING) as emails,\\n\\tcast(concat(phoneNumber.countryCode, \\"-\\", phoneNumber.areaCode, \\"-\\", phoneNumber.prefix, \\"-\\", phoneNumber.number) as STRING) as phoneNumber,\\n\\tcast(status as STRING) as status\\nfrom ingest_avro_from_kafka_personinformation\', \'\', \'\', \'\', \'personinformation\', \'\', \'\', \'avro_flat\'::public.\\"kkmt\\", \'\', 1, \'NOT_DEFINE\'::public.\\"mst\\", \'2022-04-06 19:59:41.745\', \'2022-04-06 19:59:41.745\');\\n```\\n\\n- Streaming data table\\n\\n```sql\\ncreate table personinformation (\\n\\tfirstName varchar(250) not null,\\n\\tnickName varchar(250) not null,\\n\\tlastName varchar(250) not null,\\n\\tage integer not null,\\n\\temails varchar(250) not null,\\n\\tphoneNumber varchar(250) not null,\\n\\tstatus varchar(10) not null\\n);\\n```\\n\\n#### 4.2. Spark application configuration\\n\\nThe full Spark Streaming Code you can find **[here](https://github.com/lam1051999/spark_kafka_docker/tree/main/spark_ex)**. Compile the project by running\\n\\n```bash\\nsh run.sh\\n```\\n\\nWhen all containers are running stable, Kafka has the data, we access the shell of the Spark master container\\n\\n```bash\\ndocker exec -it spark-master bash\\n```\\n\\nAfter entering the shell, you continue to run the command below to submit the Spark job\\n\\n```bash\\n$SPARK_HOME/bin/spark-submit --jars $(echo /execution_files/dependency/*.jar | tr \' \' \',\') --class com.tranlam.App /execution_files/spark_ex-1.0-SNAPSHOT.jar --app-name ingest_avro_from_kafka --jdbc-url \\"jdbc:postgresql://db:5432/postgres?user=postgres&password=postgres\\"\\n```\\n\\nSo there is already a Spark job that consumes Kafka data. Visit **[http://localhost:4040/streaming](http://localhost:4040/streaming)** to see the batches running\\n\\n![Architecture](./images/spark-ui.PNG)\\n\\nIn Postgres, query the table `personinformation` we get the data as desired\\n\\n![Postgres](./images/postgres.PNG)\\n\\nAbove is the steps for building a basic Spark streaming pipeline to stream data from Kafka. Another thing to note is that instead of committing the offset of the consumptions to a Kafka topic like in above code, you can manually commit it to a path in Zookeeper for more proactive control.\\n\\nThe code of the whole article you read can be found at: **[https://github.com/lam1051999/spark_kafka_docker](https://github.com/lam1051999/spark_kafka_docker)**"},{"id":"spark-cluster-docker","metadata":{"permalink":"/blogs/blog/spark-cluster-docker","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2022-01-01-spark-cluster-docker/index.md","source":"@site/blog/2022-01-01-spark-cluster-docker/index.md","title":"Create A Standalone Spark Cluster With Docker","description":"Cluster Overview","date":"2022-01-01T00:00:00.000Z","formattedDate":"January 1, 2022","tags":[{"label":"Bigdata","permalink":"/blogs/blog/tags/bigdata"},{"label":"Spark","permalink":"/blogs/blog/tags/spark"},{"label":"Apache","permalink":"/blogs/blog/tags/apache"},{"label":"Docker","permalink":"/blogs/blog/tags/docker"}],"readingTime":6.455,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"spark-cluster-docker","title":"Create A Standalone Spark Cluster With Docker","authors":"tranlam","tags":["Bigdata","Spark","Apache","Docker"],"image":"./images/cluster-overview.PNG"},"prevItem":{"title":"Create A Data Streaming Pipeline With Spark Streaming, Kafka And Docker","permalink":"/blogs/blog/spark-kafka-docker"},"nextItem":{"title":"AVL Tree, AVL Sorting Algorithm","permalink":"/blogs/blog/avl-tree"}},"content":"![Cluster Overview](./images/cluster-overview.PNG)\\n\\nLately, I\'ve spent a lot of time teaching myself how to build Hadoop clusters, Spark, Hive integration, and more. This article will write about how you can build a Spark cluster for data processing using Docker, including 1 master node and 2 worker nodes, the cluster type is standalone cluster (maybe the upcoming articles I will do about Hadoop cluster and integrated resource manager is Yarn). Let\'s go to the article.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Overview of a Spark cluster\\n\\nApache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute tasks across multiple computers. It was design for fast computing and use RAM for caching and processing data.\\n\\nIt provides flexibility and scalability, created to improve the performance of MapReduce but at a much higher speed: 100 times faster than Hadoop when data is stored in memory and 10 times faster when accessed CD driver.\\n\\nSpark does not have a file system of its own, but it can interact with many types of storage systems and can be used to integrate with Hadoop. Below is an overview of the structure of a Spark application.\\n\\n![Cluster Overview](./images/cluster-overview.PNG)\\n\\nEach time we submit a Spark application, it will create a driver program at the master node, which then create a SparkContext object. To be able to run in a cluster, SparkContext need to connect to a cluster resource manager, that could be Spark\u2019s standalone cluster manager, Mesos or Yarn. Once the SparkContext get the connection, it will have specific RAM and CPU resources of the worker nodes in the cluster.\\n\\nEach worker node will receive the code and tasks from the driver, compute and process the data.\\n\\nThe master node will be responsible for scheduling all the tasks and send those to the worker nodes so it\u2019s ideal when we put it in the same network area with all the worker nodes to achieve low latency between requests\\n\\nThere are 2 Spark running modes\\n\\n- **Running locally:** running all the tasks in the same machine which is your local machine, utilize the number of cores in that machine to perform parallelism\\n- **Running in a cluster:** Spark distribute the tasks to all the machine in the cluster. There are 2 deploy modes which are client mode and cluster mode, with 4 options of cluster resource manager, which are Spark standalone cluster manager, Apache Mesos, Hadoop Yarn, or Kubernetes.\\n\\n### 2. Create a base image for the cluster\\n\\nBecause the images of the nodes in a cluster need to install the same software, we will build a base image for the whole cluster first, then the following images will import from this image and add the following images. other necessary dependencies.\\n\\n```bash\\nARG debian_buster_image_tag=8-jre-slim\\nFROM openjdk:${debian_buster_image_tag}\\n\\nARG shared_workspace=/opt/workspace\\n\\nRUN mkdir -p ${shared_workspace} && \\\\\\n    apt-get update -y && \\\\\\n    apt-get install -y python3 && \\\\\\n    ln -s /usr/bin/python3 /usr/bin/python && \\\\\\n    rm -rf /var/lib/apt/lists/*\\n\\nENV SHARED_WORKSPACE=${shared_workspace}\\n\\nVOLUME ${shared_workspace}\\n```\\n\\nHere, since Spark requires java version 8 or 11, we will create an image running jdk 8, we will take the variable `shared_workspace` as the Jupyterlab working environment path (later). In addition, we will install `python3` for running Jupyterlab.\\n\\n### 3. Create a spark base image\\n\\nWe come to create a spark base image with common packages for master node and worker node.\\n\\n```bash\\nFROM spark-cluster-base\\n\\nARG spark_version=3.2.0\\nARG hadoop_version=3.2\\n\\nRUN apt-get update -y && \\\\\\n    apt-get install -y curl && \\\\\\n    curl https://archive.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz -o spark.tgz && \\\\\\n    tar -xf spark.tgz && \\\\\\n    mv spark-${spark_version}-bin-hadoop${hadoop_version} /usr/bin/ && \\\\\\n    mkdir /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}/logs && \\\\\\n    rm spark.tgz\\n\\nENV SPARK_HOME /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}\\nENV SPARK_MASTER_HOST spark-master\\nENV SPARK_MASTER_PORT 7077\\nENV PYSPARK_PYTHON python3\\n\\nWORKDIR ${SPARK_HOME}\\n```\\n\\nFirst, we will import the image from the base image above (that is `spark-cluster-base`, this name will be assigned at build time), listing the compatible Spark and Hadoop versions. You can check version compatibility on Spark\'s homepage.\\n\\n![Spark Version](./images/spark-version.PNG)\\n\\nThen it will be to download and extract Spark, along with creating the necessary environment variables to support running the command line later. Here, `SPARK_MASTER_HOST` and `SPARK_MASTER_PORT` used by worker nodes to register with the corresponding master node address.\\n\\n### 4. Create a master node image\\n\\nHaving a spark base image, we start creating the master node by importing that base image and adding the appropriate variables to the master node as the port of the web ui interface so we can interact with spark on the interface later.\\n\\n```bash\\nFROM spark-base\\n\\nARG spark_master_web_ui=8080\\n\\nEXPOSE ${spark_master_web_ui} ${SPARK_MASTER_PORT}\\nCMD bin/spark-class org.apache.spark.deploy.master.Master >> logs/spark-master.out\\n```\\n\\nThe above command is to run master node.\\n\\n### 5. Create a worker node image\\n\\nNext is to create worker node\\n\\n```bash\\nFROM spark-base\\n\\nARG spark_worker_web_ui=8081\\n\\nEXPOSE ${spark_worker_web_ui}\\nCMD bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} >> logs/spark-worker.out\\n```\\n\\nThe above command is to run the worker node and point to the address of the master node to register.\\n\\n### 6. Create a Jupyterlab image for testing\\n\\nFinally, to test the spark cluster working, we will install Jupyterlab and use pyspark to run the code.\\n\\n```bash\\nFROM spark-cluster-base\\n\\nARG spark_version=3.2.0\\nARG jupyterlab_version=3.2.5\\n\\nRUN apt-get update -y && \\\\\\n    apt-get install -y python3-pip && \\\\\\n    pip3 install wget pyspark==${spark_version} jupyterlab==${jupyterlab_version}\\n\\nEXPOSE 8888\\nWORKDIR ${SHARED_WORKSPACE}\\nCMD jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=\\n```\\n\\nAlong with that is to list the command to run Jupyter on port 8888.\\n\\n### 7. Combine images and create containers\\n\\nAfter creating all the Dockerfiles, we proceed to build the appropriate images.\\n\\n![Folder Structure](./images/folder-structure.PNG)\\n\\n**Listing versions**\\n\\n```bash\\nSPARK_VERSION=\\"3.2.0\\"\\nHADOOP_VERSION=\\"3.2\\"\\nJUPYTERLAB_VERSION=\\"3.2.5\\"\\n```\\n\\n**Build base image**\\n\\n```bash\\ndocker build \\\\\\n  --platform=linux/arm64 \\\\\\n  -f cluster_base/Dockerfile \\\\\\n  -t spark-cluster-base .\\n```\\n\\n**Build spark base image**\\n\\n```bash\\ndocker build \\\\\\n  --build-arg spark_version=\\"${SPARK_VERSION}\\" \\\\\\n  --build-arg hadoop_version=\\"${HADOOP_VERSION}\\" \\\\\\n  -f spark_base/Dockerfile \\\\\\n  -t spark-base .\\n```\\n\\n**Build master node image**\\n\\n```bash\\ndocker build \\\\\\n  -f master_node/Dockerfile \\\\\\n  -t spark-master .\\n```\\n\\n**Build worker node image**\\n\\n```bash\\ndocker build \\\\\\n  -f worker_node/Dockerfile \\\\\\n  -t spark-worker .\\n```\\n\\n**Build Jupyterlab image**\\n\\n```bash\\ndocker build \\\\\\n  --build-arg spark_version=\\"${SPARK_VERSION}\\" \\\\\\n  --build-arg jupyterlab_version=\\"${JUPYTERLAB_VERSION}\\" \\\\\\n  -f jupyter_lab/Dockerfile \\\\\\n  -t spark-jupyterlab .\\n```\\n\\nFinally, to create the necessary containers, we create a file `docker-compose.yml` with the following content\\n\\n```bash\\nversion: \\"3.6\\"\\nvolumes:\\n  shared-workspace:\\n    name: \\"hadoop-distributed-file-system\\"\\n    driver: local\\nservices:\\n  jupyterlab:\\n    image: spark-jupyterlab\\n    container_name: jupyterlab\\n    ports:\\n      - 8888:8888\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n  spark-master:\\n    image: spark-master\\n    container_name: spark-master\\n    ports:\\n      - 8080:8080\\n      - 7077:7077\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n  spark-worker-1:\\n    image: spark-worker\\n    container_name: spark-worker-1\\n    environment:\\n      - SPARK_WORKER_CORES=1\\n      - SPARK_WORKER_MEMORY=512m\\n    ports:\\n      - 8081:8081\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n    depends_on:\\n      - spark-master\\n  spark-worker-2:\\n    image: spark-worker\\n    container_name: spark-worker-2\\n    environment:\\n      - SPARK_WORKER_CORES=1\\n      - SPARK_WORKER_MEMORY=512m\\n    ports:\\n      - 8082:8081\\n    volumes:\\n      - shared-workspace:/opt/workspace\\n    depends_on:\\n      - spark-master\\n```\\n\\nInclude the volume in which we will save data so that when deleting containers, data will not be lost, along with the necessary containers (services). To each container the appropriate environment variables are added, the ports to map to the host machine, and the order in which the containers are run. Here, the master node has to run first to get the hostname, so the worker node will depend on the master node container. After that, we run `docker-compose up`, so we have launched all the necessary containers.\\n\\n### 8. Running Jupyterlab to check the cluster\\n\\nAfter running `docker-compose up` and seeing in the terminal the logs showing that the master node and worker node have been successfully started, along with the successful register status of the nodes, we go to `localhost:8080` to access spark ui.\\n\\n![Spark UI](./images/spark-ui.PNG)\\n\\nIn the interface, we can see that there are 2 workers working as red circled areas.\\n\\nEnter `localhost:8888` to access the Jupyterlab interface, execute the following code\\n\\n![Jupyter Lab](./images/jupyterlab.PNG)\\n\\nRun the code, then go back to spark ui, we can see our application is running\\n\\n![Application](./images/application.PNG)\\n\\nClick on the application, we see our workers processing the job\\n\\n![Application Workers](./images/application-workers.PNG)"},{"id":"avl-tree","metadata":{"permalink":"/blogs/blog/avl-tree","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2021-02-24-avl-tree/index.md","source":"@site/blog/2021-02-24-avl-tree/index.md","title":"AVL Tree, AVL Sorting Algorithm","description":"Intuition","date":"2021-02-24T00:00:00.000Z","formattedDate":"February 24, 2021","tags":[{"label":"tree","permalink":"/blogs/blog/tags/tree"},{"label":"avl","permalink":"/blogs/blog/tags/avl"},{"label":"binary tree","permalink":"/blogs/blog/tags/binary-tree"},{"label":"algorithms","permalink":"/blogs/blog/tags/algorithms"}],"readingTime":7.325,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"avl-tree","title":"AVL Tree, AVL Sorting Algorithm","authors":"tranlam","tags":["tree","avl","binary tree","algorithms"],"image":"./images/intuition.PNG"},"prevItem":{"title":"Create A Standalone Spark Cluster With Docker","permalink":"/blogs/blog/spark-cluster-docker"},"nextItem":{"title":"Binary Search Tree","permalink":"/blogs/blog/binarysearch-tree"}},"content":"![Intuition](./images/intuition.PNG)\\n\\nIn the previous post, I talked about the binary search tree, if you haven\'t read it yet, you can find it **[here](/blog/2021-02-22-binarysearch-tree/index.md)**. With efficiency in search, insert, delete,... binary search tree can be done in logrithmic time (${\\\\Theta(logn)}$) in the average case. In this article, I will talk about AVL tree, which is a type of binary search tree, ensuring that in all cases, the time complexity of the above operations is the same ${\\\\Theta(logn)}$.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. AVL tree\\n\\nAn AVL tree is a balanced binary search tree in which the heights of the left and right subtrees differ by at most 1 ${(1)}$. In the process of performing operations on the tree that make the tree unbalanced, we need to rebalance the tree to ensure the nature of the tree ${(1)}$.\\n\\n### 2. Tree height assessment\\n\\nThe height of the tree\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${h}$ = max{ left subtree height, right subtree height } + 1\\n\\n</p>\\n\\nThe heights of the nodes are numbered as shown above.\\n\\nWith the property ${(1)}$, the worst case of an AVL tree occurs when the right subtree is 1 unit taller than the left subtree for all nodes (or vice versa).\\n\\nFor ${N{_h}}$ is the smallest number of nodes in a tree of height ${h}$.\\n\\n![Height](./images/height.PNG)\\n\\nWith the diagram above, we have\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${N{_ {O(1)}} = O(1)}$ v\xe0 ${N{_ h} = 1 + N{_ {h-1}} + N{_{h-2}}}$\\n\\n</p>\\n\\n#### 2.1. First approach\\n\\nThe above expression reminds us of the fibonacci sequence, we have ${N{_h} > F{_h}}$ v\u1edbi ${F{_h}}$ is the ${h^{th}}$ fibonacci. We have ${F{_h} = \\\\frac{\\\\gamma^h}{\\\\sqrt{5}}}$, with ${\\\\gamma = 1.61803398875...}$, (golden ratio).\\n\\nWith ${N{_h} = n}$ (the number of nodes in the tree).\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${ n > \\\\frac{\\\\gamma^h}{\\\\sqrt{5}} => h < log{_\\\\gamma}n => h < 1.440log{_2}n }$.\\n\\n</p>\\n\\n#### 2.2. Second approach\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${=> N{_ h} = 1 + N{_ {h-1}} + N{_{h-2}}}$\\n\\n${=> N{_ h} > 1 + 2N{_{h-2}}}$\\n\\n${=> N{_ h} > 2N{_{h-2}}}$\\n\\n${=> h < 2log{_2}n}$\\n\\n</p>\\n\\nTherefore, ${h = O(logn)}$.\\n\\n### 3. AVL tree operations\\n\\n#### 3.1. Node insertion, node deletion, search node\\n\\nThe above operations in an AVL tree are the same as in a binary search tree, with ${h = O(logn)}$. The difference in node insertion and deletion is that after performing those operations, we need to do an extra step of tree balancing to ensure the integrity of the tree ${(1)}$.\\n\\n#### 3.2. Tree balancing activities\\n\\nThe following example demonstrates the need for tree rebalancing\\n\\nSuppose we have the following tree\\n\\n![Insert](./images/insert.JPG)\\n\\nWe want to insert ${23}$ into the tree, perform the insertion as in a binary search tree. After inserting, we see the property ${(1)}$ violated. We need to rebalance the tree to get\\n\\n![Insert2](./images/insert_2.JPG)\\n\\n##### 3.2.1. Balance factor\\n\\nIn a binary tree, the balance factor is defined as follows:\\n\\n${BF(x)}$ = height of left subtree if ${x}$ ${-}$ height of right subtree of ${x}$.\\n\\nThus, in the AVL tree, we have ${BF(x) \\\\in \\\\\\\\{-1, 0, 1\\\\\\\\} }$.\\n\\n##### 3.2.2. Tree balancing operations\\n\\nWhen the BF of a certain node has a value that is not in the above set of values, then we need to re-balancing the tree. We have two basic types of balancing operations of AVL trees: **right rotation** and **left rotation**\\n\\n![Rotation](./images/rotation.PNG)\\n\\n#### 3.3. Tree balancing in specific cases\\n\\n##### 3.3.1. Left left case\\n\\nThis case occurs when a node has BF = 2 and its left subtree has BF = 1. Then, we only need 1 **right rotation** at the node to be considered, then the tree is balanced.\\n\\n![Left Left](./images/left_left.PNG)\\n\\n##### 3.3.2. Left right case\\n\\nThis case occurs when a node has BF = 2 and its left subtree has BF = -1. Then, we need to do the following 2 steps in turn\\n\\n- **Left rotation** left subtree.\\n- **Right rotation** the node to be considered.\\n\\n![Left Right](./images/left_right.PNG)\\n\\n##### 3.3.3. Right right case\\n\\nThis case occurs when a node has BF = -2 and its right subtree has BF = -1. Then, we only need 1 **left rotation** at the node to be considered, then the tree is balanced.\\n\\n![Right Right](./images/right_right.PNG)\\n\\n##### 3.3.4. Right left case\\n\\nThis case occurs when a node has BF = -2 and its left subtree has BF = 1. Then, we need to do the following 2 steps in turn\\n\\n- **Right rotation** right subtree.\\n- **Left rotation** the node to be considered.\\n\\n![Right Left](./images/right_left.PNG)\\n\\n### 4. Python code for tree balancing activities\\n\\n```python\\n# ---------------------------METHOD TO HELP BALANCE THE TREE---------------------------\\n#      y                               x\\n#     / \\\\     Right Rotation          /  \\\\\\n#    x   T3   - - - - - - - >        T1   y\\n#   / \\\\       < - - - - - - -            / \\\\\\n#  T1  T2     Left Rotation            T2  T3\\n\\n# ---------------------------BALANCE THE TREE IN PARTICULAR CASES---------------------------\\n# -----Left Left Case\\n#          z                                      y\\n#         / \\\\                                   /   \\\\\\n#        y   T4      Right Rotate (z)          x      z\\n#       / \\\\          - - - - - - - - ->      /  \\\\    /  \\\\\\n#      x   T3                               T1  T2  T3  T4\\n#     / \\\\\\n#   T1   T2\\n# -----Left Right Case\\n#      z                               z                           x\\n#     / \\\\                            /   \\\\                        /  \\\\\\n#    y   T4  Left Rotate (y)        x    T4  Right Rotate(z)    y      z\\n#   / \\\\      - - - - - - - - ->    /  \\\\      - - - - - - - ->  / \\\\    / \\\\\\n# T1   x                          y    T3                    T1  T2 T3  T4\\n#     / \\\\                        / \\\\\\n#   T2   T3                    T1   T2\\n# -----Right Right Case\\n#   z                                y\\n#  /  \\\\                            /   \\\\\\n# T1   y     Left Rotate(z)       z      x\\n#     /  \\\\   - - - - - - - ->    / \\\\    / \\\\\\n#    T2   x                     T1  T2 T3  T4\\n#        / \\\\\\n#      T3  T4\\n# -----Right Left Case\\n#    z                            z                            x\\n#   / \\\\                          / \\\\                          /  \\\\\\n# T1   y   Right Rotate (y)    T1   x      Left Rotate(z)   z      y\\n#     / \\\\  - - - - - - - - ->     /  \\\\   - - - - - - - ->  / \\\\    / \\\\\\n#    x   T4                      T2   y                  T1  T2  T3  T4\\n#   / \\\\                              /  \\\\\\n# T2   T3                           T3   T4\\n\\nclass AVLNode:\\n    def __init__(self, val):\\n        self.left = None\\n        self.right = None\\n        self.val = val\\n        self.height = 1\\n\\n\\nclass AVLTree:\\n    def insert(self, root, key):\\n        # perform bst\\n        if not root:\\n            return AVLNode(key)\\n        if root.val < key:\\n            root.right = self.insert(root.right, key)\\n        if root.val > key:\\n            root.left = self.insert(root.left, key)\\n        # update the height of the ancestor node\\n        root.height = 1 + max(self.get_height(root.left),\\n                              self.get_height(root.right))\\n\\n        # get balance factor\\n        balance = self.get_balance(root)\\n\\n        # perform balance\\n        # left left\\n        if balance > 1 and key < root.left.val:\\n            return self.right_rotate(root)\\n        # right right\\n        if balance < -1 and key > root.right.val:\\n            return self.left_rotate(root)\\n        # left right\\n        if balance > 1 and key > root.left.val:\\n            root.left = self.left_rotate(root.left)\\n            return self.right_rotate(root)\\n        # right left\\n        if balance < -1 and key < root.left.val:\\n            root.right = self.right_rotate(root.right)\\n            return self.left_rotate(root)\\n\\n        return root\\n\\n    def left_rotate(self, x):\\n        y = x.right\\n        T2 = y.left\\n\\n        y.left = x\\n        x.right = T2\\n\\n        x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))\\n        y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))\\n\\n        return y\\n\\n    def right_rotate(self, y):\\n        x = y.left\\n        T2 = x.right\\n\\n        x.right = y\\n        y.left = T2\\n\\n        x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))\\n        y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))\\n\\n        return x\\n\\n    def get_height(self, root):\\n        if not root:\\n            return 0\\n\\n        return root.height\\n\\n    def get_balance(self, root):\\n        if not root:\\n            return 0\\n\\n        return self.get_height(root.left) - self.get_height(root.right)\\n\\n    def inorder(self, root):\\n        if root is not None:\\n            self.inorder(root.left)\\n            print(root.val)\\n            self.inorder(root.right)\\n\\n\\navl_tree = AVLTree()\\nroot = None\\n\\nroot = avl_tree.insert(root, 20)\\nroot = avl_tree.insert(root, 10)\\nroot = avl_tree.insert(root, 30)\\nroot = avl_tree.insert(root, 40)\\nroot = avl_tree.insert(root, 50)\\nroot = avl_tree.insert(root, 5)\\nroot = avl_tree.insert(root, 15)\\nroot = avl_tree.insert(root, 25)\\nroot = avl_tree.insert(root, 55)\\n\\navl_tree.inorder(root)\\n```\\n\\n### 5. AVL sorting algorithm\\n\\nGiven an array of ${n}$ elements, the AVL sorting algorithm is done through the following steps\\n\\n- Perform inserts ${n}$ elements into AVL tree. Each inserted operation costs ${O(logn)}$ time (as in a binary search tree). We need to insert ${n}$ elements, so the time complexity of the process will be ${O(nlogn)}$.\\n- We perform inorder traversal (as in a binary search tree). This causes us to go through all the elements, so the time complexity of the process is ${O(n)}$.\\n\\nTherefore, the total time complexity is ${O(n + nlogn) = O(nlogn)}$. However, because of the extra ${O(n)}$ makes this AVL sorting algorithm inefficient and less practical than the other sorting algorithms that I have listed, which readers can access **[here](/blog/2021-02-20-sorting-algorithms/index.md)**.\\n\\n### 6. Additional notes\\n\\nReaders can find visualizations for AVL tree operations **[here](https://www.cs.usfca.edu/~galles/visualization/AVLtree.html)**.\\n\\n### 7. References\\n\\n[AVL tree](https://en.wikipedia.org/wiki/AVL_tree)\\n\\n[AVL Trees: Rotations, Insertion, Deletion with C++ Example](https://www.guru99.com/avl-tree.html)"},{"id":"binarysearch-tree","metadata":{"permalink":"/blogs/blog/binarysearch-tree","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2021-02-22-binarysearch-tree/index.md","source":"@site/blog/2021-02-22-binarysearch-tree/index.md","title":"Binary Search Tree","description":"Intro","date":"2021-02-22T00:00:00.000Z","formattedDate":"February 22, 2021","tags":[{"label":"tree","permalink":"/blogs/blog/tags/tree"},{"label":"binary tree","permalink":"/blogs/blog/tags/binary-tree"},{"label":"algorithms","permalink":"/blogs/blog/tags/algorithms"},{"label":"search","permalink":"/blogs/blog/tags/search"}],"readingTime":10.135,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"binarysearch-tree","title":"Binary Search Tree","authors":"tranlam","tags":["tree","binary tree","algorithms","search"],"image":"./images/intro.PNG"},"prevItem":{"title":"AVL Tree, AVL Sorting Algorithm","permalink":"/blogs/blog/avl-tree"},"nextItem":{"title":"Fundamental Sorting Algorithms","permalink":"/blogs/blog/sorting-algorithms"}},"content":"![Intro](./images/intro.PNG)\\n\\nIn the process of learning programming, you will encounter many types of data structures such as arrays, linked lists, maps, etc. Each type of data structure has its own advantages and disadvantages. Today, I will talk about an interesting type of data structure called a binary search tree, a very convenient data structure for the search problem.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Highlight the main problem\\n\\nThe real problems that we or businesses solve are often divided into small problems and then applying algorithms, as well as appropriate data structures to come up with a way to do it, effectively, efficiently and least costly. For the following problem, I would like to take an example from MIT\'s course 6.006, which you can access **[here](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/lecture-5-binary-search-trees-bst-sort/)**.\\n\\nSuppose an airline has a route management program. Each flight when it arrives at the airport must request a schedule to land at a certain time. In order not to have any conflicts, the landing times must be at least separated ${k}$ minutes ${(1)}$. The list of landing times is ${R}$ including ${n}$ elements. How to add a landing time ${t}$ to satisfy the constraint ${(1)}$ above.\\n\\nI have an image to make the problem more intuitive\\n\\n![Example](./images/example.PNG)\\n\\nWith the number of elements ${n}$, we want to perform a suitable position finding and insert new landing times in effective time complexity, like ${O(logn)}$. And the following will be the evaluations for the problem with some specific data structures.\\n\\n#### 1.1. Unsorted array\\n\\nProcedure for inserting elements into an unsorted array, regardless of the constraint condition ${(1)}$, will cost ${O(1)}$.\\nProcedure for inserting elements into an unsorted array, taking care of the constraint condition ${(1)}$, will cost ${O(n)}$.\\n\\nTime complexity: ${O(n)}$.\\n\\n#### 1.2. Sorted array\\n\\n- Find the right index will costs ${O(logn)}$ (using binary search).\\n- Compare with the element on either side of the element cost ${O(1)}$.\\n- Inserting the element in the appropriate position takes ${O(n)}$ (when you will probably have to shift most of the elements up 1 position in the case of inserting the element at the beginning of the array).\\n\\nTime complexity: ${O(n)}$.\\n\\n#### 1.3. Sorted linked list\\n\\nInserting an element into a linked list will take ${O(1)}$. But finding the inserted position will takes ${O(n)}$ when we have to traverse from head to that position.\\n\\n![Linked List](./images/linked_list.PNG)\\n\\n#### 1.4. Heap tree\\n\\nInsertion into min-heap or max-heap\\n\\n- Finding the insertion location will takes ${O(n)}$ when we may have to traverse all elements.\\n- Insertion into the min/max heap tree is unstable, because maybe after inserting in a certain position, we break the properties of the min/max heap tree and have to rerun **min/max-heapify** (refer to this article [Fundamental Sorting Algorithms](/blog/2021-02-20-sorting-algorithms/index.md) for **max-heapify** implementation) to get the right tree. Rerunning **min/max-heapify** will not able to guarantee the binding condition ${(1)}$.\\n\\n_We need a better data structure to be able to do locating and inserting element in ${O(logn)}$._\\n\\n### 2. Binary Search Tree\\n\\nBinary Search Tree is a data structure that satisfy below conditions ${(2)}$\\n\\n- Each node has a maximum of 2 child nodes.\\n- The value of the left child node is less than the parent node.\\n- The value of the right child node must be greater than the parent node.\\n- The left and right subtree is also a binary search tree.\\n\\nEach node of the tree consists of\\n\\n- The value of node.\\n- The pointer points to the left child node.\\n- The pointer points to the right child node.\\n\\n```python\\nclass Node:\\n    def __init__(self, val, left=None, right=None):\\n        self.val = val\\n        self.left = left\\n        self.right = right\\n```\\n\\nTypes of binary tree:\\n\\n- **Full binary tree:** each node of the tree has 0 or 2 child nodes.\\n- **Complete binary tree:** all the tree layers are filled with nodes except for the last layer, and the bottom layer nodes must be filled from left to right\\n- **Degenerate binary tree:** a tree where all parent nodes have only one child node.\\n- **Perfect binary tree:** Every internal node has 2 children and the leaves are at the same level.\\n- **Balanced binary tree:** the height of the left and right subtrees differ by at most 1.\\n\\nHere is an illustration\\n\\n![Types](./images/types.PNG)\\n\\n### 3. Operations on binary search treee\\n\\nConsider a tree with ${n}$ nodes, height is ${h}$.\\n\\n#### 3.1. Search - Finding a value in the tree\\n\\nFor searching for a key on the tree, we do it by recursive method. Starting at the root, we compare the value of the root node with the key. If the root node value is less than the key, we have to find it on the left subtree, if the root node value is greater than the key, we find that key on the right subtree. We do this with every node we go to. If the node value is equal to the key, we return that node. If the node value is null, we conclude that the key was not found in the tree.\\n\\n![Search](./images/search.JPG)\\n\\nExample of finding the key ${40}$. We first compare ${40 < 50}$, the go down the left subtree to continue to search. We compare ${40 > 30}$, then go down the right subtree to search. Finally, we find a node whose value is ${40}$.\\n\\n**Python Code**\\n\\n```python\\ndef search(root, key):\\n    if root is None:\\n        print(\\"Cannot find the key \\" + key +  \\" in bst\\")\\n        return None\\n    # continue to traverse\\n    if root.val < key:\\n        return search(root.right, key)\\n    elif root.val > key:\\n        return search(root.left, key)\\n    else:\\n        return root\\n```\\n\\n**Algorithm analysis**: finding a key in a tree will cost ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n#### 3.2. Insert - Add a node to the tree\\n\\nThe process of inserting a node with a certain value in the tree is quite similar to search. We search until we encounter an empty node, then we insert the node we want to insert at that position. During searching, we find a node with a value equal to the key, and we return that node.\\n\\n![Insert](./images/insert.PNG)\\n\\nExample of inserting the key with value ${4}$. Compare ${4 < 6}$, go down to the left subtree, compare ${4 > 3}$, go down to the right subtree. We encounter an empty position then we insert the key ${4}$ at that location.\\n\\n**Python Code**\\n\\n```python\\ndef insert(root, key):\\n    # reached leaves\\n    if root is None:\\n        return Node(key)\\n    # continue to search, if encounter an equal value, return that node\\n    else:\\n        if root.val == key:\\n            return root\\n        elif root.val < key:\\n            root.right = insert(root.right, key)\\n        else:\\n            root.left = insert(root.left, key)\\n\\n    return root\\n```\\n\\n**Algorithm analysis**: finding the position to insert in the tree costs ${O(h)}$, insertion costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n**Solving the problem in the first section:** because at the step of inserting the node into the tree, we can see that we can add conditional statements to approve the insertion or not without affecting the properties in the tree at ${(2)}$ of a binary search tree.\\n\\n![Insert2](./images/insert_2.PNG)\\n\\nAs with the tree above, with constraint value ${k = 3}$. We want to insert ${45}$. At the node ${40}$, we have ${45 - 40 > 3}$, so the next step is to insert ${45}$ into the tree, without losing the properties of the tree. If we want to insert ${42}$, we check ${42 - 40 < 3}$, so we do not perform node insertion.\\n\\n#### 3.3. Delete - Remove a node from the tree\\n\\nThe process of deleting a node in a binary search tree occurs in 3 cases\\n\\n- The node to be deleted has no child nodes.\\n- The node to be deleted has 1 child node.\\n- The node to be deleted has both child nodes.\\n\\n**Case 1:** The node to be deleted has no child nodes\\n\\n![Delete](./images/delete.JPG)\\n\\nExample of deleting a node with a key ${40}$ in the tree above, we just need to release it from the tree.\\n\\n**Case 2:** The node to be deleted has 1 child node\\n\\n![Delete2](./images/delete_2.JPG)\\n\\nExample of deleting a node with a key ${90}$ in the tree above, we just need to replace that node with its only child node.\\n\\n**Case 3:** The node to be deleted has 2 children, we replace it with the node with the largest key in its left subtree (the rightmost node of the left subtree), or the node with the smallest key in its right subtree (the leftmost node of the right subtree).\\n\\n![Delete3](./images/delete_3.JPG)\\n\\nExample of deleting a node with a key ${30}$ in the tree above, we find the node with the smallest key in its right subtree, which is ${35}$, and replace the node with the key ${35}$ into the node with the key ${30}$. The we realize that the node has a key ${35}$ in the old position is a node with 1 child node. So we also apply the node deletion procedure like **Case 2** for that location.\\n\\n**Python Code**\\n\\n```python\\n# find leftmost node in the right subtree\\ndef find_min(root):\\n    current = root\\n    while current.left is not None:\\n        current = current.left\\n    return current\\n\\ndef delete(root, key):\\n    if root is None:\\n        return root\\n    # continue to search until we find the node with the right key\\n    if root.val < key:\\n        root.right = delete(root.right, key)\\n    elif root.val > key:\\n        root.left = delete(root.left, key)\\n    else:\\n        # case 1\\n        if root.left is None and root.right is None:\\n            root = None\\n            return root\\n        # case 2\\n        elif root.left is None:\\n            root.val = root.right.val\\n            root.right = None\\n            return root\\n        elif root.right is None:\\n            root.val = root.left.val\\n            root.left = None\\n            return root\\n        # case 3\\n        else:\\n            temp = find_min(root.right)\\n            root.val = temp.val\\n            root.right = delete(root.right, temp.val)\\n            return root\\n\\n    return root\\n```\\n\\n**Algorithmic analysis:**\\n\\n- **Case 1:** Finding the node costs ${O(h)}$, delete the node costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n- **Case 2:** Finding the node costs ${O(h)}$, delete the node costs ${O(1)}$, node transfer costs ${O(1)}$. Therefore, time complexity will be ${O(h)}$.\\n- **Case 3:** Finding the node costs ${O(h)}$, finding the leftmost node in the right subtree takes ${O(h)}$, deleting the leftmost node in the right subtree costs ${O(h)}$. Therefore, time complexity will be ${O(h)}$.\\n\\n- **Average case:** tree height ${h = \\\\Theta(logn)}$, so the time complexity will be ${O(logn)}$.\\n- **Worst case:** when the tree is a degenerate binary tree, tree height ${h = n}$ so time complexity is ${O(n)}$.\\n\\n#### 3.4. Traversal\\n\\nThere are three ways to traverse and print the values \u200b\u200bof the nodes in the tree: pre-order, in-order, and post-order.\\n\\nConsider the following tree ${(3)}$\\n\\n![Order](./images/order.PNG)\\n\\n##### 3.4.1. Pre-order traversal\\n\\nWe traverse the parent node first, to the left child node, and then to the right child node.\\n\\nExample with ${(3)}$, in the pre-order traversal, the result will be ${6, 3, 1, 10, 9, 12}$.\\n\\n**Python Code**\\n\\n```python\\ndef preorder(root):\\n    if root:\\n        print(root.val)\\n        preorder(root.left)\\n        preorder(root.right)\\n```\\n\\n##### 3.4.2. In-order traversal\\n\\nWe traverse the left child node first, to the parent node, and then to the right child node.\\n\\nExample with ${(3)}$, in the pre-order traversal, the result will be ${1, 3, 6, 9, 10, 12}$.\\n\\n**Python Code**\\n\\n```python\\ndef inorder(root):\\n    if root:\\n        inorder(root.left)\\n        print(root.val)\\n        inorder(root.right)\\n```\\n\\n##### 3.4.3. Post-order traversal\\n\\nWe traverse the left child node first, the right child node, and then the parent node.\\n\\nExample with ${(3)}$, in the post-order traversal, the result will be ${1, 3, 9, 12, 10, 6}$.\\n\\n**Python Code**\\n\\n```python\\ndef postorder(root):\\n    if root:\\n        postorder(root.left)\\n        postorder(root.right)\\n        print(root.val)\\n```\\n\\n**Algorithm analysis:**\\nWe traverse all the nodes in the tree, so the time complexity is ${O(n)}$.\\n\\n### 4. Additional notes\\n\\nBinary search trees are interesting and efficient data structures. Readers can find visualizations of binary search tree operations **[here](https://www.cs.usfca.edu/~galles/visualization/BST.html)**.\\n\\n### 5. References\\n\\n[Binary Search Tree | Set 1 (Search and Insertion)](https://www.geeksforgeeks.org/binary-search-tree-set-1-search-and-insertion/)\\n\\n[Binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree)"},{"id":"sorting-algorithms","metadata":{"permalink":"/blogs/blog/sorting-algorithms","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2021-02-20-sorting-algorithms/index.md","source":"@site/blog/2021-02-20-sorting-algorithms/index.md","title":"Fundamental Sorting Algorithms","description":"Intro","date":"2021-02-20T00:00:00.000Z","formattedDate":"February 20, 2021","tags":[{"label":"algorithms","permalink":"/blogs/blog/tags/algorithms"},{"label":"sorting","permalink":"/blogs/blog/tags/sorting"}],"readingTime":18.365,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"sorting-algorithms","title":"Fundamental Sorting Algorithms","authors":"tranlam","tags":["algorithms","sorting"],"image":"./images/intro.JPEG"},"prevItem":{"title":"Binary Search Tree","permalink":"/blogs/blog/binarysearch-tree"},"nextItem":{"title":"Peak Finding Algorithm","permalink":"/blogs/blog/peak-finding"}},"content":"![Intro](./images/intro.JPEG)\\n\\nIn these early blogs, I will only write about the most basic algorithms when I\'m just starting to learn programming. First, let me relearn the basics (because I\'m extremely forgetful). Second, for those who are new to programming, you can refer to it. This article will talk about the basic sorting algorithms I learned in school, and also taught myself.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Why do we need sorting algorithms?\\n\\n**Firstly**, simply to pass exams at university, learn some Programming Languages, Data Structures and Algorithms, etc., it\'s easy to be asked these sorts of questions when taking the exam.\\n\\n**Second**, element arrangement is usually an intermediate stage, pre-processing data in many problems, processing systems,... to perform larger jobs after it. Since the amount of data in real systems is always very large, we need efficient sorting algorithms to save the cost (time and memory).\\n\\n**Basic examples of applying sorting algorithms**\\n\\n- Sort the list of customers by name in the customer management system.\\n- Find the median element in ${\\\\Theta(1)}$, or search for an element with ${\\\\Theta(logn)}$ if there is a sorted array.\\n- The database uses merge sort algorithms to sort data sets when they are too large to load locally into memory.\\n- Files search, data compression, routes finding.\\n- Graphic application also use sorting algorithms to arrange layers to render efficiently.\\n- After finishing the meal, your mother made you wash the dishes. You struggled with dozens of bowls for an hour and now you don\'t want to waste any more time on those bowls. As it turns out, the remaining job is to arrange the dishes so that they are neat, beautiful, and most of all, quickly so that you can play with your phone. Instinctively for all Asians of average intelligence, you\'ve sorted them out very quickly and stacked them up from small to big chunks, and then you realize you\'ve accidentally applied Counting Sort algorithm.\\n\\n**Basic operations using in the intermediate stages**\\n\\n- Compare 2 elements ${(a, b)}$, return ${True}$ if ${(a > b)}$, otherwise return ${False}$.\\n- Swap 2 elements ${(a, b)}$, in Python, it can be performed easily like\\n\\n```python\\na, b = b, a\\n```\\n\\nDuring the analysis of algorithms, we assume that the above operations take only constant time ${\\\\Theta(1)}$.\\n\\n### 2. Bubble sort\\n\\nBubble sort is a simple and inefficient sort that is taught in almost all algorithm courses because it is quite intuitive. Bubble sort compares each pair of numbers in an array and swaps places if they\'re out of order. The largest elements will be pushed to the bottom of the array, while the smaller elements will gradually \\"float\\" to the top of the array.\\n\\n**Algorithm:**\\n\\n- Compare ${arr[0]}$ to ${arr[1]}$, if ${arr[0] > arr[1]}$, swap their positions. Continue doing this with (${arr[1], arr[2]}$), (${arr[2], arr[3]}$),...\\n- Repeat this step ${n}$ times.\\n\\nTo make it more intuitive, I give the following descriptive image\\n\\n![Bubble Sort](./images/bubble.GIF)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when we apply the algorithm on the sorted array. Then, there will be no swap steps in the first pass, only comparison steps, from which the algorithm will end after this pass. So the time complexity will be ${\\\\Theta(n)}$. For this reason, bubble sort is also used to check if an array is sorted.\\n- **Worst case:** occurs when the array is sorted in reverse, therefore, ${n-1}$ comparisons and swaps will be performed on the first pass, ${n-2}$ comparisons and swaps will be performed on the second pass,... Therefore, the total number of comparisons and swaps will be ${Sum = (n-1) + (n-2) +...+ 2 + 1 = \\\\frac{n \\\\times (n-1)}{2}}$. Time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef bubble_sort(arr):\\n    for i in range(len(arr)):\\n        swapped = False\\n        for j in range(len(arr) - i - 1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n                swapped = True\\n        if not swapped:\\n            return\\n\\nbubble_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 3. Insertion sort\\n\\nImagine you play card game, when you have the deck in your hand, you have many ways to arrange it depending on your personality. For me, I usually arrange the cards in order from smallest to largest. When I want to arrange a new card into the deck of cards in my hand that is in order, I just insert the card into the appropriate position, and that is also the idea of \u200b\u200binsertion sort.\\n\\n**Algorithm:**\\nWith ${i = 1, 2,..., n - 1}$, we will insert ${arr[i]}$ into the sorted array ${arr[0:i-1]}$ by moving the elements greater than ${arr[i]}$ of the array ${arr[0:i-1]}$ to the top and put ${arr[i]}$ in the desired position.\\n\\nTo make it more intuitive, I give the following descriptive image\\n\\n![Insertion Sort](./images/insertion.GIF)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when we apply the algorithm to the sorted array. Then, we only need to iterate over the array, only compare and do not need to perform a swap step at all. So the time complexity will be ${\\\\Theta(n)}$.\\n- **Worst case:** occurs when the array is sorted in reverse, there will be 1 comparison and assignment in the first pass, 2 comparisons and assignment in the second,... Therefore, the total number of operations compare and assign would be ${Sum = 1 + 2 +...+ (n-1) = \\\\frac{n \\\\times (n-1)}{2}}$. Therefore, time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef insertion_sort(arr):\\n    for key in range(1, len(arr)):\\n        value = arr[key]\\n        j = key - 1\\n        while j >= 0 and value < arr[j]:\\n            arr[j+1] = arr[j]\\n            j -= 1\\n        if key != j+1:\\n            arr[j+1] = value\\n\\ninsertion_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 4. Selection sort\\n\\nThe idea is that we will **assume** to split our array into 2 parts: sorted subarray ${arr{_1}}$ and unsorted subarray ${arr{_2}}$. At this moment, ${arr = (arr{_1})(arr{_2})}$.\\nWe will in turn find the smallest element of ${arr{_2}}$, detach and push the element to ${arr{_1}}$. The **assumption** here, that we are not actually creating 2 new sub-arrays, but the operations are performed on the original array.\\n\\n**Algorithm:**\\n\\n- Find the smallest element of ${arr{_2}}$.\\n- Swap the position of that smallest element with the first element of ${arr{_2}}$. At this point, we assume in ${arr{_2}}$, \u200bthat smallest element is gone, and now it has been merged into ${arr{_1}}$.\\n\\nI have an image to make the algorithm more intuitive\\n\\n![Selection Sort](./images/selection.PNG)\\n\\n**Algorithm analysis:**\\n\\n- **Best case:** occurs when applying the algorithm on the sorted array, we only have to compare, not swap positions. So the time complexity will be ${\\\\Theta(n)}$.\\n- **Worst case:** occurs when the above array is sorted in reverse, each time we have to find the smallest element of the subarray ${arr{_2}}$. Therefore, the total number of traversals to find the smallest elements will be ${Sum = (n-1) + (n-2) +...+ 1 = \\\\frac{n \\\\times (n-1)}{2}}$. Time complexity will be ${\\\\Theta(n^2)}$.\\n- **Space complexity:** ${\\\\Theta(1)}$.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef selection_sort(arr):\\n    for i in range(len(arr) - 1):\\n        min_index = i\\n        for j in range(i+1, len(arr)):\\n            if arr[j] < arr[min_index]:\\n                min_index = j\\n        if i != min_index:\\n            arr[min_index], arr[i] = arr[i], arr[min_index]\\n\\nselection_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 5. Merge sort\\n\\nMerge sort is one of the most efficient algorithms. The algorithm works on the principle of divide and conquer, separating the arrays into 2 sub-arrays, respectively, until the sub-arrays have only 1 element left. Then the algorithm \\"merge\\" those sub-arrays into a fully sorted array.\\n\\n**Algorithm:**\\n\\n- Divide the original array into 2 sub-arrays, 2 sub-arrays into 4 more sub-arrays,... until we get ${n}$ subarrays, each subarray contains 1 element.\\n\\n![Merge Sort 1](./images/merge_1.PNG)\\n\\n- Merge sub-arrays to create larger arrays sorted in order until we get a single array. That is the sorted array from the original array.\\n\\n![Merge Sort 2](./images/merge_2.PNG)\\n\\nSummarize the algorithm in 1 image\\n\\n![Merge Sort](./images/merge.PNG)\\n\\n**Algorithm analysis:**\\n\\n- **Split array:** the algorithm will calculate the midpoint of the array by taking the array length and then dividing it by 2, so it takes constant time ${\\\\Theta(1)}$ to split the array into 2 sub-arrays.\\n- **Sorting subarrays:** assuming array sorting costs ${T(n)}$ time. So to sort 2 sub-arrays, we spend ${2T(\\\\frac{n}{2})}$ time.\\n- **Merge 2 subarrays:** using the \\"2-finger\\" algorithm, each index finger points to the beginning of each subarray. We in turn compare 2 numbers at 2 positions that 2 fingers point to and choose the smaller number to push into the resulting array. Every element in a subarray is pushed in, we move the index finger to the next element of that array. This will make us have to traverse ${2 \\\\times \\\\frac{n}{2} = n}$ elements, therefore, that costs ${\\\\Theta(n)}$. Thus, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = \\\\Theta(1) + 2T(\\\\frac{n}{2}) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith base case here is ${T(1) = \\\\Theta(1)}$.\\n\\n![Merge Sort 3](./images/merge_3.PNG)\\n\\nFor each tree level, the algorithm executes ${\\\\Theta(n)}$ units of work, there are total ${1+logn}$ levels. Therefore, ${T(n) = \\\\Theta(n + nlogn) = \\\\Theta(nlogn)}$. Time complexity will be ${\\\\Theta(nlogn)}$.\\n\\n- **Space complexity:** Because in the \\"merge\\" step, we have to manually create 2 sub-arrays, each with a number of elements ${\\\\frac{n}{2}}$, so the auxiliary memory space will be ${\\\\Theta(n)}$.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef merge(arr, l, m, r):\\n    n1 = m - l + 1\\n    n2 = r - m\\n    L = []\\n    R = []\\n    for i in range(n1):\\n        L.append(arr[l+i])\\n    for j in range(n2):\\n        R.append(arr[m+j+1])\\n    i = 0\\n    j = 0\\n    k = l\\n    while i < n1 and j < n2:\\n        if L[i] <= R[j]:\\n            arr[k] = L[i]\\n            i += 1\\n        else:\\n            arr[k] = R[j]\\n            j += 1\\n        k += 1\\n    while i < n1:\\n        arr[k] = L[i]\\n        i += 1\\n        k += 1\\n    while j < n2:\\n        arr[k] = R[j]\\n        j += 1\\n        k += 1\\n\\ndef merge_sort(arr, l, r):\\n    if l < r:\\n        m = math.floor(l + (r-l)/2)\\n        merge_sort(arr, l, m)\\n        merge_sort(arr, m+1, r)\\n        merge(arr, l, m, r)\\n\\nmerge_sort(ini_arr, 0 ,len(ini_arr) - 1)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 6. Heap sort\\n\\nThe heap sort is based on the binary heap data structure.\\n\\n**Binary heap data structure:**\\nAn array of data can be represented as a binary tree as follows\\n\\n![Heap Sort](./images/heap.PNG)\\n\\nFor any node with the corresponding index ${i}$ in the binary tree above\\n\\n- Parent node of ${i}$, which is ${parent(i)}$ will have the index of ${\\\\Bigl\\\\lfloor\\\\dfrac{i}{2}\\\\Bigr\\\\rfloor\\\\qquad}$.\\n- Left child node, which is ${leftchild(i)}$ will have the index of ${2i}$.\\n- Right child node, which is ${rightchild(i)}$ will have the index of ${2i + 1}$.\\n\\nThere are two types of this data structure: max-heap and min-heap.\\n\\n- In max-heap, we always have ${A[parent(i)] \\\\ge A[i]}$. Therefore, the largest element is in the root, and the smallest element is in the leaf.\\n- In min-heap, we always have ${A[parent(i)] \\\\le A[i]}$. Therefore, the smallest element is in the root, and the largest element is in the leaf.\\n\\nFrom there, this sorting algorithm applies max-heap or min-heap (in this article, I will use max-heap) to create an ordered array.\\n\\n**Create a max-heap:** called **max_heapify**\\nI will give a simple example with a 3-element array for added visualization, but with an n-element array it will need to be done in a more general way\\n\\n![Heap Sort 2](./images/heap_2.PNG)\\n\\nPython code for **max_heapify** at a node with index ${i = index}$, ${length}$ is the length of the array, added as a constraint for the index of child nodes. The algorithm below says that, if the node is at ${i = index}$ ot in accordance with the max-heap rule, we will **max_heapify** the tree with the root as that node, and **max_heapify** the trees with the root being the left and right child nodes of that node.\\n\\n```python\\ndef max_heapify(arr, length, index):\\n    l = (index + 1) * 2 - 1\\n    r = (index + 1) * 2\\n    largest = index\\n    if l < length and arr[index] < arr[l]:\\n        largest = l\\n    if r < length and arr[largest] < arr[r]:\\n        largest = r\\n    if largest != index:\\n        arr[index], arr[largest] = arr[largest], arr[index]\\n        max_heapify(arr, length, largest)\\n```\\n\\n**max_heapify** will cost ${\\\\Theta(logn)}$ for each node under consideration. Because each time, that node will need to go down 1 level in the tree to be considered, the algorithm will choose the correct branch to go down and it will not backtrack back up. Therefore, the longest path this algorithm can take is from root to a leaf, which is the height of the tree. The height of the binary tree has ${n}$ nodes is ${\\\\Theta(logn)}$.\\n\\n**Algorithm:**\\n\\n- We represent the array and sort the elements to get a max-heap tree. Therefore, the root of this tree (this will correspond to the element with index ${i = 0}$ in the array, for ${i = 0, 1,..., n-1}$) will be the largest element.\\n- We swap the location of root ${arr[0]}$ with the last element of the array ${arr[n-1]}$. At this point, the largest element of the array is in the last index.\\n- Repeat steps 1 and 2 with the rest of the array ${arr[0:n-2]}$,...\\n\\n**Algorithm analysis:**\\nBuilding a max-heap tree from an unsorted array needs ${\\\\Theta(n)}$ functiohn calls **max_heapify**, each **max_heapify** cost ${\\\\Theta(logn)}$ time. Thus, the whole algorithm has a time complexity of ${\\\\Theta(nlogn)}$.\\nHowever, the heap sort algorithm has an advantage over merge sort in that it only uses ${\\\\Theta(1)}$ temporary memory, while merge sort is ${\\\\Theta(n)}$. If the memory factor is also important in your system (eg. small memory systems like embedded systems, etc.), you should consider using heap sort rather than merge sort.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef max_heapify(arr, length, index):\\n    l = (index + 1) * 2 - 1\\n    r = (index + 1) * 2\\n    largest = index\\n    if l < length and arr[index] < arr[l]:\\n        largest = l\\n    if r < length and arr[largest] < arr[r]:\\n        largest = r\\n    if largest != index:\\n        arr[index], arr[largest] = arr[largest], arr[index]\\n        max_heapify(arr, length, largest)\\n\\ndef heap_sort(arr):\\n    length = len(arr)\\n    last = math.floor(length / 2)\\n    # T\u1ea1i \u0111\xe2y, ch\u1ec9 duy\u1ec7t t\u1eeb ph\u1ea7n t\u1eed n/2 \u0111\u1ed5 v\u1ec1, v\xec ph\u1ea7n t\u1eed t\u1eeb n/2 + 1,..., n \u0111\u1ec1u l\xe0 leaves. C\xe1c leaves \u0111\xe3 \u0111\u01b0\u1ee3c th\u1ecfa m\xe3n t\xednh ch\u1ea5t max-heap\\n    for i in range(last - 1, -1, -1):\\n        max_heapify(arr, length, i)\\n    for i in range(length - 1, 0, -1):\\n        arr[i], arr[0] = arr[0], arr[i]\\n        max_heapify(arr, i, 0)\\n\\nheap_sort(ini_arr)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 7. Quick sort\\n\\nThe quick sort algorithm, developed by a British computer scientist Tony Hoare in 1959, uses the principle of divide and conquer to sort an array.\\n\\n**Algorithm:**\\n\\n- **Split array:**\\n  - Select any element (called pivot), ${A[m]}$. If we choose a good pivot, our algorithm will run very fast. However, it will be difficult to know which element is considered a good pivot. There are a few common pivot selections:\\n    - Choose a random pivot.\\n    - Select the leftmost or rightmost element.\\n    - Take 3 elements: first, middle, last of the array and pick the median from them.\\n  - Split our array into 2 subparts: ${A[l:m-1]}$ consists of elements smaller than ${A[m]}$, and ${A[m+1:r]}$ consists of elements larger than ${A[m]}$. The image below shows more visually how to divide the array, with pivot always taking the last element\\n\\n![Quick Sort 2](./images/quick_2.PNG)\\n\\n- **Conquer:** recursively sort the above 2 subsections using quick sort.\\n- **Merge:** no need to combine the divided subsections because the final result is already sorted array.\\n- **The complete recursive algorithm:**\\n  - Select a pivot. Split array into 2 parts based on pivot.\\n  - Apply quick sort on the smaller half of the pivot.\\n  - Apply quick sort on half larger than pivot.\\n\\n**Algorithm analysis:**\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(k) + T(n-k-1) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith ${k}$ is the number of elements which are smaller than pivot. The time complexity for **partitioning** process is ${\\\\Theta(n)}$.\\n\\n- **Best case:** occurs when the **partitioning** algorithm always divides our array into exactly 2 equal or nearly equal parts.\\n\\n![Quick Sort](./images/quick.PNG)\\n\\nThus, in the best case, the time complexity will be ${\\\\Theta(nlogn)}$.\\n\\n- **Worst case:** occurs when the **partitioning** algorithm always chooses the largest or the smallest number as the pivot. If we choose the pivot using the \\"always pick the last element of the array\\" strategy, the worst case will happen when the array is sorted in descending order. At this moment\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(0) + T(n-1) + \\\\Theta(n) = T(n-1) + \\\\Theta(n)}$\\n\\n</p>\\n\\nWith the base case being ${\\\\Theta(1)}$ then in the worst case, time complexity will be ${T(n) = \\\\Theta(1) + \\\\Theta(n) + \\\\Theta(n) +...+ \\\\Theta(n) = \\\\Theta(n^2)}$\\n\\nAlthough the worst case of quick sort is much slower than other sorting algorithms, in practice the partition loop can be implemented efficiently on almost all data structures, because it contains relatively fewer \\"constant factors\\" (operators that require constant time ${\\\\Theta(1)}$) than other algorithms, and if two algorithms have the same time complexity ${\\\\Theta(nlogn)}$, the algorithm with fewer \\"constant factors\\" runs faster. Furthermore, the worst case of quick sort will rarely happen. However, with a very large amount of data and stored in external memory, merge sort will be preferred over quick sort.\\n\\n**Python Code**\\n\\n```python\\nini_arr = [1, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef partition(arr, l, r):\\n    pointer = l - 1\\n    pivot = arr[r]\\n    for j in range(l, r):\\n        if arr[j] < pivot:\\n            pointer += 1\\n            arr[pointer], arr[j] = arr[j], arr[pointer]\\n    arr[pointer+1], arr[r] = arr[r], arr[pointer+1]\\n    return pointer + 1\\n\\ndef quick_sort(arr, l, r):\\n    if l < r:\\n        pivot_index = partition(arr, l, r)\\n        quick_sort(arr, l, pivot_index - 1)\\n        quick_sort(arr, pivot_index+1, r)\\n\\nquick_sort(ini_arr, 0, len(ini_arr) - 1)\\nprint(ini_arr)\\n```\\n\\nOutput\\n\\n```python\\n[1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n### 8. Counting sort\\n\\nAn interesting algorithm I learned that even runs at linear time is counting sort. This algorithm is applied almost exclusively to integers, it is difficult or impossible to apply to real numbers.\\n\\n**Algorithm:**\\n\\n- Assume the elements of the original array ${A[0, 1, 2,..., n-1]}$ contains elements with values \u200b\u200bin the range ${[0, k]}$. Counting sort creates another temporary array to count as array ${B[0, 1, 2,..., k-1]}$ including ${k}$ elements. The ${i^{th}}$ element of ${B}$ will contain the number of elements ${A[j]}$ satisfy ${A[j] = i }$ with ${j = 0, 1, 2,..., n-1}$.\\n- From ${B}$, if we flatten it, we will get an array ${C[0, 1, 2,..., n-1]}$ contains ordered elements. To be more intuitive, I have the following example\\n\\n![Counting Sort](./images/counting.PNG)\\n\\nFor the case ${A}$ contains negative elements, we find the smallest element of ${A}$ and store that element of ${A}$ at the index ${0}$ of the array ${B}$ (because there cannot exist negative index in an array).\\n\\n**Algorithm analysis:**\\n\\n- Create empty array ${B}$ cost ${\\\\Theta(k)}$ time.\\n- Calculate the elements of the array ${B}$ based on ${A}$ cost ${\\\\Theta(n)}$ time.\\n- Flatten ${B}$ to have ${C}$ will cost ${\\\\Theta(n)}$ time.\\n\\nTotal time complexity will be ${\\\\Theta(n+k)}$.\\n\\nThis analysis will be clearly written in the comments of the code.\\n\\n- **Space complexity:** ${\\\\Theta(n+k)}$\\n\\n**Python Code**\\n\\n```python\\nini_arr = [-10, -5, -20, -10, -1, -5, 5, 2, 45, 2, 32, 12, 55, 26, 77, 8]\\n\\ndef counting_sort(A):\\n    min_ele = int(min(A))\\n    max_ele = int(max(A))\\n    range_ele = max_ele - min_ele + 1\\n    B = []\\n    # This costs O(k) time\\n    for i in range(range_ele):\\n        B.append(0)\\n    ###############################\\n\\n    # This costs O(n) time\\n    for i in range(len(A)):\\n        B[A[i] - min_ele] += 1\\n    ###############################\\n\\n    C = []\\n    # We have sum(B)= n = len(A) and append() cost constant time O(1). So this step costs O(n) time\\n    for i in range(range_ele):\\n        for j in range(B[i]):\\n            C.append(i + min_ele)\\n    ###############################\\n\\n    # ----\x3e In total, the algorithm costs O(n+k) time complexity\\n    return C\\n\\nprint(counting_sort(ini_arr))\\n```\\n\\nOutput\\n\\n```python\\n[-20, -10, -10, -5, -5, -1, 2, 2, 5, 8, 12, 26, 32, 45, 55, 77]\\n```\\n\\n**Counting sort comments:**\\n\\n- As we saw above, counting sort has a time complexity and a space complexity ${\\\\Theta(n+k)}$, o counting sort is very efficient when the range of the input data is small, not much larger than ${n}$ (or when ${k}$ is quite small). For example, if ${k}$ is large, about ${k = \\\\Theta(n^2)}$, then time complexity and space complexity will be ${\\\\Theta(n^2)}$, that is very bad.\\n- Counting sort can also be suitable for problems where the elements of the array are another data structure, but that data structure must have a ${key}$ is the integer to represent each object in that data structure.\\n- Counting sort is also a subroutine for a more powerful algorithm called Radix sort, a sort algorithm that runs at linear time with values ${k}$ greater than in counting sort, which is ${k = n^{\\\\Theta(1)}}$ (constant power of ${n}$).\\n- As mentioned in the introduction, you arrange the bowls in order from small to large, accumulating the number of bowls by size into blocks one by one. That is using counting sort. As the example below, equal numbers are cubed by color.\\n\\n![Counting Sort 2](./images/counting_2.PNG)\\n\\n### 9. Additional notes\\n\\nSorting algorithms are quite interesting. One of the things that makes people feel the most comfortable is seeing their bedrooms organized and clean, the same is true when we look at other things arranged. The website about animations of sorting algorithms that give you a sense of satisfaction and relaxation can be found **[here](https://www.toptal.com/developers/sorting-algorithms)**.\\n\\n### 10. References\\n\\n[Sorting Algorithms](https://www.interviewbit.com/tutorial/sorting-algorithms/#sorting-algorithms)\\n\\n[Sorting Algorithms](https://brilliant.org/wiki/sorting-algorithms/)\\n\\n[QuickSort](https://www.geeksforgeeks.org/quick-sort/)"},{"id":"peak-finding","metadata":{"permalink":"/blogs/blog/peak-finding","editUrl":"https://github.com/lam1051999/blogs/edit/main/blog/2021-02-18-peak-finding/index.md","source":"@site/blog/2021-02-18-peak-finding/index.md","title":"Peak Finding Algorithm","description":"Intro","date":"2021-02-18T00:00:00.000Z","formattedDate":"February 18, 2021","tags":[{"label":"peak","permalink":"/blogs/blog/tags/peak"},{"label":"peak finding","permalink":"/blogs/blog/tags/peak-finding"},{"label":"algorithms","permalink":"/blogs/blog/tags/algorithms"}],"readingTime":5.11,"truncated":true,"authors":[{"name":"Lam Tran","title":"Data Engineer","url":"https://github.com/lam1051999","imageURL":"https://github.com/lam1051999.png","key":"tranlam"}],"frontMatter":{"slug":"peak-finding","title":"Peak Finding Algorithm","authors":"tranlam","tags":["peak","peak finding","algorithms"],"image":"./images/intro.PNG"},"prevItem":{"title":"Fundamental Sorting Algorithms","permalink":"/blogs/blog/sorting-algorithms"}},"content":"![Intro](./images/intro.PNG)\\n\\nToday I will talk about an extremely basic algorithm that I and most of you who are starting to learn about algorithmic programming have done. That is the Peak Finding algorithm.\\n\\n\x3c!--truncate--\x3e\\n\\n### 1. Introduction to the algorithm\\n\\nIn an array, a number is said to be a \\"peak\\" if and only if its adjacent elements are less than or equal to the element in question. Imagine that there is a mountain range like this\\n\\n![Peaks](./images/peaks.PNG)\\n\\nThe red arrows above point to the peaks of a mountain range, because those points are higher than the neighboring points around it (the points on the mountainside).\\n\\nTo be more intuitive in programming, let\'s take an example with the following array:\\n![1D Array](./images/1Darr.PNG)\\n\\nConsidering the array of symbols above, the element at position 3 is called a peak if and only if **${c \\\\ge b}$** and **${c \\\\ge d}$**. The 9th element is called a peak if and only if **${i \\\\ge h}$**.\\n\\nNotice that:\\n\\n- In an array, there will always be at least one peak.\\n- This problem of ours will be to find one peak, not all the peaks.\\n\\n### 2. Finding peak in 1-dimensional array\\n\\nSuppose we have a 1-dimensional array of **${n}$** elements, find a peak of that array.\\n\\n#### 2.1. Linear Traversing\\n\\n**Idea:** Iterate through each element of the array and check if the element under consideration satisfies the property of being a peak.\\n\\n**Algorithm analysis:** Each element being browsed will have conditional statements to check if the element is a peak, these conditional statements take constant time ${\\\\Theta(1)}$. In the worst case, we\'ll have to go through all **${n}$** elements of the array to find the peak. Therefore, the worst case of the algorithm will have a time complexity of ${\\\\Theta(n)}$.\\n\\n#### 2.2. Binary Search\\n\\n**Idea:** In this way, we will always look at the middle of the traversed array and decide which half of the array to traverse to find the peak.\\n\\n**Algorithm:**\\n\\n- Look at the location ${\\\\frac{n}{2}}$.\\n- If ${a[\\\\frac{n}{2}] \\\\lt a[\\\\frac{n}{2} - 1]}$, we look at the left half (the elements ${1, 2,...,\\\\frac{n}{2} - 1)}$ of the array under consideration to find the peak.\\n- If ${a[\\\\frac{n}{2}] \\\\lt a[\\\\frac{n}{2} + 1]}$, we look at the right half (the elements ${\\\\frac{n}{2} + 1, \\\\frac{n}{2} + 2,..., n)}$ of the array under consideration to find the peak.\\n- If both conditions are not satisfied, we return the position element ${\\\\frac{n}{2}}$ is a peak.\\n\\nTo explain this, I have a drawing to make it more intuitive\\n\\n![1D Expression](./images/1Dexp.PNG)\\n\\nThe red arrow points to the current position. Suppose we are standing on a position in the mountains, so that we can climb to the top, we will always look to the side that we see its position higher than where we are standing, and that is also the algorithm to solve for this problem.\\n\\n**Algorithm analysis:** Using divide and conquer, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(n) = T(\\\\frac{n}{2}) + \\\\Theta(1)}$\\n\\n</p>\\n\\nTime complexity for comparative conditionals ${\\\\Theta(1)}$, base case here is ${T(1) = \\\\Theta(1)}$.\\nFrom that, ${T(n) = \\\\Theta(1) + \\\\Theta(1) +...+ \\\\Theta(1) = \\\\Theta(log{_2}{n})}$.\\n\\n**Python Code**\\n\\n```python\\nimport math\\nini_arr = [10, 20, 15, 2, 23, 90, 67]\\n\\ndef peak_finding(arr):\\n    length = len(arr)\\n    middle = math.floor(length / 2)\\n    if length == 1:\\n        return arr[0]\\n    if length == 2:\\n        return arr[0] if (arr[0] >= arr[1]) else arr[1]\\n    if arr[middle] < arr[middle - 1]:\\n        return peak_finding(arr[:middle])\\n    elif arr[middle] < arr[middle + 1]:\\n        return peak_finding(arr[middle + 1:])\\n    else:\\n        return arr[middle]\\n\\n\\nprint(peak_finding(ini_arr))\\n```\\n\\nOutput\\n\\n```python\\n20\\n```\\n\\n### 3. Finding peak in 2-dimensional array\\n\\nSuppose we have a 2-dimensional array **${m \\\\times n}$** represented as a matrix of m rows and n columns\\n![2D Matrix](./images/2Dmat.PNG)\\n\\nAn element is considered to be a peak if and only if it is greater than or equal to all adjacent elements vertically and horizontally.\\n\\n#### 3.1. Direct traversal\\n\\n**Idea:** Iterate through each element of the array and check if the element under consideration satisfies the property of being a peak.\\n\\n**Algorithm analysis:** the worst case of the algorithm will have a time complexity of ${\\\\Theta(m \\\\times n)}$.\\n\\n#### 3.2. Greedy Ascent Algorithm\\n\\n**Idea:** We start at a random point. With the point under consideration, we compare it with 4 adjacent points vertically and horizontally, if any value is greater than the point under consideration, we will consider the next point to be that point.\\n\\n**Algorithm analysis:** At first glance, the algorithm seems to be more efficient, but its worst case is still ${\\\\Theta(m \\\\times n)}$ when we have to traverse most of the elements.\\n\\n#### 3.3. Jamming Binary Search Algorithm\\n\\n**Idea:** We rely on Binary Search as applied to the 1-dimensional array above.\\n\\n**Algorithm:**\\n\\n- Select the column in the middle ${i = \\\\frac{n}{2}}$. Find the maximum value of that column. Assume that value is at position ${(j, i)}$.\\n- Compare elements at positions ${(j, i - 1), (j, i), (j, i + 1)}$.\\n- We choose the left-part submatrix if ${a[j, i - 1] \\\\gt a[j, i]}$, choose the right-part submatrix if ${a[j, i + 1] \\\\gt a[j, i]}$ to consider the next step.\\n- Otherwise, we return the value ${a[j, i]}$ is a peak.\\n\\n**Algorithm analysis:** The base case here will be that we only have a single column, find the maximum value of that column. From this, we have the following expression\\n\\n<p style={{textAlign: \\"center\\"}}>\\n\\n${T(m, n) = T(m, \\\\frac{n}{2}) + \\\\Theta(m) }$\\n\\n</p>\\n\\nWith ${T(m, 1) = \\\\Theta(m)}$.\\nTherefore, ${T(m, n) = \\\\Theta(m) + \\\\Theta(m) +...+ \\\\Theta(m) = \\\\Theta(mlog{_2}{n})}$.\\n\\n**Python Code**\\n\\n```python\\nimport numpy as np\\nimport math\\nini_matrix = np.array([[14, 13, 12, 6], [15, 9, 11, 7], [\\n                      16, 17, 19, 92], [17, 18, 17, 12]])\\n\\ndef peak_finding_2d(matrix):\\n    (rows, cols) = matrix.shape\\n    j = math.floor(cols / 2)\\n    i_in_max = np.argmax(matrix[:, j])\\n    if cols == 1:\\n        return matrix[i_in_max, j]\\n    if cols == 2:\\n        peak_1 = np.amax(matrix[:, 0])\\n        peak_2 = np.amax(matrix[:, 1])\\n        return peak_1 if peak_1 >= peak_2 else peak_2\\n    if matrix[i_in_max, j] < matrix[i_in_max, j-1]:\\n        return peak_finding_2d(matrix[:, :j])\\n    elif matrix[i_in_max, j] < matrix[i_in_max, j+1]:\\n        return peak_finding_2d(matrix[:, (j+1):])\\n    else:\\n        return matrix[i_in_max, j]\\n\\n\\nprint(peak_finding_2d(ini_matrix))\\n```\\n\\nOutput\\n\\n```python\\n92\\n```"}]}')}}]);